{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcb33d0-2e7b-409d-8379-077d9754227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/LLaMA-Factory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11f0229-0ebd-4e2b-b2ef-7e91ffba2f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1317 kB]\n",
      "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2610 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]      \n",
      "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.0 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1230 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [45.2 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3612 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1526 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [53.3 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3748 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2912 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
      "Fetched 37.6 MB in 3s (14.0 MB/s)                           \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libgpm2 libsodium23 vim-common vim-runtime xxd\n",
      "Suggested packages:\n",
      "  gpm ctags vim-doc vim-scripts\n",
      "The following NEW packages will be installed:\n",
      "  libgpm2 libsodium23 vim vim-common vim-runtime xxd\n",
      "0 upgraded, 6 newly installed, 0 to remove and 124 not upgraded.\n",
      "Need to get 8878 kB of archives.\n",
      "After this operation, 38.8 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xxd amd64 2:8.2.3995-1ubuntu2.23 [51.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-common all 2:8.2.3995-1ubuntu2.23 [81.5 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgpm2 amd64 1.20.7-10build1 [15.3 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsodium23 amd64 1.0.18-1build2 [164 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-runtime all 2:8.2.3995-1ubuntu2.23 [6833 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim amd64 2:8.2.3995-1ubuntu2.23 [1732 kB]\n",
      "Fetched 8878 kB in 1s (6953 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package xxd.\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../0-xxd_2%3a8.2.3995-1ubuntu2.23_amd64.deb ...\n",
      "Unpacking xxd (2:8.2.3995-1ubuntu2.23) ...\n",
      "Selecting previously unselected package vim-common.\n",
      "Preparing to unpack .../1-vim-common_2%3a8.2.3995-1ubuntu2.23_all.deb ...\n",
      "Unpacking vim-common (2:8.2.3995-1ubuntu2.23) ...\n",
      "Selecting previously unselected package libgpm2:amd64.\n",
      "Preparing to unpack .../2-libgpm2_1.20.7-10build1_amd64.deb ...\n",
      "Unpacking libgpm2:amd64 (1.20.7-10build1) ...\n",
      "Selecting previously unselected package libsodium23:amd64.\n",
      "Preparing to unpack .../3-libsodium23_1.0.18-1build2_amd64.deb ...\n",
      "Unpacking libsodium23:amd64 (1.0.18-1build2) ...\n",
      "Selecting previously unselected package vim-runtime.\n",
      "Preparing to unpack .../4-vim-runtime_2%3a8.2.3995-1ubuntu2.23_all.deb ...\n",
      "Adding 'diversion of /usr/share/vim/vim82/doc/help.txt to /usr/share/vim/vim82/doc/help.txt.vim-tiny by vim-runtime'\n",
      "Adding 'diversion of /usr/share/vim/vim82/doc/tags to /usr/share/vim/vim82/doc/tags.vim-tiny by vim-runtime'\n",
      "Unpacking vim-runtime (2:8.2.3995-1ubuntu2.23) ...\n",
      "Selecting previously unselected package vim.\n",
      "Preparing to unpack .../5-vim_2%3a8.2.3995-1ubuntu2.23_amd64.deb ...\n",
      "Unpacking vim (2:8.2.3995-1ubuntu2.23) ...\n",
      "Setting up libsodium23:amd64 (1.0.18-1build2) ...\n",
      "Setting up libgpm2:amd64 (1.20.7-10build1) ...\n",
      "Setting up xxd (2:8.2.3995-1ubuntu2.23) ...\n",
      "Setting up vim-common (2:8.2.3995-1ubuntu2.23) ...\n",
      "Setting up vim-runtime (2:8.2.3995-1ubuntu2.23) ...\n",
      "Setting up vim (2:8.2.3995-1ubuntu2.23) ...\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/vi.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/vi.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/vi.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/vi.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/vi.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/vi.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/vi.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/vi.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/view.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/view.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/view.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/view.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/view.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/view.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/view.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/view.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/ex.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/ex.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/ex.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/ex.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/ex.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/ex.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/ex.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/ex.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/editor (editor) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/editor.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/editor.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/editor.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/editor.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/editor.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/editor.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/editor.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/editor.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install vim -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e23bda83-d774-4e87-9918-399211cd57e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers<=4.46.1,>=4.41.2 (from -r requirements.txt (line 1))\n",
      "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets<=3.1.0,>=2.16.0 (from -r requirements.txt (line 2))\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate<=1.0.1,>=0.34.0 (from -r requirements.txt (line 3))\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<=0.12.0,>=0.11.1 (from -r requirements.txt (line 4))\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 5))\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<0.20.4,>=0.19.0 (from -r requirements.txt (line 6))\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting gradio<5.0.0,>=4.0.0 (from -r requirements.txt (line 7))\n",
      "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pandas>=2.0.0 (from -r requirements.txt (line 8))\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from -r requirements.txt (line 9))\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting einops (from -r requirements.txt (line 10))\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 11))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 12))\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting protobuf (from -r requirements.txt (line 13))\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting uvicorn (from -r requirements.txt (line 14))\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic (from -r requirements.txt (line 15))\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting fastapi (from -r requirements.txt (line 16))\n",
      "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting sse-starlette (from -r requirements.txt (line 17))\n",
      "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting matplotlib>=3.7.0 (from -r requirements.txt (line 18))\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting fire (from -r requirements.txt (line 19))\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (1.24.1)\n",
      "Collecting av (from -r requirements.txt (line 23))\n",
      "  Downloading av-14.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting tyro<0.9.0 (from -r requirements.txt (line 24))\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (2023.4.0)\n",
      "Collecting aiohttp (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (2.1.0+cu118)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (4.0.0)\n",
      "Collecting ffmpy (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (2.1.2)\n",
      "Collecting orjson~=3.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (9.3.0)\n",
      "Collecting pydub (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading ruff-0.9.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (4.4.0)\n",
      "Collecting urllib3~=2.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 8)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.0.0->-r requirements.txt (line 8))\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.0.0->-r requirements.txt (line 8))\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting click>=7.0 (from uvicorn->-r requirements.txt (line 14))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 14))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->-r requirements.txt (line 15))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic->-r requirements.txt (line 15))\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions~=4.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi->-r requirements.txt (line 16))\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->-r requirements.txt (line 18))\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.7.0->-r requirements.txt (line 18))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->-r requirements.txt (line 18))\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->-r requirements.txt (line 18))\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (2.4.7)\n",
      "Collecting termcolor (from fire->-r requirements.txt (line 19))\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (1.3.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0 (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2.1.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 24)) (2.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (2.1.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m168.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m158.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m184.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m227.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m219.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m181.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m267.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m279.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m160.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m215.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
      "Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m237.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading av-14.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m152.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m144.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m261.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.0-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m175.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m158.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m174.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m217.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.9.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m228.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m159.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=7135157aa84a311cb5bb74b232f6af21799b98d1d8ae1475059ca87282b8880e\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
      "Successfully built fire\n",
      "Installing collected packages: sentencepiece, pytz, pydub, xxhash, websockets, urllib3, tzdata, typing-extensions, tqdm, tomlkit, termcolor, shtab, shellingham, semantic-version, scipy, safetensors, ruff, regex, python-multipart, pyarrow, protobuf, propcache, orjson, mdurl, kiwisolver, importlib-resources, h11, fsspec, frozenlist, fonttools, ffmpy, einops, docstring-parser, dill, cycler, contourpy, click, av, async-timeout, annotated-types, aiohappyeyeballs, aiofiles, uvicorn, requests, pydantic-core, pandas, multiprocess, multidict, matplotlib, markdown-it-py, httpcore, fire, anyio, aiosignal, yarl, tiktoken, starlette, rich, pydantic, huggingface-hub, httpx, tyro, typer, tokenizers, sse-starlette, gradio-client, fastapi, aiohttp, accelerate, transformers, gradio, peft, datasets, trl\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.13\n",
      "    Uninstalling urllib3-1.26.13:\n",
      "      Successfully uninstalled urllib3-1.26.13\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "Successfully installed accelerate-1.0.1 aiofiles-23.2.1 aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 async-timeout-5.0.1 av-14.1.0 click-8.1.8 contourpy-1.3.1 cycler-0.12.1 datasets-3.1.0 dill-0.3.8 docstring-parser-0.16 einops-0.8.1 fastapi-0.115.8 ffmpy-0.5.0 fire-0.7.0 fonttools-4.56.0 frozenlist-1.5.0 fsspec-2024.9.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.29.0 importlib-resources-6.5.2 kiwisolver-1.4.8 markdown-it-py-3.0.0 matplotlib-3.10.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 orjson-3.10.15 pandas-2.2.3 peft-0.12.0 propcache-0.2.1 protobuf-5.29.3 pyarrow-19.0.1 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 pytz-2025.1 regex-2024.11.6 requests-2.32.3 rich-13.9.4 ruff-0.9.6 safetensors-0.5.2 scipy-1.15.2 semantic-version-2.10.0 sentencepiece-0.2.0 shellingham-1.5.4 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.45.3 termcolor-2.5.0 tiktoken-0.9.0 tokenizers-0.20.3 tomlkit-0.12.0 tqdm-4.67.1 transformers-4.46.1 trl-0.9.6 typer-0.15.1 typing-extensions-4.12.2 tyro-0.8.14 tzdata-2025.1 urllib3-2.3.0 uvicorn-0.34.0 websockets-12.0 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting deepspeed==0.14.0\n",
      "  Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson (from deepspeed==0.14.0)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting ninja (from deepspeed==0.14.0)\n",
      "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed==0.14.0)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (2.10.6)\n",
      "Collecting pynvml (from deepspeed==0.14.0)\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0) (4.12.2)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->deepspeed==0.14.0)\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (2024.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed==0.14.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.14.0) (1.3.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400399 sha256=16562d00fce89eb8cf27934bb3a975ab0e0ae62099323024a06cef9de18f067b\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, pynvml, ninja, deepspeed\n",
      "Successfully installed deepspeed-0.14.0 hjson-3.1.0 ninja-1.11.1.3 nvidia-ml-py-12.570.86 py-cpuinfo-9.0.0 pynvml-12.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Obtaining file:///workspace/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<=4.46.1,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (4.46.1)\n",
      "Requirement already satisfied: datasets<=3.1.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (0.9.6)\n",
      "Requirement already satisfied: tokenizers<0.20.4,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
      "Requirement already satisfied: gradio<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.15.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (5.29.3)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.34.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.10.6)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.8)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (14.1.0)\n",
      "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (0.8.14)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-chinese\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0) (0.29.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (3.11.12)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.28.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (6.5.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (2.1.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (3.10.15)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (9.3.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.9.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (2.3.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0) (12.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.45.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1) (3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2) (2024.11.6)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0) (1.7.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.5.0)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge-chinese) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (1.18.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0) (1.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0) (2.16.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0) (0.1.2)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hChecking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba, llamafactory\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=630d71d0afa8f200552c6d5ab761153ff408326d8aa2c3f682402a0e6d972953\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/69/31/d56d90b22a1777b0b231e234b00302a55be255930f8bd92dcd\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=23888 sha256=6cd533e05155c246a7c4d81a9f85674ff6e079295363ae96ca0727af4b1a6dbe\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yliq014_/wheels/6b/fb/2e/068b37b77399a386b5777cf8b247cb07a69197b4baef3732a2\n",
      "Successfully built jieba llamafactory\n",
      "Installing collected packages: jieba, rouge-chinese, joblib, nltk, llamafactory\n",
      "Successfully installed jieba-0.42.1 joblib-1.4.2 llamafactory-0.9.2.dev0 nltk-3.9.1 rouge-chinese-1.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install deepspeed==0.14.0\n",
    "!pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3911e3a-43e5-400b-9aa4-eb5041d96ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `llamaf` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llamaf`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889ac5b-79da-427c-be27-522939f8a4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a313a6-2680-4d61-a1fa-6cf5cea7f875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 07:29:11,852] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO|2025-02-19 07:29:13] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:24481\n",
      "[2025-02-19 07:29:14,757] torch.distributed.run: [WARNING] \n",
      "[2025-02-19 07:29:14,757] torch.distributed.run: [WARNING] *****************************************\n",
      "[2025-02-19 07:29:14,757] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2025-02-19 07:29:14,757] torch.distributed.run: [WARNING] *****************************************\n",
      "[2025-02-19 07:29:17,805] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-02-19 07:29:17,806] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[WARNING|2025-02-19 07:29:18] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-02-19 07:29:18] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-02-19 07:29:18] llamafactory.hparams.parser:359 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "config.json: 100%|█████████████████████████████| 878/878 [00:00<00:00, 8.45MB/s]\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:29:18,619 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:29:18,620 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████| 54.5k/54.5k [00:00<00:00, 2.31MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 25.9MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 296/296 [00:00<00:00, 2.65MB/s]\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:19,593 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:19,593 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:19,593 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:19,593 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:19,593 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-02-19 07:29:19,923 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:29:20,231 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:29:20,231 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:20,305 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:20,305 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:20,305 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:20,305 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-19 07:29:20,305 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-02-19 07:29:20,614 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-02-19 07:29:20] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>\n",
      "[INFO|2025-02-19 07:29:20] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-02-19 07:29:20] llamafactory.data.loader:157 >> Loading dataset irene93/news-analysis-ko...\n",
      "news.csv: 100%|██████████████████████████████| 400M/400M [00:09<00:00, 42.0MB/s]\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 100%|█| 102787/102787 [00:05<00:00, 19866.97 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 2881.7\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 500/500 [00:03<00:00, 136.22\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 65895, 83628, 34804, 56773, 125441, 111068, 120155, 109862, 44005, 102783, 245, 103850, 229, 80052, 13, 3146, 22035, 30426, 115790, 334, 11184, 56773, 125441, 111068, 25941, 19954, 62060, 83290, 12399, 11, 3439, 81, 11, 56103, 4229, 11, 3288, 10622, 109862, 101360, 3024, 106612, 87472, 17835, 62226, 92245, 13, 482, 12399, 16969, 220, 16, 93, 18, 115087, 109055, 17835, 114839, 61938, 12898, 3439, 81, 16969, 95713, 104414, 52688, 13094, 112601, 33390, 220, 16, 112601, 20565, 111699, 238, 66406, 41381, 19954, 220, 15, 38164, 120, 17835, 37155, 24140, 220, 16, 123590, 46663, 43139, 62226, 92245, 12898, 56103, 4229, 16969, 95713, 104414, 52688, 57575, 105797, 102662, 53400, 99458, 88708, 126546, 107364, 35495, 11, 55925, 99458, 88708, 123286, 99458, 88708, 92705, 18918, 115115, 56069, 13094, 168, 235, 105, 84734, 106612, 87472, 17835, 114839, 92245, 13, 482, 3288, 10622, 16969, 95713, 104414, 52688, 13094, 41871, 235, 30381, 82068, 33177, 66406, 41381, 220, 16, 86503, 30381, 82068, 33177, 66406, 41381, 482, 16, 1174, 41871, 235, 30381, 82068, 105870, 49085, 86503, 30381, 25, 82068, 105870, 49085, 120903, 66406, 41381, 220, 15, 38164, 120, 17835, 37155, 24140, 220, 16, 123590, 94768, 62226, 92245, 482, 104414, 52688, 25, 23955, 56773, 32179, 22035, 33390, 99901, 25, 106788, 19954, 3024, 106612, 87472, 17835, 114839, 92245, 128009, 128006, 882, 128007, 271, 20694, 4902, 8573, 2338, 429, 2485, 1129, 1931, 39932, 6423, 6973, 68993, 14, 59091, 8856, 14, 2366, 19, 14, 605, 14, 5067, 14, 2366, 14487, 22534, 22663, 1806, 62, 12171, 23525, 22, 62, 4364, 15, 62, 4728, 4924, 77042, 1347, 33587, 1347, 33587, 1347, 33587, 1347, 9883, 100711, 120758, 17835, 41214, 85672, 31820, 124338, 121712, 24486, 107449, 48918, 101438, 100508, 26799, 102558, 227, 34961, 24814, 108, 102914, 17835, 25941, 81673, 74623, 29102, 116849, 102914, 125007, 13094, 104350, 34983, 102058, 103542, 77535, 119626, 100508, 114542, 84696, 91040, 27796, 95713, 113094, 19954, 102597, 108499, 54059, 119073, 91786, 13, 9686, 31820, 18918, 117452, 102464, 101353, 101824, 109250, 60798, 105297, 106467, 64356, 127290, 19954, 108712, 88708, 34983, 105813, 57002, 103055, 41953, 57575, 120952, 48936, 105605, 13094, 107802, 22035, 31495, 232, 103211, 16969, 57519, 105115, 101568, 16134, 1347, 33587, 1347, 9883, 24, 33177, 101787, 64356, 101015, 19954, 103386, 100968, 33390, 120380, 104065, 63171, 103168, 14260, 126906, 58368, 119864, 54780, 101787, 64356, 120072, 107031, 9686, 31820, 18918, 120952, 24486, 102464, 101353, 113094, 54780, 101327, 103168, 106024, 20565, 62398, 106646, 101568, 13, 9686, 31820, 16969, 125959, 86157, 18359, 120893, 16969, 103123, 82001, 50643, 111068, 108661, 101164, 36092, 45780, 225, 103618, 30446, 220, 1313, 60861, 17835, 114702, 53400, 120461, 41214, 17835, 11, 103123, 106113, 103194, 18359, 118667, 96318, 16969, 52491, 83628, 101464, 31820, 1278, 31820, 8, 81673, 83719, 100660, 34983, 103966, 30381, 101003, 66965, 110257, 97096, 102335, 18359, 87138, 104182, 80402, 113, 38187, 48936, 29833, 65621, 103738, 103194, 101568, 16134, 1347, 33587, 1347, 9883, 31495, 252, 27796, 220, 22, 33177, 7, 102335, 22035, 108076, 8, 101266, 107489, 69697, 112, 103236, 122340, 102423, 25941, 101436, 110976, 44690, 102058, 103542, 117526, 16969, 220, 2366, 19, 102058, 103542, 48918, 119626, 100508, 57002, 121218, 26799, 17835, 101968, 26799, 48918, 101438, 100508, 113211, 102558, 227, 34961, 24814, 108, 102914, 17835, 25941, 12692, 121713, 20423, 3714, 8, 107449, 102293, 56154, 58935, 42529, 104554, 101787, 67945, 116567, 81673, 74623, 29102, 116849, 102914, 125007, 6838, 661, 432, 12328, 74, 359, 8, 55000, 80104, 30446, 101787, 67945, 116567, 18918, 101585, 30381, 101528, 16134, 1347, 33587, 1347, 9883, 101687, 103542, 117526, 16969, 1054, 113120, 18359, 110097, 24486, 109250, 60798, 105297, 18359, 101254, 119866, 29833, 65621, 103213, 120227, 106467, 64356, 38187, 21028, 109813, 105069, 122075, 254, 17835, 84618, 108381, 107054, 9686, 31820, 18918, 121712, 34983, 59777, 99029, 20565, 109250, 60798, 105297, 37155, 98934, 19954, 62398, 105701, 49531, 102519, 50467, 20565, 107368, 103659, 55216, 58126, 102621, 109509, 102058, 103542, 101787, 100508, 114542, 45618, 57002, 52976, 863, 103959, 35495, 101585, 30381, 111436, 18918, 116283, 35859, 104828, 16134, 1347, 33587, 1347, 9883, 101314, 66965, 112953, 101852, 101796, 21028, 107034, 77437, 54780, 101968, 57390, 11, 113295, 101577, 101738, 110685, 11, 102058, 57390, 81673, 105689, 105297, 78102, 19954, 93851, 58126, 44005, 61139, 20565, 110038, 108381, 91786, 13, 9686, 31820, 18918, 120952, 34983, 101003, 121737, 97096, 102335, 18359, 59777, 82001, 104182, 66610, 104834, 48936, 29833, 36439, 58901, 98243, 33390, 122035, 103684, 106467, 64356, 38187, 20565, 108838, 109250, 60798, 105297, 54780, 101003, 66965, 105689, 66338, 106467, 64356, 18918, 45618, 49085, 48936, 29833, 117097, 11, 107744, 101687, 57390, 113094, 49085, 103055, 101272, 57390, 48936, 29833, 109077, 111590, 55216, 67945, 112931, 16134, 1347, 33587, 1347, 9883, 100654, 96318, 63171, 103168, 14260, 126906, 58368, 107022, 101015, 107031, 9686, 31820, 18918, 120952, 24486, 101327, 103168, 111426, 45618, 123360, 121856, 119073, 91786, 13, 12343, 126906, 36092, 45780, 234, 250, 34804, 9686, 31820, 113094, 18359, 108712, 88708, 24486, 5251, 127802, 66965, 102249, 101327, 103168, 124999, 101438, 103194, 18359, 97096, 108214, 101360, 91786, 13, 117012, 106958, 220, 2366, 17, 100392, 103551, 5251, 127802, 103194, 66338, 106467, 64356, 38187, 111426, 107022, 50643, 32428, 124852, 58368, 58368, 107213, 54289, 51440, 81673, 114080, 116688, 106906, 89881, 16582, 101254, 124695, 110976, 120342, 2855, 28165, 26, 35, 122369, 111809, 72043, 101568, 16134, 1347, 33587, 1347, 9883, 76242, 98, 25941, 95252, 126906, 58368, 20565, 67890, 80816, 220, 1135, 4, 18918, 64432, 101314, 24486, 65677, 127702, 72115, 25941, 71682, 42771, 107573, 51440, 122960, 125567, 25941, 2855, 437, 53, 6632, 23258, 88886, 114484, 9686, 31820, 18918, 120952, 24486, 103153, 167, 229, 101, 106467, 64356, 38187, 18918, 111426, 72043, 101568, 13, 23955, 99105, 117396, 220, 2366, 16, 100392, 102058, 42771, 101687, 90335, 115777, 81673, 9686, 31820, 126470, 103153, 167, 229, 101, 105297, 106467, 64356, 38187, 124999, 101438, 103194, 3451, 11706, 26376, 12, 12405, 14, 13121, 529, 124695, 111426, 101824, 113094, 29833, 71023, 18359, 107472, 55216, 105711, 101314, 22035, 109567, 103168, 3100, 6486, 122369, 106906, 89881, 24486, 82818, 91786, 16134, 1347, 33587, 1347, 9883, 113120, 102464, 101353, 127290, 121048, 9686, 31820, 18918, 120952, 24486, 112785, 111426, 13094, 105220, 118450, 67236, 35495, 91786, 13, 64038, 58083, 101687, 50273, 117, 119524, 106200, 29102, 126906, 58368, 81673, 121066, 34983, 9686, 31820, 126470, 117452, 66610, 21121, 86351, 101353, 113094, 18359, 106024, 72043, 101568, 13, 106200, 29102, 126906, 58368, 21028, 101003, 121737, 109862, 107375, 124689, 100796, 120, 113094, 3451, 122906, 103843, 5417, 442, 261, 8, 529, 18918, 120952, 34983, 101228, 104690, 21028, 48555, 230, 106446, 57575, 9686, 31820, 18918, 127433, 78326, 101360, 11, 64038, 29102, 101687, 50273, 117, 120226, 124852, 58368, 32428, 101796, 40011, 45780, 233, 109, 25941, 5462, 40, 8, 127561, 17835, 117452, 18359, 102464, 101353, 44005, 75908, 77437, 101568, 13, 99105, 114333, 104441, 101711, 101360, 65621, 102464, 101353, 127290, 16969, 118562, 113120, 11, 3396, 115, 234, 41953, 113120, 11, 62060, 41953, 113120, 78102, 220, 18, 60861, 117452, 102757, 101568, 16134, 1347, 33587, 1347, 33587, 1931, 4902, 8573, 2338, 429, 2485, 1129, 1931, 39932, 6423, 6973, 68993, 14, 59091, 8856, 14, 2366, 19, 14, 605, 14, 5067, 14, 2366, 14487, 22534, 21996, 2304, 62, 12171, 22266, 19, 62, 4364, 15, 62, 10961, 3592, 77042, 1347, 33587, 1347, 33587, 1347, 33587, 1347, 9883, 100508, 101015, 121048, 120380, 9686, 31820, 106024, 26799, 102823, 103405, 101930, 18359, 103745, 61394, 96318, 35495, 91786, 13, 102155, 116845, 96318, 29102, 55216, 103719, 54780, 100508, 110976, 55421, 99937, 50, 8, 41214, 110976, 101353, 103123, 124788, 121066, 34983, 9686, 31820, 8, 53017, 54780, 41214, 106467, 64356, 38187, 21028, 126728, 125959, 102612, 103123, 106113, 103194, 32428, 3451, 13447, 13094, 27796, 529, 21028, 220, 18, 101532, 55421, 124007, 18918, 84415, 101464, 102837, 121737, 102335, 57139, 66406, 1361, 97234, 12, 2783, 8, 113094, 18359, 120952, 34983, 110572, 82273, 103719, 17835, 111850, 80732, 101528, 13, 102155, 103123, 124788, 220, 679, 21, 116614, 9686, 31820, 53017, 19954, 93851, 58126, 44005, 3451, 123789, 110311, 529, 103123, 106113, 103194, 21028, 220, 18, 101532, 55421, 124007, 18918, 110572, 82273, 103719, 111850, 80732, 44005, 78102, 120380, 104065, 57575, 41214, 106024, 109969, 82001, 26799, 17835, 110606, 121, 101709, 35495, 91786, 16134, 1347, 33587, 1347, 9883, 108922, 66406, 57139, 122851, 115978, 105297, 55421, 107625, 54596, 97, 86351, 101353, 110976, 110816, 41953, 7, 105297, 29102, 54780, 116567, 116504, 1054, 8318, 31820, 20565, 101003, 66965, 110257, 97096, 102335, 19954, 126652, 101412, 108200, 105453, 112024, 13094, 116283, 113048, 86351, 111323, 95713, 127290, 18918, 106024, 83290, 105813, 57002, 19954, 115839, 122910, 16969, 102058, 125337, 121856, 119073, 91786, 863, 51440, 101203, 1054, 54059, 102436, 105813, 57002, 104182, 117012, 120952, 24486, 106467, 64356, 38187, 78102, 13094, 115839, 53400, 33229, 109065, 16969, 47782, 102077, 11, 64432, 93917, 124852, 58368, 100711, 106153, 121803, 107779, 80816, 101709, 96451, 115602, 127423, 106478, 101353, 52976, 863, 105771, 114942, 101528, 16134, 1347, 33587, 1347, 9883, 108922, 116567, 16969, 1054, 101272, 38187, 17835, 122851, 115978, 105297, 125422, 220, 679, 23, 116899, 117870, 24486, 46810, 113120, 106434, 106024, 19954, 103386, 100968, 33390, 11, 66610, 21121, 46810, 113120, 57575, 73653, 220, 9413, 123590, 9686, 31820, 20565, 74959, 33943, 112039, 863, 51440, 101203, 1054, 8318, 31820, 20565, 105813, 57002, 104182, 112700, 117460, 20565, 65621, 107054, 49508, 105164, 57519, 110572, 106024, 26799, 102823, 106024, 72043, 32428, 103123, 101015, 112373, 11, 109287, 18359, 127992, 101852, 105711, 102893, 102464, 101353, 101360, 106467, 64356, 127369, 64432, 61727, 105, 13094, 112098, 111590, 55216, 67945, 52976, 863, 105771, 67236, 13447, 103850, 117653, 16134, 1347, 33587, 1347, 9883, 24486, 104790, 102058, 103542, 117526, 16969, 121066, 34983, 109018, 41214, 106024, 26799, 105880, 102058, 103542, 48918, 119626, 100508, 57002, 121218, 26799, 17835, 101585, 30381, 24486, 82818, 91786, 13, 121066, 34983, 121218, 113211, 103236, 110517, 102423, 108742, 29102, 102525, 17155, 4306, 258, 13528, 24551, 8, 124852, 58368, 108733, 102953, 82233, 29833, 102080, 86503, 56154, 41953, 54780, 103745, 99029, 75984, 103618, 102668, 73653, 5549, 4361, 42255, 1543, 8, 107449, 102266, 250, 101272, 105010, 13094, 116323, 67945, 101787, 67945, 116567, 16969, 78872, 20565, 113295, 101577, 106906, 101015, 81673, 112655, 59134, 48424, 69332, 27797, 44005, 22035, 19954, 102597, 55925, 63375, 21028, 117210, 18918, 106589, 101948, 104182, 82818, 116407, 16969, 108366, 235, 21121, 82068, 121712, 18359, 23955, 167, 97, 226, 96318, 11, 124141, 777, 62060, 27433, 58260, 52375, 118433, 122035, 103684, 107696, 83628, 18359, 111426, 127369, 55216, 58126, 24486, 100994, 17835, 18918, 127507, 107094, 103211, 16134, 1347, 33587, 1347, 9883, 24486, 33931, 55430, 126887, 23561, 64, 1839, 429, 39626, 25, 5104, 73, 31, 295, 6423, 6973, 68993, 760, 5104, 73, 31, 295, 6423, 6973, 68993, 524, 64, 9414, 27, 1347, 33587, 1347, 9883, 13094, 105292, 167, 47318, 7, 2185, 39932, 6423, 6973, 68993, 705, 101480, 101353, 66965, 58232, 101824, 29833, 102201, 11, 102888, 103588, 101796, 101136, 22035, 9668, 33587, 1347, 33587, 64, 1839, 1151, 2485, 1129, 2185, 39932, 6423, 6973, 68993, 11576, 2218, 53905, 10399, 6404, 101204, 29102, 57139, 115896, 119567, 83628, 52688, 23955, 105292, 167, 47318, 524, 64, 29, 128009, 128006, 78191, 128007, 13922, 1743, 1232, 364, 57139, 100654, 48918, 101438, 100508, 26799, 102558, 227, 34961, 24814, 108, 102914, 17835, 25941, 81673, 74623, 29102, 116849, 102914, 125007, 13094, 96677, 120758, 17835, 41214, 85672, 31820, 124338, 121712, 83290, 104350, 34983, 102058, 103542, 77535, 119626, 100508, 114542, 121218, 116429, 11, 9686, 31820, 19954, 102597, 125718, 13094, 108499, 54059, 119073, 103924, 13, 9686, 31820, 16969, 101003, 121737, 97096, 102335, 18359, 66610, 104834, 48936, 29833, 65621, 96451, 111490, 120693, 112795, 117452, 102464, 101353, 54780, 109250, 60798, 105297, 106467, 64356, 127290, 57575, 101003, 105115, 24486, 113094, 17835, 56773, 88708, 107094, 35495, 103924, 13, 111530, 120380, 104065, 63171, 103168, 56154, 81673, 106024, 120072, 102823, 23955, 113094, 18359, 120952, 24486, 101327, 103168, 111426, 54780, 102464, 101353, 113094, 106024, 19954, 106249, 102133, 101709, 116768, 101360, 103924, 16045, 364, 14625, 81, 56291, 1232, 364, 15, 518, 364, 76409, 4229, 1232, 2570, 17470, 14649, 518, 364, 16482, 20772, 4181, 364, 25526, 10622, 1232, 220, 16, 92, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 주어진 뉴스를 분석하는 챗봇입니다. **지시사항**:- 주어진 뉴스에 대하여 summary, advr, stk_code, sent_score 분석하고 json 형태로 출력하세요. - summary는 1~3줄 사이로 작성합니다.- advr는 해당 본문이 광고면 1 광고가 아닐경우에 0 으로 정수 1개의 값으로 출력하세요.- stk_code는 해당 본문에서 언급된 종목명을 찾고, 그 종목명의 종목 코드를 찾아 파이썬 리스트 형태로 작성하세요. - sent_score는 해당 본문이 긍정적일경우 1 부정적일경우 -1, 긍정적이지도 부정:적이지도 않을경우 0 으로 정수 1개의 값을 출력하세요 - 본문: 이 주어지면 결과: 다음에 json 형태로 작성하세요<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<img alt=\"\" src=\"https://img.etoday.co.kr/pto_db/2024/10/600/20241008155337_2086417_1200_800.jpg\"/><br/><br/><br/><br/>마이크로 RNA(miRNA)를 발견한 미국 생물학자 빅터 앰브로스와 개리 러브컨이 올해 노벨생리의학상을 받으면서 해당 기술에 대한 높아지고 있다. miRNA를 암 진단 및 난치병 치료 분야에 접목해 임상 현장에서 활용할 날이 머지않았다는 전망이다.<br/><br/>9일 의료계에 따르면 국내외 제약·바이오 기업과 의료기관에서는 miRNA를 활용한 진단 기술과 신약 연구가 한창이다. miRNA는 핵산을 이루는 단위체 뉴클레오타이드 22개로 구성된 작은 RNA로, 단백질을 만들어내는 메신저RNA(mRNA)와 결합해 특정 유전자의 발현을 선택적으로 억제할 수 있는 물질이다.<br/><br/>앞서 7일(현지시간) 스웨덴 카롤린스카연구소 노벨위원회는 2024 노벨 생리의학상 수상자로 분자 생물학자인 빅터 앰브로스(Victor Ambros) 미국 매사 추세츠 의대 교수와 개리 러브컨(Gary Ruvkun) 하버드 의대 교수를 선정했다.<br/><br/>노벨위원회는 “암을 포함한 난치병을 고칠 수 있는 차세대 치료제의 비밀 열쇠로 여겨지는 miRNA를 발견해 인류가 난치병 정복에 한 걸음 더 다가가는 데 기여했기에 노벨 의학상을 시상한다” 라고 선정 이유를 밝혔다.<br/><br/>유전자는 세포의 증식과 분화, 면역반응, 노화와 질병 등에 관여하는 정보가 담겨 있다. miRNA를 활용해 유전자 발현을 인위적으로 조절할 수 있게 되면 효과적인 치료제가 없는 난치병과 유전 질환 치료를 시도할 수 있으며, 항노화 기술도 현실화할 수 있을 것으로 기대된다.<br/><br/>국내 제약·바이오 업계에서는 miRNA를 활용한 신약 개발 시도가 이어지고 있다. SK바이오팜은 miRNA 기술을 접목한 뇌전증 신약 후보물질을 발굴하고 있다. 이를 위해 2022년부터 뇌질환 치료제 개발 업체인 바이오오케스트라와 협력을 체결하 고 공동연구개발(R&amp;D)을 진행 중이다.<br/><br/>넥스턴바이오가 지분 50%를 보유한 자회사 로스비보 테라퓨틱스(RosVivo Therapeutics)는 miRNA를 활용한 당뇨 치료제를 개발 중이다. 이 회사는 2021년 노보노디스크와 miRNA 기반 당뇨병 치료제 후보물질 ‘RSVI-301/302’ 공동 개발 및 기술 수출을 위한 기밀유지협약(CDA)을 체결한 바 있다.<br/><br/>암 진단 분야에서도 miRNA를 활용한 제품 개발이 속도를 내고 있다. 클 리노믹스는 누리바이오와 지난해 miRNA 기반 암 조기진단 기술을 연구 중이다. 누리바이오의 유전자 분석 플랫폼 기술 ‘프로머(Promer)’를 활용해 소량의 혈액에서 miRNA를 탐색하고, 클리노믹스의 바이오인포매틱스(BI)기술로 암을 진단하는 방식이다. 회사가 집중하고 있는 진단 분야는 폐암, 췌장암, 대장암 등 3개 암종이다.<br/><br/><img alt=\"\" src=\"https://img.etoday.co.kr/pto_db/2024/10/600/20241008160705_2086424_1200_456.png\"/><br/><br/><br/><br/>학계에서도 국내 miRNA 연구자들이 두각을 드러내고 있다. 김빛내리 기초과학연구원(IBS) RNA연구단 단장은 지난해 miRNA) 생성과 RNA 치료제의 중요한 핵심 단백질인 ‘다이서’의 3차원 구조를 초저온전자현미경(cryo-EM) 기술을 활용해 세계 최초로 규명했다. 김 단장은 2016년에는 miRNA 생성에 관여하는 ‘드로셔’ 단백질의 3차원 구조를 세계 최초 규명하는 등 국내외에서 RNA 연구 권위자로 꼽히고 있다.<br/><br/>김경미 삼성서울병원 맞춤진단연구센터장(병리과 교수)은 “miRNA가 유전자의 발현에 영향을 미친다는 사실이 밝혀진 이후 해당 분야를 연구하여 임상에 적용하려는 노력이 이어지고 있다”라며 “아직 임상적으로 이를 활용한 치료제 등이 적용된 사례는 없지만, 보조 바이오마커로서 충분히 가능성이 있다고 판단한다”라고 설명했다.<br/><br/>김 교수는 “실제로 삼성서울병원이 2018년에 발표한 위암 관련 연구에 따르면, 조기 위암에서만 132개의 miRNA가 확인됐다”라며 “miRNA가 임상적으로 어떤 의미가 있는지는 아 직 전 세계 연구자들이 연구 중인 단계이며, 병을 더욱 세밀하게 진단하고 치료하는데 보탬이 될 것으로 기대한다”라고 내다봤다.<br/><br/>한편 노벨위원회는 지난해에도 RNA 연구자들을 노벨 생리의학상 수상자로 선정한 바 있다. 지난해 수상자인 카탈린 커리코(Katalin Kariko) 바이오엔테크 수석 부사장과 드류 와이즈만(Drew Weissman) 미국 펜실베이니아대 의대 교수는 mRNA가 면역 체계와 어떻게 상호 작용하는지에 대한 그간의 이해를 근본적으로 바꾸는 획기적 발견을 이뤄내, 코로나19 대유행 기간 효과적인 백신을 개발하는데 기여한 공로를 인정받았다.<br/><br/>한성주 기자(<a href=\"mailto:hsj@etoday.co.kr\">hsj@etoday.co.kr</a>)<br/><br/>이투데이(www.etoday.co.kr), 무단전재 및 수집, 재배포금지<br/><br/><a href='https://www.etoday.co.kr/' target='_blank'>프리미엄 경제신문 이투데이</a><|eot_id|><|start_header_id|>assistant<|end_header_id|>{'summary': '미국 생물학자 빅터 앰브로스와 개리 러브컨이 마이크로 RNA(miRNA)를 발견하여 올해 노벨생리의학상을 수상하면서, miRNA에 대한 관심이 높아지고 있습니다. miRNA는 유전자 발현을 조절할 수 있는 가능성을 가지고 있어 암 진단과 난치병 치료 분야에서 유망한 기술로 주목받고 있습니다. 현재 국내외 제약사와 연구기관들이 이 기술을 활용한 신약 개발과 진단 기술 연구에 활발히 참여하고 있습니다.', 'advr_tp': '0','stk_code': ['326030', '352770'],'sent_score': 1}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13922, 1743, 1232, 364, 57139, 100654, 48918, 101438, 100508, 26799, 102558, 227, 34961, 24814, 108, 102914, 17835, 25941, 81673, 74623, 29102, 116849, 102914, 125007, 13094, 96677, 120758, 17835, 41214, 85672, 31820, 124338, 121712, 83290, 104350, 34983, 102058, 103542, 77535, 119626, 100508, 114542, 121218, 116429, 11, 9686, 31820, 19954, 102597, 125718, 13094, 108499, 54059, 119073, 103924, 13, 9686, 31820, 16969, 101003, 121737, 97096, 102335, 18359, 66610, 104834, 48936, 29833, 65621, 96451, 111490, 120693, 112795, 117452, 102464, 101353, 54780, 109250, 60798, 105297, 106467, 64356, 127290, 57575, 101003, 105115, 24486, 113094, 17835, 56773, 88708, 107094, 35495, 103924, 13, 111530, 120380, 104065, 63171, 103168, 56154, 81673, 106024, 120072, 102823, 23955, 113094, 18359, 120952, 24486, 101327, 103168, 111426, 54780, 102464, 101353, 113094, 106024, 19954, 106249, 102133, 101709, 116768, 101360, 103924, 16045, 364, 14625, 81, 56291, 1232, 364, 15, 518, 364, 76409, 4229, 1232, 2570, 17470, 14649, 518, 364, 16482, 20772, 4181, 364, 25526, 10622, 1232, 220, 16, 92, 128009]\n",
      "labels:\n",
      "{'summary': '미국 생물학자 빅터 앰브로스와 개리 러브컨이 마이크로 RNA(miRNA)를 발견하여 올해 노벨생리의학상을 수상하면서, miRNA에 대한 관심이 높아지고 있습니다. miRNA는 유전자 발현을 조절할 수 있는 가능성을 가지고 있어 암 진단과 난치병 치료 분야에서 유망한 기술로 주목받고 있습니다. 현재 국내외 제약사와 연구기관들이 이 기술을 활용한 신약 개발과 진단 기술 연구에 활발히 참여하고 있습니다.', 'advr_tp': '0','stk_code': ['326030', '352770'],'sent_score': 1}<|eot_id|>\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:29:42,660 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:29:42,662 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "model.safetensors.index.json: 100%|████████| 20.9k/20.9k [00:00<00:00, 11.6MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s][INFO|modeling_utils.py:3937] 2025-02-19 07:29:43,068 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 10.5M/4.97G [00:00<02:08, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 21.0M/4.97G [00:00<02:00, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 31.5M/4.97G [00:00<02:07, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 41.9M/4.97G [00:01<01:58, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 52.4M/4.97G [00:01<01:52, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 62.9M/4.97G [00:01<01:53, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 73.4M/4.97G [00:01<01:53, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 83.9M/4.97G [00:01<01:53, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 94.4M/4.97G [00:02<01:54, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 105M/4.97G [00:02<01:53, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 115M/4.97G [00:02<01:52, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 126M/4.97G [00:02<01:53, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 136M/4.97G [00:03<01:52, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 147M/4.97G [00:03<01:52, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 157M/4.97G [00:03<01:51, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 168M/4.97G [00:04<02:10, 36.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 178M/4.97G [00:04<01:53, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 189M/4.97G [00:04<01:53, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 199M/4.97G [00:04<01:52, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 210M/4.97G [00:04<01:52, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 220M/4.97G [00:05<01:52, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 231M/4.97G [00:05<01:51, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 241M/4.97G [00:05<01:51, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 252M/4.97G [00:05<01:50, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 262M/4.97G [00:06<01:50, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 273M/4.97G [00:06<01:50, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 283M/4.97G [00:06<01:52, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 294M/4.97G [00:06<01:51, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 304M/4.97G [00:07<01:50, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 315M/4.97G [00:07<01:50, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 325M/4.97G [00:07<01:50, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 336M/4.97G [00:07<01:49, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 346M/4.97G [00:08<01:50, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 357M/4.97G [00:08<01:48, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 367M/4.97G [00:08<01:50, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 377M/4.97G [00:08<01:49, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 388M/4.97G [00:09<01:46, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 398M/4.97G [00:09<01:46, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 409M/4.97G [00:09<01:46, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 419M/4.97G [00:09<01:45, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 430M/4.97G [00:10<02:00, 37.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 440M/4.97G [00:10<01:50, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 451M/4.97G [00:10<01:49, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 461M/4.97G [00:10<01:47, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 472M/4.97G [00:11<01:46, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 482M/4.97G [00:11<01:45, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 493M/4.97G [00:11<01:46, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 503M/4.97G [00:11<01:44, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 514M/4.97G [00:12<01:44, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 524M/4.97G [00:12<01:44, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 535M/4.97G [00:12<01:44, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 545M/4.97G [00:12<01:48, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 556M/4.97G [00:13<01:44, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 566M/4.97G [00:13<01:41, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 577M/4.97G [00:13<01:41, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 587M/4.97G [00:13<01:41, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 598M/4.97G [00:14<01:48, 40.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 608M/4.97G [00:14<01:48, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 619M/4.97G [00:14<01:49, 39.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 629M/4.97G [00:15<01:54, 37.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 640M/4.97G [00:15<01:54, 37.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 650M/4.97G [00:15<01:53, 38.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 661M/4.97G [00:15<01:49, 39.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 671M/4.97G [00:16<01:46, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 682M/4.97G [00:16<01:44, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 692M/4.97G [00:16<01:42, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 703M/4.97G [00:16<01:41, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 713M/4.97G [00:17<01:40, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 724M/4.97G [00:17<01:40, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 734M/4.97G [00:17<01:39, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 744M/4.97G [00:17<01:39, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 755M/4.97G [00:18<01:39, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 765M/4.97G [00:18<01:38, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 776M/4.97G [00:18<01:39, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 786M/4.97G [00:18<01:38, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 797M/4.97G [00:19<01:37, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 807M/4.97G [00:19<01:37, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 818M/4.97G [00:19<01:37, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 828M/4.97G [00:19<01:37, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 839M/4.97G [00:20<01:37, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 849M/4.97G [00:20<01:36, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 860M/4.97G [00:20<01:37, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 870M/4.97G [00:20<01:36, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 881M/4.97G [00:21<01:36, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 891M/4.97G [00:21<01:35, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 902M/4.97G [00:21<01:35, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 912M/4.97G [00:21<01:35, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 923M/4.97G [00:22<01:35, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 933M/4.97G [00:22<01:34, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 944M/4.97G [00:22<01:33, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 954M/4.97G [00:22<01:33, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 965M/4.97G [00:23<01:33, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 975M/4.97G [00:23<01:32, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 986M/4.97G [00:23<01:33, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█    | 996M/4.97G [00:23<01:32, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 1.01G/4.97G [00:23<01:32, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 1.02G/4.97G [00:24<01:31, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 1.03G/4.97G [00:24<01:32, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 1.04G/4.97G [00:24<01:31, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 1.05G/4.97G [00:24<01:32, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 1.06G/4.97G [00:25<01:31, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▊   | 1.07G/4.97G [00:25<01:31, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▊   | 1.08G/4.97G [00:25<01:31, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 1.09G/4.97G [00:25<01:31, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 1.10G/4.97G [00:26<01:31, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 1.11G/4.97G [00:26<01:31, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.12G/4.97G [00:26<01:30, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.13G/4.97G [00:26<01:30, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.14G/4.97G [00:27<01:30, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.15G/4.97G [00:27<01:30, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.16G/4.97G [00:27<01:29, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.17G/4.97G [00:27<01:29, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.18G/4.97G [00:28<01:29, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.20G/4.97G [00:28<01:28, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.21G/4.97G [00:28<01:28, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.22G/4.97G [00:28<01:28, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|▉   | 1.23G/4.97G [00:29<01:27, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|▉   | 1.24G/4.97G [00:29<01:50, 33.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█   | 1.26G/4.97G [00:29<01:21, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.27G/4.97G [00:30<01:23, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.28G/4.97G [00:30<01:24, 43.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.29G/4.97G [00:30<01:24, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.30G/4.97G [00:30<01:24, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.31G/4.97G [00:31<01:24, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.32G/4.97G [00:31<01:25, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.33G/4.97G [00:31<01:24, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.34G/4.97G [00:31<01:24, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.35G/4.97G [00:32<01:24, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.36G/4.97G [00:32<01:23, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 1.37G/4.97G [00:32<01:23, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 1.38G/4.97G [00:32<01:23, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 1.39G/4.97G [00:33<01:23, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▏  | 1.41G/4.97G [00:33<01:22, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.42G/4.97G [00:33<01:22, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.43G/4.97G [00:33<01:22, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.44G/4.97G [00:34<01:22, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.45G/4.97G [00:34<01:22, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.46G/4.97G [00:34<01:22, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.47G/4.97G [00:34<01:22, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.48G/4.97G [00:35<01:21, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.49G/4.97G [00:35<01:21, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.50G/4.97G [00:35<01:20, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.51G/4.97G [00:35<01:20, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 1.52G/4.97G [00:36<01:20, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 1.53G/4.97G [00:36<01:20, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 1.54G/4.97G [00:36<01:20, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▎  | 1.55G/4.97G [00:36<01:20, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▎  | 1.56G/4.97G [00:37<01:19, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 1.57G/4.97G [00:37<01:19, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 1.58G/4.97G [00:37<01:19, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 1.59G/4.97G [00:37<01:20, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 1.60G/4.97G [00:38<01:19, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 1.61G/4.97G [00:38<01:19, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 1.63G/4.97G [00:38<01:18, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 1.64G/4.97G [00:38<01:19, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 1.65G/4.97G [00:39<01:18, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 1.66G/4.97G [00:39<01:17, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 1.67G/4.97G [00:39<01:17, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 1.68G/4.97G [00:39<01:19, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 1.69G/4.97G [00:40<01:18, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 1.70G/4.97G [00:40<01:17, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▍  | 1.71G/4.97G [00:40<01:16, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 1.72G/4.97G [00:40<01:18, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 1.73G/4.97G [00:41<01:17, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 1.74G/4.97G [00:41<01:16, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 1.75G/4.97G [00:41<01:25, 37.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 1.76G/4.97G [00:41<01:14, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 1.77G/4.97G [00:42<01:13, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 1.78G/4.97G [00:42<01:13, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 1.79G/4.97G [00:42<01:13, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 1.80G/4.97G [00:42<01:16, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 1.81G/4.97G [00:43<01:15, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 1.82G/4.97G [00:43<01:14, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 1.84G/4.97G [00:43<01:14, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 1.85G/4.97G [00:43<01:15, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 1.86G/4.97G [00:44<01:14, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 1.87G/4.97G [00:44<01:13, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 1.88G/4.97G [00:44<01:13, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 1.89G/4.97G [00:44<01:14, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 1.90G/4.97G [00:45<01:12, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 1.91G/4.97G [00:45<01:12, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 1.92G/4.97G [00:45<01:11, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 1.93G/4.97G [00:45<01:13, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 1.94G/4.97G [00:46<01:12, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 1.95G/4.97G [00:46<01:11, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 1.96G/4.97G [00:46<01:10, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 1.97G/4.97G [00:46<01:12, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 1.98G/4.97G [00:47<01:11, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 1.99G/4.97G [00:47<01:11, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 2.00G/4.97G [00:47<01:10, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▌  | 2.01G/4.97G [00:47<01:11, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 2.02G/4.97G [00:48<01:10, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 2.03G/4.97G [00:48<01:09, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 2.04G/4.97G [00:48<01:08, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 2.06G/4.97G [00:48<01:10, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 2.07G/4.97G [00:49<01:09, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 2.08G/4.97G [00:49<01:08, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 2.09G/4.97G [00:49<01:08, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 2.10G/4.97G [00:49<01:09, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 2.11G/4.97G [00:50<01:08, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 2.12G/4.97G [00:50<01:07, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 2.13G/4.97G [00:50<01:07, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 2.14G/4.97G [00:50<01:08, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 2.15G/4.97G [00:51<01:07, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 2.16G/4.97G [00:51<01:06, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▋  | 2.17G/4.97G [00:51<01:06, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 2.18G/4.97G [00:51<01:07, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 2.19G/4.97G [00:52<01:05, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 2.20G/4.97G [00:52<01:05, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 2.21G/4.97G [00:52<01:05, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 2.22G/4.97G [00:52<01:06, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 2.23G/4.97G [00:53<01:05, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 2.24G/4.97G [00:53<01:04, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 2.25G/4.97G [00:53<01:03, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 2.26G/4.97G [00:53<01:05, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 2.28G/4.97G [00:54<01:04, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 2.29G/4.97G [00:54<01:03, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 2.30G/4.97G [00:54<01:03, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 2.31G/4.97G [00:54<01:04, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▊  | 2.32G/4.97G [00:55<01:03, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 2.33G/4.97G [00:55<01:02, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 2.34G/4.97G [00:55<01:02, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 2.35G/4.97G [00:55<01:03, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 2.36G/4.97G [00:56<01:01, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 2.37G/4.97G [00:56<01:01, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 2.38G/4.97G [00:56<01:01, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 2.39G/4.97G [00:56<01:02, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 2.40G/4.97G [00:57<01:01, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 2.41G/4.97G [00:57<01:00, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 2.42G/4.97G [00:57<00:59, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 2.43G/4.97G [00:57<01:01, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 2.44G/4.97G [00:58<01:00, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 2.45G/4.97G [00:58<00:59, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█▉  | 2.46G/4.97G [00:58<00:59, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█▉  | 2.47G/4.97G [00:58<01:00, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 2.49G/4.97G [00:59<00:59, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 2.50G/4.97G [00:59<00:58, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 2.51G/4.97G [00:59<00:58, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 2.52G/4.97G [00:59<00:59, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 2.53G/4.97G [01:00<00:58, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 2.54G/4.97G [01:00<00:57, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 2.55G/4.97G [01:00<00:57, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 2.56G/4.97G [01:00<00:58, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 2.57G/4.97G [01:01<00:57, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 2.58G/4.97G [01:01<00:56, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 2.59G/4.97G [01:01<00:56, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 2.60G/4.97G [01:01<00:57, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 2.61G/4.97G [01:02<00:56, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 2.62G/4.97G [01:02<00:55, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 2.63G/4.97G [01:02<00:54, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▏ | 2.64G/4.97G [01:02<00:56, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▏ | 2.65G/4.97G [01:03<01:04, 35.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 2.66G/4.97G [01:03<00:51, 44.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 2.67G/4.97G [01:03<00:52, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 2.68G/4.97G [01:03<00:53, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 2.69G/4.97G [01:04<00:53, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 2.71G/4.97G [01:04<00:52, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 2.72G/4.97G [01:04<00:52, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 2.73G/4.97G [01:04<00:53, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 2.74G/4.97G [01:05<00:52, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 2.75G/4.97G [01:05<00:52, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 2.76G/4.97G [01:05<00:51, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 2.77G/4.97G [01:05<00:53, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 2.78G/4.97G [01:06<00:52, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 2.79G/4.97G [01:06<00:51, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▎ | 2.80G/4.97G [01:06<00:51, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 2.81G/4.97G [01:06<00:52, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 2.82G/4.97G [01:07<00:51, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 2.83G/4.97G [01:07<00:52, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 2.84G/4.97G [01:07<00:52, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 2.85G/4.97G [01:07<00:51, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 2.86G/4.97G [01:08<00:53, 39.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 2.87G/4.97G [01:08<00:51, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 2.88G/4.97G [01:08<00:50, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 2.89G/4.97G [01:08<00:49, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 2.90G/4.97G [01:09<00:49, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 2.92G/4.97G [01:09<00:48, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 2.93G/4.97G [01:09<00:48, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 2.94G/4.97G [01:09<00:48, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 2.95G/4.97G [01:10<00:47, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 2.96G/4.97G [01:10<00:47, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 2.97G/4.97G [01:10<00:47, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 2.98G/4.97G [01:10<00:46, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 2.99G/4.97G [01:11<00:46, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 3.00G/4.97G [01:11<00:46, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 3.01G/4.97G [01:11<00:45, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 3.02G/4.97G [01:11<00:45, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 3.03G/4.97G [01:12<00:45, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 3.04G/4.97G [01:12<00:45, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 3.05G/4.97G [01:12<00:45, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 3.06G/4.97G [01:12<00:44, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 3.07G/4.97G [01:13<00:44, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 3.08G/4.97G [01:13<00:44, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 3.09G/4.97G [01:13<00:43, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 3.10G/4.97G [01:13<00:44, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 3.11G/4.97G [01:14<00:43, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 3.12G/4.97G [01:14<00:43, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 3.14G/4.97G [01:14<00:44, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 3.15G/4.97G [01:14<00:43, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 3.16G/4.97G [01:15<00:42, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 3.17G/4.97G [01:15<00:42, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 3.18G/4.97G [01:15<00:42, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 3.19G/4.97G [01:15<00:42, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 3.20G/4.97G [01:16<00:41, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 3.21G/4.97G [01:16<00:41, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 3.22G/4.97G [01:16<00:41, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 3.23G/4.97G [01:16<00:41, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 3.24G/4.97G [01:17<00:40, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 3.25G/4.97G [01:17<00:41, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 3.26G/4.97G [01:17<00:40, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 3.27G/4.97G [01:17<00:40, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 3.28G/4.97G [01:18<00:39, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 3.29G/4.97G [01:18<00:39, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 3.30G/4.97G [01:18<00:39, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 3.31G/4.97G [01:18<00:39, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 3.32G/4.97G [01:19<00:39, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 3.33G/4.97G [01:19<00:38, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 3.34G/4.97G [01:19<00:38, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 3.36G/4.97G [01:19<00:38, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 3.37G/4.97G [01:20<00:38, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 3.38G/4.97G [01:20<00:37, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 3.39G/4.97G [01:20<00:37, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 3.40G/4.97G [01:20<00:37, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▋ | 3.41G/4.97G [01:21<00:37, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 3.42G/4.97G [01:21<00:36, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 3.43G/4.97G [01:21<00:36, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 3.44G/4.97G [01:21<00:36, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 3.45G/4.97G [01:22<00:36, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 3.46G/4.97G [01:22<00:35, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 3.47G/4.97G [01:22<00:35, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 3.48G/4.97G [01:22<00:35, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 3.49G/4.97G [01:23<00:35, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 3.50G/4.97G [01:23<00:34, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 3.51G/4.97G [01:23<00:34, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 3.52G/4.97G [01:23<00:34, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 3.53G/4.97G [01:24<00:34, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 3.54G/4.97G [01:24<00:33, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▊ | 3.55G/4.97G [01:24<00:33, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▊ | 3.57G/4.97G [01:24<00:33, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 3.58G/4.97G [01:25<00:33, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 3.59G/4.97G [01:25<00:32, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 3.60G/4.97G [01:25<00:32, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 3.61G/4.97G [01:25<00:32, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 3.62G/4.97G [01:26<00:32, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 3.63G/4.97G [01:26<00:31, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 3.64G/4.97G [01:26<00:31, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 3.65G/4.97G [01:26<00:31, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 3.66G/4.97G [01:27<00:31, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 3.67G/4.97G [01:27<00:30, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 3.68G/4.97G [01:27<00:31, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 3.69G/4.97G [01:27<00:30, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 3.70G/4.97G [01:28<00:29, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 3.71G/4.97G [01:28<00:29, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 3.72G/4.97G [01:28<00:29, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███ | 3.73G/4.97G [01:28<00:29, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███ | 3.74G/4.97G [01:29<00:29, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 3.75G/4.97G [01:29<00:28, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 3.76G/4.97G [01:29<00:28, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 3.77G/4.97G [01:29<00:28, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 3.79G/4.97G [01:30<00:28, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 3.80G/4.97G [01:30<00:27, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 3.81G/4.97G [01:30<00:27, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 3.82G/4.97G [01:30<00:27, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 3.83G/4.97G [01:31<00:27, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 3.84G/4.97G [01:31<00:29, 38.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 3.85G/4.97G [01:31<00:25, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 3.86G/4.97G [01:31<00:26, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 3.87G/4.97G [01:32<00:25, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▏| 3.88G/4.97G [01:32<00:25, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▏| 3.89G/4.97G [01:32<00:25, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 3.90G/4.97G [01:32<00:25, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 3.91G/4.97G [01:33<00:25, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 3.92G/4.97G [01:33<00:24, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 3.93G/4.97G [01:33<00:24, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 3.94G/4.97G [01:33<00:24, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 3.95G/4.97G [01:34<00:24, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 3.96G/4.97G [01:34<00:23, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 3.97G/4.97G [01:34<00:23, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 3.98G/4.97G [01:34<00:23, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 4.00G/4.97G [01:34<00:21, 45.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 4.02G/4.97G [01:35<00:17, 54.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 4.03G/4.97G [01:35<00:24, 38.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▎| 4.04G/4.97G [01:36<00:23, 39.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 4.05G/4.97G [01:36<00:22, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 4.06G/4.97G [01:36<00:22, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 4.07G/4.97G [01:36<00:22, 40.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 4.08G/4.97G [01:37<00:21, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 4.09G/4.97G [01:37<00:21, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 4.10G/4.97G [01:37<00:21, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 4.11G/4.97G [01:37<00:20, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 4.12G/4.97G [01:38<00:20, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 4.13G/4.97G [01:38<00:19, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 4.14G/4.97G [01:38<00:19, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 4.15G/4.97G [01:38<00:19, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 4.16G/4.97G [01:39<00:19, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 4.17G/4.97G [01:39<00:18, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 4.18G/4.97G [01:39<00:18, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▍| 4.19G/4.97G [01:39<00:18, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 4.20G/4.97G [01:40<00:18, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 4.22G/4.97G [01:40<00:17, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 4.23G/4.97G [01:40<00:17, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 4.24G/4.97G [01:40<00:17, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 4.25G/4.97G [01:41<00:17, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 4.26G/4.97G [01:41<00:16, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 4.27G/4.97G [01:41<00:16, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 4.28G/4.97G [01:41<00:16, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 4.29G/4.97G [01:42<00:16, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 4.30G/4.97G [01:42<00:15, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 4.31G/4.97G [01:42<00:15, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 4.32G/4.97G [01:42<00:15, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 4.33G/4.97G [01:43<00:15, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 4.34G/4.97G [01:43<00:14, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 4.35G/4.97G [01:43<00:14, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 4.36G/4.97G [01:43<00:14, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 4.37G/4.97G [01:44<00:14, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 4.38G/4.97G [01:44<00:13, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 4.39G/4.97G [01:44<00:13, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 4.40G/4.97G [01:44<00:13, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 4.41G/4.97G [01:45<00:16, 33.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 4.44G/4.97G [01:45<00:11, 45.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 4.45G/4.97G [01:45<00:11, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 4.46G/4.97G [01:46<00:11, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 4.47G/4.97G [01:46<00:11, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 4.48G/4.97G [01:46<00:11, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 4.49G/4.97G [01:46<00:11, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▌| 4.50G/4.97G [01:47<00:11, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 4.51G/4.97G [01:47<00:10, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 4.52G/4.97G [01:47<00:10, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 4.53G/4.97G [01:47<00:10, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 4.54G/4.97G [01:48<00:10, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 4.55G/4.97G [01:48<00:09, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 4.56G/4.97G [01:48<00:09, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 4.57G/4.97G [01:48<00:09, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 4.58G/4.97G [01:49<00:09, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 4.59G/4.97G [01:49<00:08, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 4.60G/4.97G [01:49<00:08, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 4.61G/4.97G [01:49<00:08, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 4.62G/4.97G [01:50<00:08, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 4.63G/4.97G [01:50<00:07, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▋| 4.65G/4.97G [01:50<00:07, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 4.66G/4.97G [01:50<00:07, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 4.67G/4.97G [01:51<00:07, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 4.68G/4.97G [01:51<00:06, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 4.69G/4.97G [01:51<00:06, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 4.70G/4.97G [01:51<00:06, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 4.71G/4.97G [01:52<00:06, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 4.72G/4.97G [01:52<00:05, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 4.73G/4.97G [01:52<00:05, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 4.74G/4.97G [01:52<00:05, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 4.75G/4.97G [01:53<00:05, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 4.76G/4.97G [01:53<00:04, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 4.77G/4.97G [01:53<00:04, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 4.78G/4.97G [01:53<00:04, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 4.79G/4.97G [01:54<00:04, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▊| 4.80G/4.97G [01:54<00:03, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 4.81G/4.97G [01:54<00:03, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 4.82G/4.97G [01:54<00:03, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 4.83G/4.97G [01:55<00:03, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 4.84G/4.97G [01:55<00:02, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 4.85G/4.97G [01:55<00:02, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 4.87G/4.97G [01:55<00:02, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 4.88G/4.97G [01:56<00:02, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 4.89G/4.97G [01:56<00:01, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 4.90G/4.97G [01:56<00:01, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 4.91G/4.97G [01:56<00:01, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 4.92G/4.97G [01:57<00:01, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 4.93G/4.97G [01:57<00:00, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 4.94G/4.97G [01:57<00:00, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|███▉| 4.95G/4.97G [01:57<00:00, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|███▉| 4.96G/4.97G [01:58<00:00, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|████| 4.97G/4.97G [01:58<00:00, 42.0MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████            | 1/2 [01:58<01:58, 118.37s/it]\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/1.46G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 10.5M/1.46G [00:00<00:33, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 21.0M/1.46G [00:00<00:33, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 31.5M/1.46G [00:00<00:33, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|    | 41.9M/1.46G [00:00<00:33, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏   | 52.4M/1.46G [00:01<00:33, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏   | 62.9M/1.46G [00:01<00:32, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▏   | 73.4M/1.46G [00:01<00:32, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▏   | 83.9M/1.46G [00:01<00:32, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎   | 94.4M/1.46G [00:02<00:31, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 105M/1.46G [00:02<00:33, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍    | 115M/1.46G [00:02<00:32, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 126M/1.46G [00:02<00:30, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 136M/1.46G [00:03<00:30, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▌    | 147M/1.46G [00:03<00:30, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 157M/1.46G [00:03<00:30, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 168M/1.46G [00:03<00:30, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▌    | 178M/1.46G [00:04<00:29, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 189M/1.46G [00:04<00:29, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 199M/1.46G [00:04<00:29, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 210M/1.46G [00:04<00:29, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▊    | 220M/1.46G [00:05<00:28, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 231M/1.46G [00:05<00:28, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▊    | 241M/1.46G [00:05<00:28, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▊    | 252M/1.46G [00:05<00:28, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 262M/1.46G [00:06<00:27, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 273M/1.46G [00:06<00:32, 36.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 283M/1.46G [00:06<00:26, 45.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█    | 294M/1.46G [00:06<00:26, 44.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█    | 304M/1.46G [00:07<00:26, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 315M/1.46G [00:07<00:26, 43.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 325M/1.46G [00:07<00:26, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 336M/1.46G [00:07<00:26, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 346M/1.46G [00:08<00:25, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 357M/1.46G [00:08<00:25, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▎   | 367M/1.46G [00:08<00:25, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 377M/1.46G [00:08<00:25, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 388M/1.46G [00:09<00:25, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 398M/1.46G [00:09<00:24, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 409M/1.46G [00:09<00:26, 39.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 419M/1.46G [00:09<00:25, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 430M/1.46G [00:10<00:25, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▌   | 440M/1.46G [00:10<00:24, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 451M/1.46G [00:10<00:24, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 461M/1.46G [00:10<00:23, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 472M/1.46G [00:11<00:23, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 482M/1.46G [00:11<00:23, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 493M/1.46G [00:11<00:22, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 503M/1.46G [00:11<00:22, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▊   | 514M/1.46G [00:12<00:22, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▊   | 524M/1.46G [00:12<00:21, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 535M/1.46G [00:12<00:21, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 545M/1.46G [00:12<00:21, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▉   | 556M/1.46G [00:13<00:21, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▉   | 566M/1.46G [00:13<00:20, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▉   | 577M/1.46G [00:13<00:20, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|██   | 587M/1.46G [00:13<00:20, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|██   | 598M/1.46G [00:14<00:20, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|██   | 608M/1.46G [00:14<00:19, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|██   | 619M/1.46G [00:14<00:19, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|██▏  | 629M/1.46G [00:14<00:19, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|██▏  | 640M/1.46G [00:15<00:19, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|██▏  | 650M/1.46G [00:15<00:19, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|██▎  | 661M/1.46G [00:15<00:18, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|██▎  | 671M/1.46G [00:15<00:18, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|██▎  | 682M/1.46G [00:16<00:18, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|██▎  | 692M/1.46G [00:16<00:18, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|██▍  | 703M/1.46G [00:16<00:17, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|██▍  | 713M/1.46G [00:16<00:17, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██▍  | 724M/1.46G [00:17<00:17, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██▌  | 734M/1.46G [00:17<00:17, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██▌  | 744M/1.46G [00:17<00:16, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██▌  | 755M/1.46G [00:17<00:16, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██▌  | 765M/1.46G [00:17<00:16, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██▋  | 776M/1.46G [00:18<00:16, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▋  | 786M/1.46G [00:18<00:15, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▋  | 797M/1.46G [00:18<00:15, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▊  | 807M/1.46G [00:18<00:15, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▊  | 818M/1.46G [00:19<00:15, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▊  | 828M/1.46G [00:19<00:14, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▊  | 839M/1.46G [00:19<00:14, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▉  | 849M/1.46G [00:19<00:14, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▉  | 860M/1.46G [00:20<00:13, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▉  | 870M/1.46G [00:20<00:13, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|███  | 881M/1.46G [00:20<00:13, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|███  | 891M/1.46G [00:20<00:13, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|███  | 902M/1.46G [00:21<00:13, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|███  | 912M/1.46G [00:21<00:12, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|███▏ | 923M/1.46G [00:21<00:12, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|███▏ | 933M/1.46G [00:21<00:12, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|███▏ | 944M/1.46G [00:22<00:12, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|███▎ | 954M/1.46G [00:22<00:12, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|███▎ | 965M/1.46G [00:22<00:11, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|███▎ | 975M/1.46G [00:22<00:11, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|███▍ | 986M/1.46G [00:23<00:12, 36.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|███▍ | 996M/1.46G [00:23<00:10, 44.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▊ | 1.01G/1.46G [00:23<00:10, 44.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|██▊ | 1.02G/1.46G [00:23<00:10, 43.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|██▊ | 1.03G/1.46G [00:24<00:09, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|██▊ | 1.04G/1.46G [00:24<00:09, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|██▊ | 1.05G/1.46G [00:24<00:09, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|██▉ | 1.06G/1.46G [00:24<00:09, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|██▉ | 1.07G/1.46G [00:25<00:09, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|██▉ | 1.08G/1.46G [00:25<00:08, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|██▉ | 1.09G/1.46G [00:25<00:08, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|███ | 1.10G/1.46G [00:25<00:08, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███ | 1.11G/1.46G [00:26<00:08, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███ | 1.12G/1.46G [00:26<00:07, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███ | 1.13G/1.46G [00:26<00:07, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███▏| 1.14G/1.46G [00:26<00:07, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▏| 1.15G/1.46G [00:27<00:07, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███▏| 1.16G/1.46G [00:27<00:06, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███▏| 1.17G/1.46G [00:27<00:06, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|███▏| 1.18G/1.46G [00:27<00:06, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|███▎| 1.20G/1.46G [00:28<00:06, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|███▎| 1.21G/1.46G [00:28<00:05, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|███▎| 1.22G/1.46G [00:28<00:05, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|███▎| 1.23G/1.46G [00:28<00:05, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|███▍| 1.24G/1.46G [00:29<00:05, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|███▍| 1.25G/1.46G [00:29<00:04, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|███▍| 1.26G/1.46G [00:29<00:04, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|███▍| 1.27G/1.46G [00:29<00:04, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|███▌| 1.28G/1.46G [00:30<00:04, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|███▌| 1.29G/1.46G [00:30<00:03, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|███▌| 1.30G/1.46G [00:30<00:03, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|███▌| 1.31G/1.46G [00:30<00:03, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|███▌| 1.32G/1.46G [00:31<00:03, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|███▋| 1.33G/1.46G [00:31<00:02, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|███▋| 1.34G/1.46G [00:31<00:02, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|███▋| 1.35G/1.46G [00:31<00:02, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|███▋| 1.36G/1.46G [00:32<00:02, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|███▊| 1.37G/1.46G [00:32<00:02, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|███▊| 1.38G/1.46G [00:32<00:01, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|███▊| 1.39G/1.46G [00:32<00:01, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|███▊| 1.41G/1.46G [00:32<00:01, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|███▉| 1.42G/1.46G [00:33<00:01, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|███▉| 1.43G/1.46G [00:33<00:00, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|███▉| 1.44G/1.46G [00:33<00:00, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|███▉| 1.45G/1.46G [00:33<00:00, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|████| 1.46G/1.46G [00:34<00:00, 42.6MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [02:32<00:00, 76.38s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [02:32<00:00, 76.39s/it]\n",
      "[INFO|modeling_utils.py:1670] 2025-02-19 07:32:15,845 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2025-02-19 07:32:15,857 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.15it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "[INFO|modeling_utils.py:4800] 2025-02-19 07:32:17,705 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2025-02-19 07:32:17,705 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "generation_config.json: 100%|██████████████████| 189/189 [00:00<00:00, 2.39MB/s]\n",
      "[INFO|configuration_utils.py:1051] 2025-02-19 07:32:17,840 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2025-02-19 07:32:17,841 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-02-19 07:32:17] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-02-19 07:32:17] llamafactory.model.model_utils.attention:157 >> Using vanilla attention implementation.\n",
      "[INFO|2025-02-19 07:32:17] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-02-19 07:32:17] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-02-19 07:32:17] llamafactory.model.model_utils.misc:157 >> Found linear modules: up_proj,gate_proj,v_proj,q_proj,o_proj,down_proj,k_proj\n",
      "[INFO|2025-02-19 07:32:18] llamafactory.model.loader:157 >> trainable params: 12,156,928 || all params: 3,224,906,752 || trainable%: 0.3770\n",
      "[INFO|trainer.py:698] 2025-02-19 07:32:18,130 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2313] 2025-02-19 07:32:18,389 >> ***** Running training *****\n",
      "[INFO|trainer.py:2314] 2025-02-19 07:32:18,389 >>   Num examples = 450\n",
      "[INFO|trainer.py:2315] 2025-02-19 07:32:18,389 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:2316] 2025-02-19 07:32:18,389 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2319] 2025-02-19 07:32:18,389 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2320] 2025-02-19 07:32:18,389 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2321] 2025-02-19 07:32:18,389 >>   Total optimization steps = 1,120\n",
      "[INFO|trainer.py:2322] 2025-02-19 07:32:18,392 >>   Number of trainable parameters = 12,156,928\n",
      "{'loss': 1.6241, 'grad_norm': 1.1456103324890137, 'learning_rate': 8.92857142857143e-06, 'epoch': 0.18}\n",
      "{'loss': 1.5584, 'grad_norm': 1.0913846492767334, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.36}\n",
      "{'loss': 1.4519, 'grad_norm': 0.914071798324585, 'learning_rate': 2.6785714285714288e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3215, 'grad_norm': 0.7808189988136292, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.71}\n",
      "{'loss': 1.2228, 'grad_norm': 1.0093995332717896, 'learning_rate': 4.464285714285715e-05, 'epoch': 0.89}\n",
      "{'loss': 1.2371, 'grad_norm': 0.8985193371772766, 'learning_rate': 5.3571428571428575e-05, 'epoch': 1.07}\n",
      "{'loss': 1.0791, 'grad_norm': 1.1291964054107666, 'learning_rate': 6.25e-05, 'epoch': 1.24}\n",
      "{'loss': 1.0289, 'grad_norm': 0.9593020677566528, 'learning_rate': 7.142857142857143e-05, 'epoch': 1.42}\n",
      "{'loss': 1.0398, 'grad_norm': 1.1215709447860718, 'learning_rate': 8.035714285714287e-05, 'epoch': 1.6}\n",
      "{'loss': 0.9908, 'grad_norm': 1.3353396654129028, 'learning_rate': 8.92857142857143e-05, 'epoch': 1.78}\n",
      "  9%|███▌                                    | 100/1120 [02:51<28:31,  1.68s/it][INFO|trainer.py:4117] 2025-02-19 07:35:10,210 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-02-19 07:35:10,210 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-02-19 07:35:10,210 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 11.21it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:02,  7.58it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  7.36it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  7.19it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.44it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.17it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:01,  7.03it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  6.99it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  7.10it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  6.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.24it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.85it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.83it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:02<00:00,  6.29it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  6.87it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  7.39it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  6.82it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  6.77it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.9802068471908569, 'eval_runtime': 3.5925, 'eval_samples_per_second': 13.918, 'eval_steps_per_second': 6.959, 'epoch': 1.78}\n",
      "  9%|███▌                                    | 100/1120 [02:55<28:31,  1.68s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.16it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-02-19 07:35:13,802 >> Saving model checkpoint to news_analysis/checkpoint/checkpoint-100\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:35:13,985 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:35:13,986 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-02-19 07:35:14,039 >> tokenizer config file saved in news_analysis/checkpoint/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-02-19 07:35:14,039 >> Special tokens file saved in news_analysis/checkpoint/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 1.0228, 'grad_norm': 1.1533478498458862, 'learning_rate': 9.821428571428572e-05, 'epoch': 1.96}\n",
      "{'loss': 1.0178, 'grad_norm': 0.987558901309967, 'learning_rate': 9.998445910004082e-05, 'epoch': 2.13}\n",
      "{'loss': 0.8785, 'grad_norm': 1.2676514387130737, 'learning_rate': 9.992134075089084e-05, 'epoch': 2.31}\n",
      "{'loss': 0.8813, 'grad_norm': 1.2336872816085815, 'learning_rate': 9.980973490458728e-05, 'epoch': 2.49}\n",
      "{'loss': 0.8598, 'grad_norm': 1.2968043088912964, 'learning_rate': 9.964974996142698e-05, 'epoch': 2.67}\n",
      "{'loss': 0.9081, 'grad_norm': 1.6565775871276855, 'learning_rate': 9.944154131125642e-05, 'epoch': 2.84}\n",
      "{'loss': 0.9548, 'grad_norm': 1.589822769165039, 'learning_rate': 9.918531118254507e-05, 'epoch': 3.02}\n",
      "{'loss': 0.7258, 'grad_norm': 1.40223228931427, 'learning_rate': 9.888130844596524e-05, 'epoch': 3.2}\n",
      "{'loss': 0.7268, 'grad_norm': 1.791788935661316, 'learning_rate': 9.852982837266955e-05, 'epoch': 3.38}\n",
      "{'loss': 0.6931, 'grad_norm': 1.5631234645843506, 'learning_rate': 9.81312123475006e-05, 'epoch': 3.56}\n",
      " 18%|███████▏                                | 200/1120 [05:45<25:32,  1.67s/it][INFO|trainer.py:4117] 2025-02-19 07:38:04,328 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-02-19 07:38:04,328 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-02-19 07:38:04,328 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 11.23it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.07it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:02,  7.58it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  7.36it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  7.18it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.43it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.16it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  7.00it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  6.95it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  7.07it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  6.95it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.22it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.82it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:02<00:00,  6.28it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  6.85it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  7.38it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  6.81it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  6.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.9486038684844971, 'eval_runtime': 3.599, 'eval_samples_per_second': 13.893, 'eval_steps_per_second': 6.946, 'epoch': 3.56}\n",
      " 18%|███████▏                                | 200/1120 [05:49<25:32,  1.67s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.16it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-02-19 07:38:07,927 >> Saving model checkpoint to news_analysis/checkpoint/checkpoint-200\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:38:08,088 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:38:08,089 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-02-19 07:38:08,133 >> tokenizer config file saved in news_analysis/checkpoint/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-02-19 07:38:08,134 >> Special tokens file saved in news_analysis/checkpoint/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.6851, 'grad_norm': 1.9954345226287842, 'learning_rate': 9.768584753741134e-05, 'epoch': 3.73}\n",
      "{'loss': 0.6837, 'grad_norm': 2.0085432529449463, 'learning_rate': 9.719416651541839e-05, 'epoch': 3.91}\n",
      "{'loss': 0.6928, 'grad_norm': 1.4569488763809204, 'learning_rate': 9.665664684045333e-05, 'epoch': 4.09}\n",
      "{'loss': 0.5184, 'grad_norm': 2.224841356277466, 'learning_rate': 9.607381059352038e-05, 'epoch': 4.27}\n",
      "{'loss': 0.5284, 'grad_norm': 1.7127119302749634, 'learning_rate': 9.544622387061055e-05, 'epoch': 4.44}\n",
      "{'loss': 0.544, 'grad_norm': 2.2224273681640625, 'learning_rate': 9.477449623286505e-05, 'epoch': 4.62}\n",
      "{'loss': 0.5329, 'grad_norm': 2.2552688121795654, 'learning_rate': 9.405928011452211e-05, 'epoch': 4.8}\n",
      "{'loss': 0.527, 'grad_norm': 2.2471256256103516, 'learning_rate': 9.330127018922194e-05, 'epoch': 4.98}\n",
      "{'loss': 0.4208, 'grad_norm': 1.7131680250167847, 'learning_rate': 9.250120269528546e-05, 'epoch': 5.16}\n",
      "{'loss': 0.4112, 'grad_norm': 2.444755792617798, 'learning_rate': 9.165985472062246e-05, 'epoch': 5.33}\n",
      " 27%|██████████▋                             | 300/1120 [08:43<25:14,  1.85s/it][INFO|trainer.py:4117] 2025-02-19 07:41:01,512 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-02-19 07:41:01,512 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-02-19 07:41:01,512 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 11.23it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.07it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:02,  7.58it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  7.36it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  7.18it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.43it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.17it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:01,  7.01it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  6.98it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  7.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  6.96it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.22it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.81it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:02<00:00,  6.28it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  6.85it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  7.37it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  6.79it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  6.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.1045516729354858, 'eval_runtime': 3.5989, 'eval_samples_per_second': 13.893, 'eval_steps_per_second': 6.947, 'epoch': 5.33}\n",
      " 27%|██████████▋                             | 300/1120 [08:46<25:14,  1.85s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.14it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-02-19 07:41:05,111 >> Saving model checkpoint to news_analysis/checkpoint/checkpoint-300\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:41:05,810 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:41:05,810 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-02-19 07:41:05,855 >> tokenizer config file saved in news_analysis/checkpoint/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-02-19 07:41:05,855 >> Special tokens file saved in news_analysis/checkpoint/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.4161, 'grad_norm': 2.365471124649048, 'learning_rate': 9.077804344796302e-05, 'epoch': 5.51}\n",
      "{'loss': 0.4068, 'grad_norm': 2.229360580444336, 'learning_rate': 8.985662536114613e-05, 'epoch': 5.69}\n",
      "{'loss': 0.4194, 'grad_norm': 2.3069796562194824, 'learning_rate': 8.889649541323574e-05, 'epoch': 5.87}\n",
      "{'loss': 0.3853, 'grad_norm': 1.5629088878631592, 'learning_rate': 8.789858615727265e-05, 'epoch': 6.04}\n",
      "{'loss': 0.2703, 'grad_norm': 2.4371635913848877, 'learning_rate': 8.68638668405062e-05, 'epoch': 6.22}\n",
      "{'loss': 0.3181, 'grad_norm': 2.0597705841064453, 'learning_rate': 8.579334246298593e-05, 'epoch': 6.4}\n",
      "{'loss': 0.292, 'grad_norm': 2.284719944000244, 'learning_rate': 8.468805280142709e-05, 'epoch': 6.58}\n",
      "{'loss': 0.2769, 'grad_norm': 2.338344097137451, 'learning_rate': 8.354907139929851e-05, 'epoch': 6.76}\n",
      "{'loss': 0.3012, 'grad_norm': 2.728445053100586, 'learning_rate': 8.237750452411353e-05, 'epoch': 6.93}\n",
      "{'loss': 0.2807, 'grad_norm': 1.9670052528381348, 'learning_rate': 8.117449009293668e-05, 'epoch': 7.11}\n",
      " 36%|██████████████▎                         | 400/1120 [11:38<20:03,  1.67s/it][INFO|trainer.py:4117] 2025-02-19 07:43:57,137 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-02-19 07:43:57,137 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-02-19 07:43:57,137 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 11.24it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.04it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:02,  7.56it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  7.34it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  7.16it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.42it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.16it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:01,  7.01it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  6.97it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  7.08it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  6.96it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.23it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.82it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:02<00:00,  6.29it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  6.86it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  7.38it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  6.81it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  6.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2400095462799072, 'eval_runtime': 3.598, 'eval_samples_per_second': 13.897, 'eval_steps_per_second': 6.948, 'epoch': 7.11}\n",
      " 36%|██████████████▎                         | 400/1120 [11:42<20:03,  1.67s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.16it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-02-19 07:44:00,735 >> Saving model checkpoint to news_analysis/checkpoint/checkpoint-400\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:44:00,910 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:44:00,912 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-02-19 07:44:00,988 >> tokenizer config file saved in news_analysis/checkpoint/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-02-19 07:44:00,989 >> Special tokens file saved in news_analysis/checkpoint/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.2048, 'grad_norm': 2.7691075801849365, 'learning_rate': 7.994119656715002e-05, 'epoch': 7.29}\n",
      "{'loss': 0.2127, 'grad_norm': 2.1339821815490723, 'learning_rate': 7.86788218175523e-05, 'epoch': 7.47}\n",
      "{'loss': 0.2306, 'grad_norm': 3.0073461532592773, 'learning_rate': 7.738859196089358e-05, 'epoch': 7.64}\n",
      "{'loss': 0.2131, 'grad_norm': 1.7685649394989014, 'learning_rate': 7.60717601689749e-05, 'epoch': 7.82}\n",
      "{'loss': 0.2535, 'grad_norm': 5.443517208099365, 'learning_rate': 7.472960545147038e-05, 'epoch': 8.0}\n",
      "{'loss': 0.1583, 'grad_norm': 2.3453121185302734, 'learning_rate': 7.33634314136531e-05, 'epoch': 8.18}\n",
      "{'loss': 0.1638, 'grad_norm': 2.108194589614868, 'learning_rate': 7.197456499023225e-05, 'epoch': 8.36}\n",
      "{'loss': 0.1406, 'grad_norm': 2.029937267303467, 'learning_rate': 7.056435515653059e-05, 'epoch': 8.53}\n",
      "{'loss': 0.1631, 'grad_norm': 2.2132580280303955, 'learning_rate': 6.91341716182545e-05, 'epoch': 8.71}\n",
      "{'loss': 0.1641, 'grad_norm': 2.4523563385009766, 'learning_rate': 6.768540348112907e-05, 'epoch': 8.89}\n",
      " 45%|█████████████████▊                      | 500/1120 [14:37<16:53,  1.64s/it][INFO|trainer.py:4117] 2025-02-19 07:46:55,595 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-02-19 07:46:55,595 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-02-19 07:46:55,595 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 11.22it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:02,  7.57it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  7.35it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  7.17it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.42it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.16it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:01,  7.01it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  6.98it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  7.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  6.96it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.23it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.85it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.82it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:02<00:00,  6.29it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  6.86it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  7.38it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  6.81it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  6.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3306117057800293, 'eval_runtime': 3.5988, 'eval_samples_per_second': 13.893, 'eval_steps_per_second': 6.947, 'epoch': 8.89}\n",
      " 45%|█████████████████▊                      | 500/1120 [14:40<16:53,  1.64s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.15it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-02-19 07:46:59,194 >> Saving model checkpoint to news_analysis/checkpoint/checkpoint-500\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:46:59,355 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:46:59,355 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-02-19 07:46:59,399 >> tokenizer config file saved in news_analysis/checkpoint/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-02-19 07:46:59,399 >> Special tokens file saved in news_analysis/checkpoint/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.1645, 'grad_norm': 1.793515682220459, 'learning_rate': 6.621945790169036e-05, 'epoch': 9.07}\n",
      "{'loss': 0.107, 'grad_norm': 1.5999675989151, 'learning_rate': 6.473775872054521e-05, 'epoch': 9.24}\n",
      "{'loss': 0.1149, 'grad_norm': 2.0029866695404053, 'learning_rate': 6.324174507942637e-05, 'epoch': 9.42}\n",
      "{'loss': 0.1084, 'grad_norm': 3.8015363216400146, 'learning_rate': 6.173287002338577e-05, 'epoch': 9.6}\n",
      "{'loss': 0.1387, 'grad_norm': 1.7377768754959106, 'learning_rate': 6.021259908948402e-05, 'epoch': 9.78}\n",
      "{'loss': 0.1198, 'grad_norm': 2.197751045227051, 'learning_rate': 5.868240888334653e-05, 'epoch': 9.96}\n",
      "{'loss': 0.1043, 'grad_norm': 1.5599991083145142, 'learning_rate': 5.714378564496901e-05, 'epoch': 10.13}\n",
      "{'loss': 0.0917, 'grad_norm': 1.4933608770370483, 'learning_rate': 5.559822380516539e-05, 'epoch': 10.31}\n",
      "{'loss': 0.083, 'grad_norm': 2.139331102371216, 'learning_rate': 5.404722453406017e-05, 'epoch': 10.49}\n",
      "{'loss': 0.0853, 'grad_norm': 1.247467279434204, 'learning_rate': 5.249229428303486e-05, 'epoch': 10.67}\n",
      " 54%|█████████████████████▍                  | 600/1120 [17:31<13:00,  1.50s/it][INFO|trainer.py:4117] 2025-02-19 07:49:49,552 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-02-19 07:49:49,552 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-02-19 07:49:49,552 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 11.25it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.08it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:02,  7.59it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  7.36it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  7.18it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.40it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.15it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  6.98it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  6.96it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  7.07it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  6.94it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.21it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.83it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.80it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:02<00:00,  6.27it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  6.84it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  7.36it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  6.80it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  6.74it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4982119798660278, 'eval_runtime': 3.6045, 'eval_samples_per_second': 13.872, 'eval_steps_per_second': 6.936, 'epoch': 10.67}\n",
      " 54%|█████████████████████▍                  | 600/1120 [17:34<13:00,  1.50s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.12it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-02-19 07:49:53,157 >> Saving model checkpoint to news_analysis/checkpoint/checkpoint-600\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:49:53,319 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:49:53,320 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-02-19 07:49:53,364 >> tokenizer config file saved in news_analysis/checkpoint/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-02-19 07:49:53,365 >> Special tokens file saved in news_analysis/checkpoint/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 0.0833, 'grad_norm': 1.5261486768722534, 'learning_rate': 5.0934943321545115e-05, 'epoch': 10.84}\n",
      "{'loss': 0.0984, 'grad_norm': 1.2553526163101196, 'learning_rate': 4.9376684270229254e-05, 'epoch': 11.02}\n",
      "{'loss': 0.0629, 'grad_norm': 1.701124906539917, 'learning_rate': 4.781903063173321e-05, 'epoch': 11.2}\n",
      "{'loss': 0.0574, 'grad_norm': 1.0221877098083496, 'learning_rate': 4.626349532067879e-05, 'epoch': 11.38}\n",
      "{'loss': 0.0615, 'grad_norm': 1.4718865156173706, 'learning_rate': 4.471158919420312e-05, 'epoch': 11.56}\n",
      "{'loss': 0.0598, 'grad_norm': 1.6380928754806519, 'learning_rate': 4.316481958449634e-05, 'epoch': 11.73}\n",
      "{'loss': 0.07, 'grad_norm': 1.3246357440948486, 'learning_rate': 4.162468883476319e-05, 'epoch': 11.91}\n",
      "{'loss': 0.0621, 'grad_norm': 0.6885108947753906, 'learning_rate': 4.0092692840030134e-05, 'epoch': 12.09}\n",
      "{'loss': 0.0441, 'grad_norm': 2.686049461364746, 'learning_rate': 3.857031959421553e-05, 'epoch': 12.27}\n",
      "{'loss': 0.0441, 'grad_norm': 1.2175554037094116, 'learning_rate': 3.705904774487396e-05, 'epoch': 12.44}\n",
      " 62%|█████████████████████████               | 700/1120 [20:30<12:52,  1.84s/it][INFO|trainer.py:4117] 2025-02-19 07:52:49,252 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-02-19 07:52:49,252 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-02-19 07:52:49,252 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 11.21it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:02,  7.58it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  7.35it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  7.17it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.42it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.16it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:01,  7.00it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  6.97it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  7.08it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  6.95it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.22it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.81it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:02<00:00,  6.28it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  6.85it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  7.37it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  6.81it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  6.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.6082754135131836, 'eval_runtime': 3.6011, 'eval_samples_per_second': 13.885, 'eval_steps_per_second': 6.942, 'epoch': 12.44}\n",
      " 62%|█████████████████████████               | 700/1120 [20:34<12:52,  1.84s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.14it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-02-19 07:52:52,853 >> Saving model checkpoint to news_analysis/checkpoint/checkpoint-700\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:52:53,013 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:52:53,014 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-02-19 07:52:53,057 >> tokenizer config file saved in news_analysis/checkpoint/checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-02-19 07:52:53,058 >> Special tokens file saved in news_analysis/checkpoint/checkpoint-700/special_tokens_map.json\n",
      "{'loss': 0.0391, 'grad_norm': 0.4998260736465454, 'learning_rate': 3.556034515701852e-05, 'epoch': 12.62}\n",
      "{'loss': 0.0451, 'grad_norm': 1.2709753513336182, 'learning_rate': 3.4075667487415785e-05, 'epoch': 12.8}\n",
      "{'loss': 0.0483, 'grad_norm': 1.2780518531799316, 'learning_rate': 3.2606456770738636e-05, 'epoch': 12.98}\n",
      "{'loss': 0.0401, 'grad_norm': 0.6342048645019531, 'learning_rate': 3.115414001894974e-05, 'epoch': 13.16}\n",
      "{'loss': 0.029, 'grad_norm': 0.526355504989624, 'learning_rate': 2.9720127835276256e-05, 'epoch': 13.33}\n",
      "{'loss': 0.0287, 'grad_norm': 0.8523580431938171, 'learning_rate': 2.8305813044122097e-05, 'epoch': 13.51}\n",
      "{'loss': 0.0298, 'grad_norm': 0.7335214018821716, 'learning_rate': 2.6912569338248315e-05, 'epoch': 13.69}\n",
      "{'loss': 0.0336, 'grad_norm': 0.9167590141296387, 'learning_rate': 2.5541749944535554e-05, 'epoch': 13.87}\n",
      "{'loss': 0.0336, 'grad_norm': 0.4653192162513733, 'learning_rate': 2.4194686309624663e-05, 'epoch': 14.04}\n",
      "{'loss': 0.0245, 'grad_norm': 0.8508287072181702, 'learning_rate': 2.2872686806712035e-05, 'epoch': 14.22}\n",
      " 71%|████████████████████████████▌           | 800/1120 [23:27<09:25,  1.77s/it][INFO|trainer.py:4117] 2025-02-19 07:55:46,281 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-02-19 07:55:46,281 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-02-19 07:55:46,281 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 11.20it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.06it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:02,  7.58it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  7.35it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  7.18it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.43it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.17it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:01,  7.01it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  6.98it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  7.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  6.96it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.22it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.83it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.81it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:02<00:00,  6.27it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  6.85it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  7.37it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  6.81it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  6.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.693821668624878, 'eval_runtime': 3.6002, 'eval_samples_per_second': 13.888, 'eval_steps_per_second': 6.944, 'epoch': 14.22}\n",
      " 71%|████████████████████████████▌           | 800/1120 [23:31<09:25,  1.77s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.14it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-02-19 07:55:49,881 >> Saving model checkpoint to news_analysis/checkpoint/checkpoint-800\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:55:50,046 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:55:50,046 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-02-19 07:55:50,089 >> tokenizer config file saved in news_analysis/checkpoint/checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-02-19 07:55:50,090 >> Special tokens file saved in news_analysis/checkpoint/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0247, 'grad_norm': 0.4209062159061432, 'learning_rate': 2.157703546475539e-05, 'epoch': 14.4}\n",
      "{'loss': 0.0233, 'grad_norm': 0.41170912981033325, 'learning_rate': 2.0308990721324927e-05, 'epoch': 14.58}\n",
      "{'loss': 0.0235, 'grad_norm': 0.5409049987792969, 'learning_rate': 1.906978420031059e-05, 'epoch': 14.76}\n",
      "{'loss': 0.0255, 'grad_norm': 0.38718390464782715, 'learning_rate': 1.7860619515673033e-05, 'epoch': 14.93}\n",
      "{'loss': 0.0221, 'grad_norm': 0.2655009329319, 'learning_rate': 1.6682671102399805e-05, 'epoch': 15.11}\n",
      "{'loss': 0.0209, 'grad_norm': 0.2704254984855652, 'learning_rate': 1.553708307580265e-05, 'epoch': 15.29}\n",
      "{'loss': 0.0213, 'grad_norm': 0.26605159044265747, 'learning_rate': 1.4424968120263504e-05, 'epoch': 15.47}\n",
      "{'loss': 0.0186, 'grad_norm': 0.32803627848625183, 'learning_rate': 1.3347406408508695e-05, 'epoch': 15.64}\n",
      "{'loss': 0.0178, 'grad_norm': 0.29835405945777893, 'learning_rate': 1.230544455246101e-05, 'epoch': 15.82}\n",
      "{'loss': 0.022, 'grad_norm': 0.618025004863739, 'learning_rate': 1.130009458668863e-05, 'epoch': 16.0}\n",
      " 80%|████████████████████████████████▏       | 900/1120 [26:25<06:22,  1.74s/it][INFO|trainer.py:4117] 2025-02-19 07:58:44,013 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-02-19 07:58:44,013 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-02-19 07:58:44,013 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 11.25it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.10it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:02,  7.60it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  7.38it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  7.20it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.45it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.18it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:01,  7.04it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  7.00it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  7.11it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  6.98it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.24it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.86it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.83it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:02<00:00,  6.30it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  6.88it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  7.40it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  6.82it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  6.77it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7607600688934326, 'eval_runtime': 3.5916, 'eval_samples_per_second': 13.921, 'eval_steps_per_second': 6.961, 'epoch': 16.0}\n",
      " 80%|████████████████████████████████▏       | 900/1120 [26:29<06:22,  1.74s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.17it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-02-19 07:58:47,603 >> Saving model checkpoint to news_analysis/checkpoint/checkpoint-900\n",
      "[INFO|configuration_utils.py:679] 2025-02-19 07:58:47,771 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-19 07:58:47,772 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-02-19 07:58:47,805 >> tokenizer config file saved in news_analysis/checkpoint/checkpoint-900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-02-19 07:58:47,806 >> Special tokens file saved in news_analysis/checkpoint/checkpoint-900/special_tokens_map.json\n",
      "{'loss': 0.0176, 'grad_norm': 0.2668221592903137, 'learning_rate': 1.0332332985438248e-05, 'epoch': 16.18}\n",
      "{'loss': 0.0195, 'grad_norm': 0.46797052025794983, 'learning_rate': 9.403099714207175e-06, 'epoch': 16.36}\n",
      "{'loss': 0.0156, 'grad_norm': 0.2228594422340393, 'learning_rate': 8.513297316775625e-06, 'epoch': 16.53}\n",
      "{'loss': 0.0171, 'grad_norm': 0.6409543752670288, 'learning_rate': 7.663790038585793e-06, 'epoch': 16.71}\n",
      "{'loss': 0.0181, 'grad_norm': 0.2493765652179718, 'learning_rate': 6.855402987319348e-06, 'epoch': 16.89}\n",
      "{'loss': 0.0173, 'grad_norm': 0.2990388870239258, 'learning_rate': 6.088921331488568e-06, 'epoch': 17.07}\n",
      " 86%|██████████████████████████████████▍     | 965/1120 [28:22<04:43,  1.83s/it]"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train examples/train_lora/llama3_lora_newsanalysis.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffd3957-6590-406e-b419-5a3f26524f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.31s/it]\n",
      "Loading PEFT: ./news_analysis/checkpoint/checkpoint-900\n",
      "Running merge_and_unload\n",
      "tokenizer_config.json: 100%|███████████████| 54.5k/54.5k [00:00<00:00, 1.87MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 36.7MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 296/296 [00:00<00:00, 1.22MB/s]\n",
      "[2025-02-20 06:50:01,283] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Model saved to ./news_analysis/model\n"
     ]
    }
   ],
   "source": [
    "!python /workspace/LLaMA-Factory/merge.py \\\n",
    "    --base_model_name_or_path meta-llama/Llama-3.2-3B-Instruct \\\n",
    "    --peft_model_path ./news_analysis/checkpoint/checkpoint-900 \\\n",
    "    --output_dir ./news_analysis/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdd50c-330f-49b9-a54e-6a0ab3a7de0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
