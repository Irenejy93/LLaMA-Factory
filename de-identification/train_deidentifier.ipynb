{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507ac07a-e6ff-4d2e-8752-6c7fda408f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Irenejy93/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263806ab-d167-4777-ab6a-2ebee59f3655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/LLaMA-Factory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02eb9612-e718-4feb-abf4-a357eb8ec517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1234 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \n",
      "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2560 kB]\n",
      "Get:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1519 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [53.3 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3663 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2859 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1228 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [45.2 kB]\n",
      "Get:22 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3527 kB]\n",
      "Fetched 37.2 MB in 2s (15.1 MB/s)                            \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libgpm2 libsodium23 vim-common vim-runtime xxd\n",
      "Suggested packages:\n",
      "  gpm ctags vim-doc vim-scripts\n",
      "The following NEW packages will be installed:\n",
      "  libgpm2 libsodium23 vim vim-common vim-runtime xxd\n",
      "0 upgraded, 6 newly installed, 0 to remove and 122 not upgraded.\n",
      "Need to get 8876 kB of archives.\n",
      "After this operation, 38.8 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xxd amd64 2:8.2.3995-1ubuntu2.21 [52.3 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-common all 2:8.2.3995-1ubuntu2.21 [81.5 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgpm2 amd64 1.20.7-10build1 [15.3 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsodium23 amd64 1.0.18-1build2 [164 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-runtime all 2:8.2.3995-1ubuntu2.21 [6834 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim amd64 2:8.2.3995-1ubuntu2.21 [1729 kB]\n",
      "Fetched 8876 kB in 1s (8905 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package xxd.\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../0-xxd_2%3a8.2.3995-1ubuntu2.21_amd64.deb ...\n",
      "Unpacking xxd (2:8.2.3995-1ubuntu2.21) ...\n",
      "Selecting previously unselected package vim-common.\n",
      "Preparing to unpack .../1-vim-common_2%3a8.2.3995-1ubuntu2.21_all.deb ...\n",
      "Unpacking vim-common (2:8.2.3995-1ubuntu2.21) ...\n",
      "Selecting previously unselected package libgpm2:amd64.\n",
      "Preparing to unpack .../2-libgpm2_1.20.7-10build1_amd64.deb ...\n",
      "Unpacking libgpm2:amd64 (1.20.7-10build1) ...\n",
      "Selecting previously unselected package libsodium23:amd64.\n",
      "Preparing to unpack .../3-libsodium23_1.0.18-1build2_amd64.deb ...\n",
      "Unpacking libsodium23:amd64 (1.0.18-1build2) ...\n",
      "Selecting previously unselected package vim-runtime.\n",
      "Preparing to unpack .../4-vim-runtime_2%3a8.2.3995-1ubuntu2.21_all.deb ...\n",
      "Adding 'diversion of /usr/share/vim/vim82/doc/help.txt to /usr/share/vim/vim82/doc/help.txt.vim-tiny by vim-runtime'\n",
      "Adding 'diversion of /usr/share/vim/vim82/doc/tags to /usr/share/vim/vim82/doc/tags.vim-tiny by vim-runtime'\n",
      "Unpacking vim-runtime (2:8.2.3995-1ubuntu2.21) ...\n",
      "Selecting previously unselected package vim.\n",
      "Preparing to unpack .../5-vim_2%3a8.2.3995-1ubuntu2.21_amd64.deb ...\n",
      "Unpacking vim (2:8.2.3995-1ubuntu2.21) ...\n",
      "Setting up libsodium23:amd64 (1.0.18-1build2) ...\n",
      "Setting up libgpm2:amd64 (1.20.7-10build1) ...\n",
      "Setting up xxd (2:8.2.3995-1ubuntu2.21) ...\n",
      "Setting up vim-common (2:8.2.3995-1ubuntu2.21) ...\n",
      "Setting up vim-runtime (2:8.2.3995-1ubuntu2.21) ...\n",
      "Setting up vim (2:8.2.3995-1ubuntu2.21) ...\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/vi.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/vi.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/vi.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/vi.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/vi.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/vi.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/vi.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/vi.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/view.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/view.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/view.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/view.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/view.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/view.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/view.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/view.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/ex.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/ex.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/ex.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/ex.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/ex.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/ex.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/ex.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/ex.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/editor (editor) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/editor.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/editor.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/editor.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/editor.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/editor.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/editor.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/editor.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/editor.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install vim -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e7d989-a3d0-4330-984d-756f58bf909e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers<=4.46.1,>=4.41.2 (from -r requirements.txt (line 1))\n",
      "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets<=3.1.0,>=2.16.0 (from -r requirements.txt (line 2))\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate<=1.0.1,>=0.34.0 (from -r requirements.txt (line 3))\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<=0.12.0,>=0.11.1 (from -r requirements.txt (line 4))\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 5))\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<0.20.4,>=0.19.0 (from -r requirements.txt (line 6))\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting gradio<5.0.0,>=4.0.0 (from -r requirements.txt (line 7))\n",
      "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pandas>=2.0.0 (from -r requirements.txt (line 8))\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from -r requirements.txt (line 9))\n",
      "  Downloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting einops (from -r requirements.txt (line 10))\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 11))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 12))\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting protobuf (from -r requirements.txt (line 13))\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting uvicorn (from -r requirements.txt (line 14))\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic (from -r requirements.txt (line 15))\n",
      "  Downloading pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting fastapi (from -r requirements.txt (line 16))\n",
      "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting sse-starlette (from -r requirements.txt (line 17))\n",
      "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting matplotlib>=3.7.0 (from -r requirements.txt (line 18))\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting fire (from -r requirements.txt (line 19))\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (1.24.1)\n",
      "Collecting av (from -r requirements.txt (line 23))\n",
      "  Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting tyro<0.9.0 (from -r requirements.txt (line 24))\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (2023.4.0)\n",
      "Collecting aiohttp (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (2.1.0+cu118)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (4.0.0)\n",
      "Collecting ffmpy (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (2.1.2)\n",
      "Collecting orjson~=3.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading orjson-3.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (9.3.0)\n",
      "Collecting pydub (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading ruff-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (4.4.0)\n",
      "Collecting urllib3~=2.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 8)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.0.0->-r requirements.txt (line 8))\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.0.0->-r requirements.txt (line 8))\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting click>=7.0 (from uvicorn->-r requirements.txt (line 14))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 14))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->-r requirements.txt (line 15))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic->-r requirements.txt (line 15))\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions~=4.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi->-r requirements.txt (line 16))\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->-r requirements.txt (line 18))\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.7.0->-r requirements.txt (line 18))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->-r requirements.txt (line 18))\n",
      "  Downloading fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->-r requirements.txt (line 18))\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 18)) (2.4.7)\n",
      "Collecting termcolor (from fire->-r requirements.txt (line 19))\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (1.3.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7)) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0 (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2.1.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 24)) (2.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (2.1.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 7))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 24))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m178.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m187.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m183.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m202.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m141.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m176.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m181.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m183.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
      "Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m197.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m201.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m170.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m173.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.4/130.4 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m139.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m175.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=f0cf5ea050acdebb582f60002424843178047da02054bf67a1a138e3c4d882e0\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
      "Successfully built fire\n",
      "Installing collected packages: sentencepiece, pytz, pydub, xxhash, websockets, urllib3, tzdata, typing-extensions, tqdm, tomlkit, termcolor, shtab, shellingham, semantic-version, scipy, safetensors, ruff, regex, python-multipart, pyarrow, protobuf, propcache, orjson, mdurl, kiwisolver, importlib-resources, h11, fsspec, frozenlist, fonttools, ffmpy, einops, docstring-parser, dill, cycler, contourpy, click, av, async-timeout, annotated-types, aiohappyeyeballs, aiofiles, uvicorn, requests, pydantic-core, pandas, multiprocess, multidict, matplotlib, markdown-it-py, httpcore, fire, anyio, aiosignal, yarl, tiktoken, starlette, rich, pydantic, huggingface-hub, httpx, tyro, typer, tokenizers, sse-starlette, gradio-client, fastapi, aiohttp, accelerate, transformers, gradio, peft, datasets, trl\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.13\n",
      "    Uninstalling urllib3-1.26.13:\n",
      "      Successfully uninstalled urllib3-1.26.13\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "Successfully installed accelerate-1.0.1 aiofiles-23.2.1 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 async-timeout-5.0.1 av-14.0.1 click-8.1.8 contourpy-1.3.1 cycler-0.12.1 datasets-3.1.0 dill-0.3.8 docstring-parser-0.16 einops-0.8.0 fastapi-0.115.6 ffmpy-0.5.0 fire-0.7.0 fonttools-4.55.3 frozenlist-1.5.0 fsspec-2024.9.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.27.1 importlib-resources-6.5.2 kiwisolver-1.4.8 markdown-it-py-3.0.0 matplotlib-3.10.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 orjson-3.10.14 pandas-2.2.3 peft-0.12.0 propcache-0.2.1 protobuf-5.29.3 pyarrow-19.0.0 pydantic-2.10.5 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 pytz-2024.2 regex-2024.11.6 requests-2.32.3 rich-13.9.4 ruff-0.9.2 safetensors-0.5.2 scipy-1.15.1 semantic-version-2.10.0 sentencepiece-0.2.0 shellingham-1.5.4 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.41.3 termcolor-2.5.0 tiktoken-0.8.0 tokenizers-0.20.3 tomlkit-0.12.0 tqdm-4.67.1 transformers-4.46.1 trl-0.9.6 typer-0.15.1 typing-extensions-4.12.2 tyro-0.8.14 tzdata-2024.2 urllib3-2.3.0 uvicorn-0.34.0 websockets-12.0 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting deepspeed==0.14.0\n",
      "  Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson (from deepspeed==0.14.0)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting ninja (from deepspeed==0.14.0)\n",
      "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed==0.14.0)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (2.10.5)\n",
      "Collecting pynvml (from deepspeed==0.14.0)\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0) (4.12.2)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->deepspeed==0.14.0)\n",
      "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (2024.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed==0.14.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.14.0) (1.3.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400402 sha256=065a52878cdfbfaeab7f949ad8766142166744ddfb0b40141d0a380dce2b02c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, pynvml, ninja, deepspeed\n",
      "Successfully installed deepspeed-0.14.0 hjson-3.1.0 ninja-1.11.1.3 nvidia-ml-py-12.560.30 py-cpuinfo-9.0.0 pynvml-12.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Obtaining file:///workspace/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<=4.46.1,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (4.46.1)\n",
      "Requirement already satisfied: datasets<=3.1.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (0.9.6)\n",
      "Requirement already satisfied: tokenizers<0.20.4,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
      "Requirement already satisfied: gradio<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.15.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (5.29.3)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.34.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.10.5)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.6)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (14.0.1)\n",
      "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (0.8.14)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m151.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-chinese\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0) (3.11.11)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.28.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (6.5.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (2.1.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (3.10.14)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (9.3.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.9.2)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0) (2.3.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0) (12.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.41.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1) (3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2) (2024.11.6)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0) (1.7.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.5.0)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge-chinese) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0) (1.18.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0) (1.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0) (2.16.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0) (0.1.2)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hChecking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba, llamafactory\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=5983503b45191c8c0894a09b488218bcc8243bca9dd76ef51a228125f338a43a\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/69/31/d56d90b22a1777b0b231e234b00302a55be255930f8bd92dcd\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=23888 sha256=3c404e527c0666593bae5d1b13239f17a95b808de151a7ad15f11133fcfaca06\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-aj3qzw0f/wheels/6b/fb/2e/068b37b77399a386b5777cf8b247cb07a69197b4baef3732a2\n",
      "Successfully built jieba llamafactory\n",
      "Installing collected packages: jieba, rouge-chinese, joblib, nltk, llamafactory\n",
      "Successfully installed jieba-0.42.1 joblib-1.4.2 llamafactory-0.9.2.dev0 nltk-3.9.1 rouge-chinese-1.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install deepspeed==0.14.0\n",
    "!pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61602ad9-132d-4882-8154-ae0ea725f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `llamaf` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llamaf`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token ## 토큰값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf675c9-81d1-4c85-a918-837b35b0f645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-17 00:13:22,085] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO|2025-01-17 00:13:24] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:21097\n",
      "[2025-01-17 00:13:26,167] torch.distributed.run: [WARNING] \n",
      "[2025-01-17 00:13:26,167] torch.distributed.run: [WARNING] *****************************************\n",
      "[2025-01-17 00:13:26,167] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2025-01-17 00:13:26,167] torch.distributed.run: [WARNING] *****************************************\n",
      "[2025-01-17 00:13:30,493] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-01-17 00:13:30,551] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-01-17 00:13:30,939] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[WARNING|2025-01-17 00:13:31] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-01-17 00:13:31] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-01-17 00:13:31] llamafactory.hparams.parser:359 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-01-17 00:13:31] llamafactory.hparams.parser:359 >> Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "config.json: 100%|█████████████████████████████| 772/772 [00:00<00:00, 7.29MB/s]\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:13:31,879 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:13:31,880 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|████████████████| 51.0k/51.0k [00:00<00:00, 101MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 20.5MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 350/350 [00:00<00:00, 1.93MB/s]\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:33,146 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:33,146 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:33,146 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:33,146 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:33,147 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-01-17 00:13:33,511 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:13:34,110 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:13:34,111 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:34,262 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:34,262 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:34,262 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:34,262 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-17 00:13:34,262 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-01-17 00:13:34,607 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-01-17 00:13:34] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>\n",
      "[INFO|2025-01-17 00:13:34] llamafactory.data.loader:157 >> Loading dataset deidentification.json...\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 9919 examples [00:00, 12859.09 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 2282.2\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 500/500 [00:03<00:00, 161.83\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 65895, 83628, 34804, 111097, 105922, 18918, 103185, 54596, 108, 119087, 72115, 103850, 229, 80052, 382, 567, 67890, 30426, 126840, 48826, 16, 13, 55430, 125441, 62060, 57390, 57575, 117559, 64254, 18359, 510, 94283, 16, 1145, 510, 94283, 17, 60, 78102, 43139, 126912, 106248, 27796, 19954, 106725, 62060, 50643, 101360, 11, 121731, 24486, 87134, 13094, 64857, 98934, 113191, 50152, 105718, 62060, 60798, 32179, 18918, 41820, 61938, 627, 17, 13, 101347, 105316, 102657, 11, 23955, 85767, 33177, 11, 56773, 44690, 1174, 95303, 113002, 73148, 49085, 127141, 510, 74471, 16, 1145, 510, 74471, 17, 60, 78102, 11, 510, 37004, 16, 15304, 37004, 17, 60, 78102, 11, 510, 62105, 16, 15304, 62105, 17, 60, 102278, 1174, 510, 74483, 16, 1145, 510, 74483, 17, 60, 78102, 38164, 120, 17835, 62060, 60798, 101360, 121731, 24486, 61139, 20565, 64857, 98934, 107205, 50152, 102772, 105718, 62060, 60798, 32179, 18918, 41820, 61938, 627, 18, 13, 67945, 60798, 32179, 18918, 114839, 48936, 106745, 107285, 103843, 29102, 55216, 48424, 61415, 11, 74618, 55055, 77437, 75908, 118156, 108533, 22035, 104508, 35495, 101971, 52688, 43139, 23955, 108503, 108533, 119978, 720, 19, 13, 82001, 111850, 117035, 34804, 62060, 57390, 110078, 19954, 105701, 110218, 84656, 101106, 65219, 58901, 115839, 61938, 13, 720, 65895, 83628, 13094, 111097, 105922, 18918, 103185, 71023, 62060, 57390, 96318, 101577, 80052, 13, 720, 25, 43449, 25, 128009, 128006, 882, 128007, 271, 126862, 56154, 25, 96270, 124409, 11, 23955, 100551, 121346, 113629, 80052, 13, 118947, 18359, 101703, 81673, 30446, 109883, 117677, 1980, 35495, 108583, 25, 109296, 112938, 57575, 125069, 72043, 32428, 109231, 112994, 13094, 112655, 98243, 114067, 1980, 126862, 56154, 25, 103315, 11, 111530, 125069, 53400, 112994, 34804, 91811, 254, 73653, 59877, 101584, 55421, 80052, 382, 35495, 108583, 25, 91811, 254, 73653, 59877, 101584, 55421, 110917, 36811, 1980, 126862, 56154, 25, 107625, 39331, 13, 122851, 103236, 123789, 83719, 38187, 111132, 33390, 30027, 255, 122906, 105519, 89359, 125069, 48555, 250, 104727, 13094, 115839, 114409, 382, 35495, 108583, 25, 119879, 125475, 13, 4815, 126862, 56154, 25, 106354, 103236, 30446, 61415, 101480, 102233, 41953, 39250, 101136, 45618, 102772, 91811, 254, 73653, 59877, 101584, 125422, 124208, 114409, 382, 35495, 108583, 25, 106237, 33390, 122851, 103236, 123789, 83719, 38187, 108302, 118176, 20565, 98243, 114067, 1980, 126862, 56154, 25, 122851, 103236, 123789, 83719, 38187, 16582, 101272, 50152, 30027, 255, 122906, 105519, 89359, 125069, 48555, 250, 104727, 18359, 84696, 34609, 30426, 58901, 124005, 382, 35495, 108583, 25, 102066, 115284, 382, 126862, 56154, 25, 123978, 107152, 105519, 89359, 114656, 19954, 112107, 106788, 104685, 19954, 125069, 48555, 250, 104727, 18359, 84696, 34609, 30426, 58901, 116654, 105512, 35495, 36811, 382, 35495, 108583, 25, 91811, 254, 73653, 59877, 101584, 55421, 56773, 123926, 55000, 101103, 33390, 112655, 124157, 105454, 36811, 1980, 126862, 56154, 25, 127264, 96451, 16582, 108405, 22720, 13, 125921, 116534, 119673, 61139, 18918, 74959, 110513, 109670, 13, 126100, 73148, 121121, 30446, 109883, 58901, 36811, 382, 35495, 108583, 25, 100994, 33177, 79225, 113470, 13094, 56154, 89359, 107455, 13094, 58368, 119866, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 116534, 119673, 102132, 79053, 54780, 56773, 123935, 74959, 101360, 120709, 106917, 18359, 101703, 81673, 124365, 115284, 382, 35495, 108583, 25, 102132, 79053, 34804, 102155, 101090, 112106, 109816, 56773, 44690, 16969, 106010, 30426, 124091, 89359, 107573, 125519, 103272, 17835, 220, 914, 80052, 382, 126862, 56154, 25, 74959, 117078, 61938, 13, 103236, 30446, 83719, 38187, 18918, 106958, 95303, 113002, 85721, 48424, 18918, 102484, 108381, 113161, 103373, 36630, 106744, 1980, 35495, 108583, 25, 103315, 11, 95303, 113002, 85721, 48424, 16969, 62060, 83628, 102249, 103131, 106313, 101796, 220, 4513, 12, 10961, 12, 16474, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 127264, 34804, 122851, 103236, 123789, 55000, 101272, 103521, 122665, 1980, 35495, 108583, 25, 103315, 11, 107625, 39331, 13, 101480, 126344, 96102, 64189, 125502, 122665, 1980, 126862, 56154, 25, 103315, 11, 107455, 125085, 101480, 126344, 96102, 64189, 20565, 96451, 61938, 13, 101703, 81673, 30446, 109883, 117677, 1980, 35495, 108583, 25, 121121, 109616, 382, 126862, 56154, 25, 109562, 30426, 73653, 123118, 101103, 56773, 51402, 13, 83719, 38187, 74959, 72043, 80052, 1131, 103315, 11, 107123, 64356, 65219, 13879, 90463, 13, 69508, 108515, 115790, 36439, 34609, 83628, 122665, 1980, 35495, 108583, 25, 111699, 247, 22720, 382, 126862, 56154, 25, 117078, 61938, 13, 102678, 16969, 23955, 30381, 101607, 13094, 13879, 90463, 13, 111937, 123106, 98243, 51402, 0, 4815, 35495, 108583, 25, 103315, 11, 117078, 61938, 13, 111937, 123106, 121599, 51402, 13, 128009, 128006, 78191, 128007, 126862, 56154, 25, 96270, 124409, 11, 23955, 100551, 121346, 113629, 80052, 13, 118947, 18359, 101703, 81673, 30446, 109883, 117677, 1980, 35495, 108583, 25, 109296, 112938, 57575, 125069, 72043, 32428, 109231, 112994, 13094, 112655, 98243, 114067, 1980, 126862, 56154, 25, 103315, 11, 111530, 125069, 53400, 112994, 34804, 91811, 254, 73653, 59877, 101584, 55421, 80052, 382, 35495, 108583, 25, 91811, 254, 73653, 59877, 101584, 55421, 110917, 36811, 1980, 126862, 56154, 25, 107625, 39331, 13, 122851, 103236, 123789, 83719, 38187, 111132, 33390, 30027, 255, 122906, 105519, 89359, 125069, 48555, 250, 104727, 13094, 115839, 114409, 382, 35495, 108583, 25, 119879, 125475, 382, 126862, 56154, 25, 106354, 103236, 30446, 61415, 101480, 102233, 41953, 39250, 101136, 45618, 102772, 91811, 254, 73653, 59877, 101584, 125422, 124208, 114409, 382, 35495, 108583, 25, 106237, 33390, 122851, 103236, 123789, 83719, 38187, 108302, 118176, 20565, 98243, 114067, 1980, 126862, 56154, 25, 122851, 103236, 123789, 83719, 38187, 16582, 101272, 50152, 30027, 255, 122906, 105519, 89359, 125069, 48555, 250, 104727, 18359, 84696, 34609, 30426, 58901, 124005, 382, 35495, 108583, 25, 102066, 115284, 382, 126862, 56154, 25, 123978, 107152, 105519, 89359, 114656, 19954, 112107, 106788, 104685, 19954, 125069, 48555, 250, 104727, 18359, 84696, 34609, 30426, 58901, 116654, 105512, 35495, 36811, 382, 35495, 108583, 25, 91811, 254, 73653, 59877, 101584, 55421, 56773, 123926, 55000, 101103, 33390, 112655, 124157, 105454, 36811, 1980, 126862, 56154, 25, 127264, 96451, 16582, 108405, 22720, 13, 125921, 116534, 119673, 61139, 18918, 74959, 110513, 109670, 13, 126100, 73148, 121121, 30446, 109883, 58901, 36811, 382, 35495, 108583, 25, 510, 74471, 16, 2595, 126862, 56154, 25, 117078, 61938, 13, 116534, 119673, 102132, 79053, 54780, 56773, 123935, 74959, 101360, 120709, 106917, 18359, 101703, 81673, 124365, 115284, 382, 35495, 108583, 25, 102132, 79053, 34804, 510, 94283, 16, 60, 109816, 56773, 44690, 16969, 510, 62105, 16, 60, 80052, 382, 126862, 56154, 25, 74959, 117078, 61938, 13, 103236, 30446, 83719, 38187, 18918, 106958, 95303, 113002, 85721, 48424, 18918, 102484, 108381, 113161, 103373, 36630, 106744, 1980, 35495, 108583, 25, 103315, 11, 95303, 113002, 85721, 48424, 16969, 510, 74483, 16, 60, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 127264, 34804, 122851, 103236, 123789, 55000, 101272, 103521, 122665, 1980, 35495, 108583, 25, 103315, 11, 107625, 39331, 13, 101480, 126344, 96102, 64189, 125502, 122665, 1980, 126862, 56154, 25, 103315, 11, 107455, 125085, 101480, 126344, 96102, 64189, 20565, 96451, 61938, 13, 101703, 81673, 30446, 109883, 117677, 1980, 35495, 108583, 25, 121121, 109616, 382, 126862, 56154, 25, 109562, 30426, 73653, 123118, 101103, 56773, 51402, 13, 83719, 38187, 74959, 72043, 80052, 1131, 103315, 11, 107123, 64356, 65219, 13879, 90463, 13, 69508, 108515, 115790, 36439, 34609, 83628, 122665, 1980, 35495, 108583, 25, 111699, 247, 22720, 382, 126862, 56154, 25, 117078, 61938, 13, 102678, 16969, 510, 94283, 17, 60, 13094, 13879, 90463, 13, 111937, 123106, 98243, 51402, 2268, 35495, 108583, 25, 103315, 11, 117078, 61938, 13, 111937, 123106, 121599, 51402, 13, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 개인정보를 감춰주는 로봇입니다.\n",
      "\n",
      "## 지시 사항 ##\n",
      "1.주어진 대화에서 사람이름을 [PERSON1], [PERSON2] 등으로 등장 순서에 따라 대체하고, 동일한 이름이 반복될 경우 같은 대치어를 사용합니다.\n",
      "2.연락처, 이메일, 주소, 계좌번호도 각각 [CONTACT1], [CONTACT2] 등, [EMAIL1],[EMAIL2] 등, [ADDRESS1],[ADDRESS2]등, [ACCOUNT1], [ACCOUNT2] 등 으로 대치하고 동일한 정보가 반복되는 경우에는 같은 대치어를 사용합니다.\n",
      "3.대치어를 작성할때 글머리 기호나, 나열식 방식을 쓰지말고 평문으로 이어서 쓰십시오 \n",
      "4.위 규칙은 대화 전체에 걸쳐 일관되게 적용합니다. \n",
      "당신이 개인정보를 감출 대화내역입니다. \n",
      ": 입력:<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "상담사: 안녕하세요, 이월 쇼핑입니다. 무엇을 도와드릴까요?\n",
      "\n",
      "고객: 지금 방송에서 할인 중인 상품 가격이 어떻게 되나요?\n",
      "\n",
      "상담사: 네, 현재 할인된 가격은 칠만 구천원입니다.\n",
      "\n",
      "고객: 칠만 구천원이라고요?\n",
      "\n",
      "상담사: 맞습니다. 삼성 카드로 결제하시면 십프로 청구 할인 혜택이 적용됩니다.\n",
      "\n",
      "고객: 그렇군요. \n",
      "\n",
      "상담사: 일반 카드나 무통장 입금 시에는 칠만 구천원이 유지됩니다.\n",
      "\n",
      "고객: 그러면 삼성 카드로 결제하면 얼마가 되나요?\n",
      "\n",
      "상담사: 삼성 카드로 결제하실 경우 십프로 청구 할인 혜택을 받으시게 됩니다.\n",
      "\n",
      "고객: 알겠습니다.\n",
      "\n",
      "상담사: 그래서 총 청구 금액에 대해 다음 달에 할인 혜택을 받으시게 되는 것이고요.\n",
      "\n",
      "고객: 칠만 구천원 주문을 하려면 어떻게 해야 하나요?\n",
      "\n",
      "상담사: 주문 가능하십니다. 먼저 고객님의 정보를 확인해야 합니다. 전화번호 부탁드릴게요.\n",
      "\n",
      "고객: 공일공 팔이사구 삼이오칠입니다.\n",
      "\n",
      "상담사: 감사합니다. 고객님의 성함과 주소를 확인하고 회원 등록을 도와드리겠습니다.\n",
      "\n",
      "고객: 성함은 김영훈이고 주소는 서울시 강남구 테헤란로 25입니다.\n",
      "\n",
      "상담사: 확인 감사합니다. 카드 결제를 위해 계좌 번호를 남겨주시겠습니까?\n",
      "\n",
      "고객: 네, 계좌 번호는 대신증권 점포 123-456-789입니다.\n",
      "\n",
      "상담사: 감사합니다. 주문은 삼성 카드로 하실 건가요?\n",
      "\n",
      "고객: 네, 맞습니다. 무이자 할부 가능한가요?\n",
      "\n",
      "상담사: 네, 삼개월 무이자 할부가 가능합니다. 도와드릴까요?\n",
      "\n",
      "고객: 부탁드립니다.\n",
      "\n",
      "상담사: 잠시만 기다려 주세요. 결제 확인 중입니다... 네, 완료되었습니다. 추가 문의사항 있으신가요?\n",
      "\n",
      "고객: 아닙니다.\n",
      "\n",
      "상담사: 감사합니다. 저는 이정민이었습니다. 좋은 하루 되세요! \n",
      "\n",
      "고객: 네, 감사합니다. 좋은 하루 보내세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>상담사: 안녕하세요, 이월 쇼핑입니다. 무엇을 도와드릴까요?\n",
      "\n",
      "고객: 지금 방송에서 할인 중인 상품 가격이 어떻게 되나요?\n",
      "\n",
      "상담사: 네, 현재 할인된 가격은 칠만 구천원입니다.\n",
      "\n",
      "고객: 칠만 구천원이라고요?\n",
      "\n",
      "상담사: 맞습니다. 삼성 카드로 결제하시면 십프로 청구 할인 혜택이 적용됩니다.\n",
      "\n",
      "고객: 그렇군요.\n",
      "\n",
      "상담사: 일반 카드나 무통장 입금 시에는 칠만 구천원이 유지됩니다.\n",
      "\n",
      "고객: 그러면 삼성 카드로 결제하면 얼마가 되나요?\n",
      "\n",
      "상담사: 삼성 카드로 결제하실 경우 십프로 청구 할인 혜택을 받으시게 됩니다.\n",
      "\n",
      "고객: 알겠습니다.\n",
      "\n",
      "상담사: 그래서 총 청구 금액에 대해 다음 달에 할인 혜택을 받으시게 되는 것이고요.\n",
      "\n",
      "고객: 칠만 구천원 주문을 하려면 어떻게 해야 하나요?\n",
      "\n",
      "상담사: 주문 가능하십니다. 먼저 고객님의 정보를 확인해야 합니다. 전화번호 부탁드릴게요.\n",
      "\n",
      "고객: [CONTACT1]\n",
      "\n",
      "상담사: 감사합니다. 고객님의 성함과 주소를 확인하고 회원 등록을 도와드리겠습니다.\n",
      "\n",
      "고객: 성함은 [PERSON1]이고 주소는 [ADDRESS1]입니다.\n",
      "\n",
      "상담사: 확인 감사합니다. 카드 결제를 위해 계좌 번호를 남겨주시겠습니까?\n",
      "\n",
      "고객: 네, 계좌 번호는 [ACCOUNT1]입니다.\n",
      "\n",
      "상담사: 감사합니다. 주문은 삼성 카드로 하실 건가요?\n",
      "\n",
      "고객: 네, 맞습니다. 무이자 할부 가능한가요?\n",
      "\n",
      "상담사: 네, 삼개월 무이자 할부가 가능합니다. 도와드릴까요?\n",
      "\n",
      "고객: 부탁드립니다.\n",
      "\n",
      "상담사: 잠시만 기다려 주세요. 결제 확인 중입니다... 네, 완료되었습니다. 추가 문의사항 있으신가요?\n",
      "\n",
      "고객: 아닙니다.\n",
      "\n",
      "상담사: 감사합니다. 저는 [PERSON2]이었습니다. 좋은 하루 되세요!\n",
      "\n",
      "고객: 네, 감사합니다. 좋은 하루 보내세요.<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 126862, 56154, 25, 96270, 124409, 11, 23955, 100551, 121346, 113629, 80052, 13, 118947, 18359, 101703, 81673, 30446, 109883, 117677, 1980, 35495, 108583, 25, 109296, 112938, 57575, 125069, 72043, 32428, 109231, 112994, 13094, 112655, 98243, 114067, 1980, 126862, 56154, 25, 103315, 11, 111530, 125069, 53400, 112994, 34804, 91811, 254, 73653, 59877, 101584, 55421, 80052, 382, 35495, 108583, 25, 91811, 254, 73653, 59877, 101584, 55421, 110917, 36811, 1980, 126862, 56154, 25, 107625, 39331, 13, 122851, 103236, 123789, 83719, 38187, 111132, 33390, 30027, 255, 122906, 105519, 89359, 125069, 48555, 250, 104727, 13094, 115839, 114409, 382, 35495, 108583, 25, 119879, 125475, 382, 126862, 56154, 25, 106354, 103236, 30446, 61415, 101480, 102233, 41953, 39250, 101136, 45618, 102772, 91811, 254, 73653, 59877, 101584, 125422, 124208, 114409, 382, 35495, 108583, 25, 106237, 33390, 122851, 103236, 123789, 83719, 38187, 108302, 118176, 20565, 98243, 114067, 1980, 126862, 56154, 25, 122851, 103236, 123789, 83719, 38187, 16582, 101272, 50152, 30027, 255, 122906, 105519, 89359, 125069, 48555, 250, 104727, 18359, 84696, 34609, 30426, 58901, 124005, 382, 35495, 108583, 25, 102066, 115284, 382, 126862, 56154, 25, 123978, 107152, 105519, 89359, 114656, 19954, 112107, 106788, 104685, 19954, 125069, 48555, 250, 104727, 18359, 84696, 34609, 30426, 58901, 116654, 105512, 35495, 36811, 382, 35495, 108583, 25, 91811, 254, 73653, 59877, 101584, 55421, 56773, 123926, 55000, 101103, 33390, 112655, 124157, 105454, 36811, 1980, 126862, 56154, 25, 127264, 96451, 16582, 108405, 22720, 13, 125921, 116534, 119673, 61139, 18918, 74959, 110513, 109670, 13, 126100, 73148, 121121, 30446, 109883, 58901, 36811, 382, 35495, 108583, 25, 510, 74471, 16, 2595, 126862, 56154, 25, 117078, 61938, 13, 116534, 119673, 102132, 79053, 54780, 56773, 123935, 74959, 101360, 120709, 106917, 18359, 101703, 81673, 124365, 115284, 382, 35495, 108583, 25, 102132, 79053, 34804, 510, 94283, 16, 60, 109816, 56773, 44690, 16969, 510, 62105, 16, 60, 80052, 382, 126862, 56154, 25, 74959, 117078, 61938, 13, 103236, 30446, 83719, 38187, 18918, 106958, 95303, 113002, 85721, 48424, 18918, 102484, 108381, 113161, 103373, 36630, 106744, 1980, 35495, 108583, 25, 103315, 11, 95303, 113002, 85721, 48424, 16969, 510, 74483, 16, 60, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 127264, 34804, 122851, 103236, 123789, 55000, 101272, 103521, 122665, 1980, 35495, 108583, 25, 103315, 11, 107625, 39331, 13, 101480, 126344, 96102, 64189, 125502, 122665, 1980, 126862, 56154, 25, 103315, 11, 107455, 125085, 101480, 126344, 96102, 64189, 20565, 96451, 61938, 13, 101703, 81673, 30446, 109883, 117677, 1980, 35495, 108583, 25, 121121, 109616, 382, 126862, 56154, 25, 109562, 30426, 73653, 123118, 101103, 56773, 51402, 13, 83719, 38187, 74959, 72043, 80052, 1131, 103315, 11, 107123, 64356, 65219, 13879, 90463, 13, 69508, 108515, 115790, 36439, 34609, 83628, 122665, 1980, 35495, 108583, 25, 111699, 247, 22720, 382, 126862, 56154, 25, 117078, 61938, 13, 102678, 16969, 510, 94283, 17, 60, 13094, 13879, 90463, 13, 111937, 123106, 98243, 51402, 2268, 35495, 108583, 25, 103315, 11, 117078, 61938, 13, 111937, 123106, 121599, 51402, 13, 128009]\n",
      "labels:\n",
      "상담사: 안녕하세요, 이월 쇼핑입니다. 무엇을 도와드릴까요?\n",
      "\n",
      "고객: 지금 방송에서 할인 중인 상품 가격이 어떻게 되나요?\n",
      "\n",
      "상담사: 네, 현재 할인된 가격은 칠만 구천원입니다.\n",
      "\n",
      "고객: 칠만 구천원이라고요?\n",
      "\n",
      "상담사: 맞습니다. 삼성 카드로 결제하시면 십프로 청구 할인 혜택이 적용됩니다.\n",
      "\n",
      "고객: 그렇군요.\n",
      "\n",
      "상담사: 일반 카드나 무통장 입금 시에는 칠만 구천원이 유지됩니다.\n",
      "\n",
      "고객: 그러면 삼성 카드로 결제하면 얼마가 되나요?\n",
      "\n",
      "상담사: 삼성 카드로 결제하실 경우 십프로 청구 할인 혜택을 받으시게 됩니다.\n",
      "\n",
      "고객: 알겠습니다.\n",
      "\n",
      "상담사: 그래서 총 청구 금액에 대해 다음 달에 할인 혜택을 받으시게 되는 것이고요.\n",
      "\n",
      "고객: 칠만 구천원 주문을 하려면 어떻게 해야 하나요?\n",
      "\n",
      "상담사: 주문 가능하십니다. 먼저 고객님의 정보를 확인해야 합니다. 전화번호 부탁드릴게요.\n",
      "\n",
      "고객: [CONTACT1]\n",
      "\n",
      "상담사: 감사합니다. 고객님의 성함과 주소를 확인하고 회원 등록을 도와드리겠습니다.\n",
      "\n",
      "고객: 성함은 [PERSON1]이고 주소는 [ADDRESS1]입니다.\n",
      "\n",
      "상담사: 확인 감사합니다. 카드 결제를 위해 계좌 번호를 남겨주시겠습니까?\n",
      "\n",
      "고객: 네, 계좌 번호는 [ACCOUNT1]입니다.\n",
      "\n",
      "상담사: 감사합니다. 주문은 삼성 카드로 하실 건가요?\n",
      "\n",
      "고객: 네, 맞습니다. 무이자 할부 가능한가요?\n",
      "\n",
      "상담사: 네, 삼개월 무이자 할부가 가능합니다. 도와드릴까요?\n",
      "\n",
      "고객: 부탁드립니다.\n",
      "\n",
      "상담사: 잠시만 기다려 주세요. 결제 확인 중입니다... 네, 완료되었습니다. 추가 문의사항 있으신가요?\n",
      "\n",
      "고객: 아닙니다.\n",
      "\n",
      "상담사: 감사합니다. 저는 [PERSON2]이었습니다. 좋은 하루 되세요!\n",
      "\n",
      "고객: 네, 감사합니다. 좋은 하루 보내세요.<|eot_id|>\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:13:41,491 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:13:41,492 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "model.safetensors.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 7.95MB/s]\n",
      "[INFO|modeling_utils.py:3937] 2025-01-17 00:13:41,908 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/model.safetensors.index.json\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0%|             | 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 10.5M/4.98G [00:00<02:51, 29.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 21.0M/4.98G [00:00<02:20, 35.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 31.5M/4.98G [00:00<02:07, 38.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 41.9M/4.98G [00:01<02:02, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 52.4M/4.98G [00:01<01:59, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 62.9M/4.98G [00:01<01:57, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 73.4M/4.98G [00:01<01:56, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|    | 83.9M/4.98G [00:02<01:55, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|    | 94.4M/4.98G [00:02<01:55, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|     | 105M/4.98G [00:02<01:54, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|     | 115M/4.98G [00:02<01:54, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 126M/4.98G [00:03<01:53, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 136M/4.98G [00:03<01:53, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 147M/4.98G [00:03<01:53, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 157M/4.98G [00:03<01:52, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 168M/4.98G [00:04<01:52, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 178M/4.98G [00:04<01:51, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 189M/4.98G [00:04<01:52, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 199M/4.98G [00:04<01:52, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 210M/4.98G [00:05<01:51, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 220M/4.98G [00:05<01:51, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▏    | 231M/4.98G [00:05<01:50, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▏    | 241M/4.98G [00:05<01:51, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎    | 252M/4.98G [00:06<01:51, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎    | 262M/4.98G [00:06<01:50, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎    | 273M/4.98G [00:06<01:50, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 283M/4.98G [00:06<01:50, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 294M/4.98G [00:07<01:50, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 304M/4.98G [00:07<01:49, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 315M/4.98G [00:07<01:49, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 325M/4.98G [00:07<01:49, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 336M/4.98G [00:07<01:48, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 346M/4.98G [00:08<01:48, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 357M/4.98G [00:08<01:48, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 367M/4.98G [00:08<01:48, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 377M/4.98G [00:08<01:47, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 388M/4.98G [00:09<01:47, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 398M/4.98G [00:09<01:47, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 409M/4.98G [00:09<01:47, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 419M/4.98G [00:09<01:47, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 430M/4.98G [00:10<01:46, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 440M/4.98G [00:10<02:04, 36.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 451M/4.98G [00:10<02:00, 37.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 461M/4.98G [00:11<01:55, 39.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 472M/4.98G [00:11<01:52, 40.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▍    | 482M/4.98G [00:11<01:50, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▍    | 493M/4.98G [00:11<01:49, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▌    | 503M/4.98G [00:12<01:47, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▌    | 514M/4.98G [00:12<01:46, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 524M/4.98G [00:12<01:45, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 535M/4.98G [00:12<01:44, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 545M/4.98G [00:13<01:46, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 556M/4.98G [00:13<01:44, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 566M/4.98G [00:13<01:43, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 577M/4.98G [00:13<01:42, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 587M/4.98G [00:14<01:42, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 598M/4.98G [00:14<01:42, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 608M/4.98G [00:14<01:42, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 619M/4.98G [00:14<01:42, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 629M/4.98G [00:15<01:42, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 640M/4.98G [00:15<01:41, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 650M/4.98G [00:15<01:42, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 661M/4.98G [00:15<01:42, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 671M/4.98G [00:16<01:42, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 682M/4.98G [00:16<01:41, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 692M/4.98G [00:16<01:41, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 703M/4.98G [00:16<01:40, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 713M/4.98G [00:17<01:40, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▋    | 724M/4.98G [00:17<01:40, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▋    | 734M/4.98G [00:17<01:40, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▋    | 744M/4.98G [00:17<01:39, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▊    | 755M/4.98G [00:18<01:39, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▊    | 765M/4.98G [00:18<01:38, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 776M/4.98G [00:18<01:38, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 786M/4.98G [00:18<01:38, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 797M/4.98G [00:18<01:38, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 807M/4.98G [00:19<01:38, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 818M/4.98G [00:19<01:38, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 828M/4.98G [00:19<01:38, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 839M/4.98G [00:19<01:37, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 849M/4.98G [00:20<01:36, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 860M/4.98G [00:20<01:20, 51.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 881M/4.98G [00:20<01:34, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 891M/4.98G [00:21<01:38, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 902M/4.98G [00:21<01:39, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 912M/4.98G [00:21<01:37, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 923M/4.98G [00:21<01:36, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 933M/4.98G [00:22<01:36, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 944M/4.98G [00:22<01:36, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 954M/4.98G [00:22<01:35, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 965M/4.98G [00:22<01:34, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|▉    | 975M/4.98G [00:23<01:34, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|▉    | 986M/4.98G [00:23<01:34, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|█    | 996M/4.98G [00:23<01:34, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|▊   | 1.01G/4.98G [00:23<01:34, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|▊   | 1.02G/4.98G [00:24<01:33, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.03G/4.98G [00:24<01:33, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.04G/4.98G [00:24<01:33, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.05G/4.98G [00:24<01:32, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.06G/4.98G [00:25<01:55, 33.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|▊   | 1.08G/4.98G [00:25<01:26, 44.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|▉   | 1.09G/4.98G [00:25<01:27, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|▉   | 1.10G/4.98G [00:26<01:28, 43.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|▉   | 1.11G/4.98G [00:26<01:28, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.12G/4.98G [00:26<01:28, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.13G/4.98G [00:26<01:29, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.14G/4.98G [00:27<01:29, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.15G/4.98G [00:27<01:29, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.16G/4.98G [00:27<01:29, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.17G/4.98G [00:27<01:30, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.18G/4.98G [00:28<01:29, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.20G/4.98G [00:28<01:29, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.21G/4.98G [00:28<01:28, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.22G/4.98G [00:28<01:30, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|▉   | 1.23G/4.98G [00:29<01:30, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|▉   | 1.24G/4.98G [00:29<01:29, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█   | 1.25G/4.98G [00:29<01:28, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█   | 1.26G/4.98G [00:29<01:28, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█   | 1.27G/4.98G [00:30<01:28, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.28G/4.98G [00:30<01:27, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.29G/4.98G [00:30<01:27, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.30G/4.98G [00:30<01:28, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.31G/4.98G [00:31<01:27, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.32G/4.98G [00:31<01:26, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.33G/4.98G [00:31<01:27, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.34G/4.98G [00:31<01:29, 40.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.35G/4.98G [00:32<01:29, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.36G/4.98G [00:32<01:27, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█   | 1.37G/4.98G [00:32<01:26, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█   | 1.38G/4.98G [00:32<01:26, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█   | 1.39G/4.98G [00:33<01:25, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█▏  | 1.41G/4.98G [00:33<01:24, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█▏  | 1.42G/4.98G [00:33<01:24, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.43G/4.98G [00:33<01:24, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.44G/4.98G [00:34<01:23, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.45G/4.98G [00:34<01:23, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.46G/4.98G [00:34<01:22, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.47G/4.98G [00:34<01:22, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.48G/4.98G [00:35<01:21, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.49G/4.98G [00:35<01:21, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.50G/4.98G [00:35<01:22, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.51G/4.98G [00:35<01:23, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.52G/4.98G [00:36<01:21, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.53G/4.98G [00:36<01:21, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.54G/4.98G [00:36<01:21, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.55G/4.98G [00:36<01:22, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▎  | 1.56G/4.98G [00:37<01:20, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.57G/4.98G [00:37<01:20, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.58G/4.98G [00:37<01:19, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.59G/4.98G [00:37<01:21, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.60G/4.98G [00:38<01:20, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.61G/4.98G [00:38<01:19, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.63G/4.98G [00:38<01:19, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.64G/4.98G [00:38<01:20, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.65G/4.98G [00:39<01:21, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.66G/4.98G [00:39<01:19, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.67G/4.98G [00:39<01:19, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.68G/4.98G [00:39<01:18, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.69G/4.98G [00:40<01:18, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.70G/4.98G [00:40<01:17, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.71G/4.98G [00:40<01:16, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.72G/4.98G [00:40<01:17, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.73G/4.98G [00:41<01:17, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.74G/4.98G [00:41<01:16, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.75G/4.98G [00:41<01:16, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.76G/4.98G [00:41<01:17, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.77G/4.98G [00:42<01:29, 35.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.78G/4.98G [00:42<01:12, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.79G/4.98G [00:42<01:17, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.80G/4.98G [00:43<01:22, 38.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.81G/4.98G [00:43<01:19, 39.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.82G/4.98G [00:43<01:18, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.84G/4.98G [00:43<01:16, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.85G/4.98G [00:43<01:15, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.86G/4.98G [00:44<01:14, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.87G/4.98G [00:44<01:14, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.88G/4.98G [00:44<01:13, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.89G/4.98G [00:44<01:13, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.90G/4.98G [00:45<01:12, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.91G/4.98G [00:45<01:12, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.92G/4.98G [00:45<01:11, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.93G/4.98G [00:45<01:12, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.94G/4.98G [00:46<01:11, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.95G/4.98G [00:46<01:11, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.96G/4.98G [00:46<01:10, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 1.97G/4.98G [00:46<01:10, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 1.98G/4.98G [00:47<01:10, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 1.99G/4.98G [00:47<01:09, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 2.00G/4.98G [00:47<01:09, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 2.01G/4.98G [00:47<01:09, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.02G/4.98G [00:48<01:09, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.03G/4.98G [00:48<01:09, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.04G/4.98G [00:48<01:08, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.06G/4.98G [00:48<01:08, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.07G/4.98G [00:49<01:08, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.08G/4.98G [00:49<01:07, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.09G/4.98G [00:49<01:07, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.10G/4.98G [00:49<01:08, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.11G/4.98G [00:50<01:07, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.12G/4.98G [00:50<01:07, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.13G/4.98G [00:50<01:06, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.14G/4.98G [00:50<01:07, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.15G/4.98G [00:51<01:06, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.16G/4.98G [00:51<01:06, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▋  | 2.17G/4.98G [00:51<01:06, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.18G/4.98G [00:51<01:06, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.19G/4.98G [00:52<01:06, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.20G/4.98G [00:52<01:06, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.21G/4.98G [00:52<01:05, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.22G/4.98G [00:52<01:05, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.23G/4.98G [00:53<01:05, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.24G/4.98G [00:53<01:05, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.25G/4.98G [00:53<01:04, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.26G/4.98G [00:53<01:04, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.28G/4.98G [00:54<01:04, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.29G/4.98G [00:54<01:03, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.30G/4.98G [00:54<01:03, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.31G/4.98G [00:54<01:04, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▊  | 2.32G/4.98G [00:55<01:03, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▊  | 2.33G/4.98G [00:55<01:02, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.34G/4.98G [00:55<01:02, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.35G/4.98G [00:55<01:03, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.36G/4.98G [00:56<01:02, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.37G/4.98G [00:56<01:02, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.38G/4.98G [00:56<01:01, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.39G/4.98G [00:56<01:02, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.40G/4.98G [00:57<01:01, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.41G/4.98G [00:57<01:00, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.42G/4.98G [00:57<01:00, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.43G/4.98G [00:57<01:01, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.44G/4.98G [00:58<01:00, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.45G/4.98G [00:58<01:00, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.46G/4.98G [00:58<00:59, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.47G/4.98G [00:58<01:00, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.49G/4.98G [00:59<00:59, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██  | 2.50G/4.98G [00:59<00:58, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██  | 2.51G/4.98G [00:59<00:58, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.52G/4.98G [00:59<00:59, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.53G/4.98G [01:00<00:58, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.54G/4.98G [01:00<00:57, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.55G/4.98G [01:00<00:57, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.56G/4.98G [01:00<00:58, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.57G/4.98G [01:01<00:57, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.58G/4.98G [01:01<00:56, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.59G/4.98G [01:01<00:56, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.60G/4.98G [01:01<00:57, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.61G/4.98G [01:02<00:56, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██  | 2.62G/4.98G [01:02<00:55, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██  | 2.63G/4.98G [01:02<00:55, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██  | 2.64G/4.98G [01:02<00:56, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██▏ | 2.65G/4.98G [01:03<00:55, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.66G/4.98G [01:03<00:54, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.67G/4.98G [01:03<00:54, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.68G/4.98G [01:03<00:55, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.69G/4.98G [01:04<00:54, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.71G/4.98G [01:04<00:53, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.72G/4.98G [01:04<00:53, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.73G/4.98G [01:04<00:54, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.74G/4.98G [01:05<01:04, 34.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.76G/4.98G [01:05<00:49, 44.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.77G/4.98G [01:05<00:51, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.78G/4.98G [01:06<00:50, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.79G/4.98G [01:06<00:50, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▎ | 2.80G/4.98G [01:06<00:50, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▎ | 2.81G/4.98G [01:06<00:51, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.82G/4.98G [01:07<00:51, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.83G/4.98G [01:07<00:50, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.84G/4.98G [01:07<00:50, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.85G/4.98G [01:07<00:51, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.86G/4.98G [01:08<00:50, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.87G/4.98G [01:08<00:50, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.88G/4.98G [01:08<00:49, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.89G/4.98G [01:08<00:49, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.90G/4.98G [01:09<00:49, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.92G/4.98G [01:09<00:49, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.93G/4.98G [01:09<00:48, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.94G/4.98G [01:09<00:49, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.95G/4.98G [01:10<00:48, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▍ | 2.96G/4.98G [01:10<00:48, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.97G/4.98G [01:10<00:47, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.98G/4.98G [01:10<00:48, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.99G/4.98G [01:11<00:47, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 3.00G/4.98G [01:11<00:46, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 3.01G/4.98G [01:11<00:46, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.02G/4.98G [01:11<00:47, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.03G/4.98G [01:12<00:46, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.04G/4.98G [01:12<00:45, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.05G/4.98G [01:12<00:45, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.06G/4.98G [01:12<00:46, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.07G/4.98G [01:13<00:45, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.08G/4.98G [01:13<00:44, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.09G/4.98G [01:13<00:44, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.10G/4.98G [01:13<00:45, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.11G/4.98G [01:14<00:44, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.12G/4.98G [01:14<00:44, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.14G/4.98G [01:14<00:43, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.15G/4.98G [01:14<00:44, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.16G/4.98G [01:15<00:43, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.17G/4.98G [01:15<00:42, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.18G/4.98G [01:15<00:42, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.19G/4.98G [01:15<00:43, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.20G/4.98G [01:16<00:42, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.21G/4.98G [01:16<00:42, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.22G/4.98G [01:16<00:41, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.23G/4.98G [01:16<00:41, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.24G/4.98G [01:17<00:41, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.25G/4.98G [01:17<00:40, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▌ | 3.26G/4.98G [01:17<00:40, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.27G/4.98G [01:17<00:41, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.28G/4.98G [01:18<00:40, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.29G/4.98G [01:18<00:40, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.30G/4.98G [01:18<00:40, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.31G/4.98G [01:18<00:39, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.32G/4.98G [01:19<00:39, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.33G/4.98G [01:19<00:39, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.34G/4.98G [01:19<00:38, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.36G/4.98G [01:19<00:38, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.37G/4.98G [01:20<00:38, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.38G/4.98G [01:20<00:37, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.39G/4.98G [01:20<00:37, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.40G/4.98G [01:20<00:38, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.41G/4.98G [01:21<00:37, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██▋ | 3.42G/4.98G [01:21<00:36, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.43G/4.98G [01:21<00:36, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.44G/4.98G [01:21<00:37, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.45G/4.98G [01:22<00:36, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.46G/4.98G [01:22<00:35, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.47G/4.98G [01:22<00:35, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.48G/4.98G [01:22<00:35, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.49G/4.98G [01:23<00:35, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.50G/4.98G [01:23<00:34, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.51G/4.98G [01:23<00:34, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.52G/4.98G [01:23<00:35, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.53G/4.98G [01:24<00:34, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.54G/4.98G [01:24<00:34, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.55G/4.98G [01:24<00:33, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▊ | 3.57G/4.98G [01:24<00:34, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▊ | 3.58G/4.98G [01:25<00:33, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.59G/4.98G [01:25<00:32, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.60G/4.98G [01:25<00:32, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.61G/4.98G [01:25<00:33, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.62G/4.98G [01:26<00:32, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.63G/4.98G [01:26<00:32, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.64G/4.98G [01:26<00:31, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.65G/4.98G [01:26<00:32, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.66G/4.98G [01:27<00:31, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.67G/4.98G [01:27<00:30, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.68G/4.98G [01:27<00:30, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.69G/4.98G [01:27<00:31, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.70G/4.98G [01:28<00:37, 33.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|██▉ | 3.72G/4.98G [01:28<00:28, 44.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███ | 3.73G/4.98G [01:28<00:28, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███ | 3.74G/4.98G [01:29<00:28, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███ | 3.75G/4.98G [01:29<00:28, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.76G/4.98G [01:29<00:28, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.77G/4.98G [01:29<00:28, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.79G/4.98G [01:30<00:28, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.80G/4.98G [01:30<00:27, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.81G/4.98G [01:30<00:27, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.82G/4.98G [01:30<00:28, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.83G/4.98G [01:31<00:27, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.84G/4.98G [01:31<00:27, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.85G/4.98G [01:31<00:26, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.86G/4.98G [01:31<00:26, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.87G/4.98G [01:32<00:26, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.88G/4.98G [01:32<00:26, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███▏| 3.89G/4.98G [01:32<00:25, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███▏| 3.90G/4.98G [01:32<00:25, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.91G/4.98G [01:33<00:25, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.92G/4.98G [01:33<00:25, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.93G/4.98G [01:33<00:24, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.94G/4.98G [01:33<00:24, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.95G/4.98G [01:34<00:24, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 3.96G/4.98G [01:34<00:23, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 3.97G/4.98G [01:34<00:23, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 3.98G/4.98G [01:34<00:23, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 4.00G/4.98G [01:35<00:23, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 4.01G/4.98G [01:35<00:23, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.02G/4.98G [01:35<00:22, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.03G/4.98G [01:35<00:22, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.04G/4.98G [01:36<00:22, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▎| 4.05G/4.98G [01:36<00:23, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.06G/4.98G [01:36<00:22, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.07G/4.98G [01:36<00:21, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.08G/4.98G [01:37<00:21, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.09G/4.98G [01:37<00:21, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.10G/4.98G [01:37<00:20, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.11G/4.98G [01:37<00:21, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.12G/4.98G [01:38<00:20, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.13G/4.98G [01:38<00:20, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.14G/4.98G [01:38<00:19, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.15G/4.98G [01:38<00:19, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.16G/4.98G [01:39<00:19, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.17G/4.98G [01:39<00:18, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.18G/4.98G [01:39<00:18, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.19G/4.98G [01:39<00:18, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▍| 4.20G/4.98G [01:40<00:18, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.22G/4.98G [01:40<00:17, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.23G/4.98G [01:40<00:17, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.24G/4.98G [01:40<00:17, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.25G/4.98G [01:41<00:17, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.26G/4.98G [01:41<00:16, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.27G/4.98G [01:41<00:16, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.28G/4.98G [01:41<00:16, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.29G/4.98G [01:42<00:16, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.30G/4.98G [01:42<00:15, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.31G/4.98G [01:42<00:15, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.32G/4.98G [01:42<00:15, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.33G/4.98G [01:43<00:15, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.34G/4.98G [01:43<00:15, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.35G/4.98G [01:43<00:14, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.36G/4.98G [01:44<00:17, 34.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.38G/4.98G [01:44<00:13, 43.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.39G/4.98G [01:44<00:13, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.40G/4.98G [01:44<00:13, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.41G/4.98G [01:45<00:13, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.42G/4.98G [01:45<00:12, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.44G/4.98G [01:45<00:12, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.45G/4.98G [01:45<00:12, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.46G/4.98G [01:46<00:12, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.47G/4.98G [01:46<00:12, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.48G/4.98G [01:46<00:12, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.49G/4.98G [01:46<00:12, 39.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.50G/4.98G [01:47<00:12, 39.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▌| 4.51G/4.98G [01:47<00:11, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.52G/4.98G [01:47<00:11, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.53G/4.98G [01:47<00:10, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.54G/4.98G [01:48<00:10, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.55G/4.98G [01:48<00:10, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.56G/4.98G [01:48<00:09, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.57G/4.98G [01:48<00:09, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.58G/4.98G [01:49<00:09, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.59G/4.98G [01:49<00:09, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.60G/4.98G [01:49<00:08, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.61G/4.98G [01:49<00:08, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.62G/4.98G [01:50<00:08, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.63G/4.98G [01:50<00:08, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.65G/4.98G [01:50<00:07, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▋| 4.66G/4.98G [01:50<00:07, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.67G/4.98G [01:51<00:07, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.68G/4.98G [01:51<00:07, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.69G/4.98G [01:51<00:06, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.70G/4.98G [01:51<00:06, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.71G/4.98G [01:52<00:06, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.72G/4.98G [01:52<00:06, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.73G/4.98G [01:52<00:05, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.74G/4.98G [01:52<00:05, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.75G/4.98G [01:53<00:05, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.76G/4.98G [01:53<00:05, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.77G/4.98G [01:53<00:04, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.78G/4.98G [01:53<00:04, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.79G/4.98G [01:54<00:04, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.80G/4.98G [01:54<00:04, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▊| 4.81G/4.98G [01:54<00:03, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.82G/4.98G [01:54<00:03, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.83G/4.98G [01:55<00:03, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.84G/4.98G [01:55<00:03, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.85G/4.98G [01:55<00:02, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.87G/4.98G [01:55<00:02, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.88G/4.98G [01:56<00:02, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.89G/4.98G [01:56<00:02, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.90G/4.98G [01:56<00:01, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.91G/4.98G [01:56<00:01, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.92G/4.98G [01:57<00:01, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.93G/4.98G [01:57<00:01, 39.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.94G/4.98G [01:57<00:00, 39.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.95G/4.98G [01:57<00:00, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|███▉| 4.96G/4.98G [01:58<00:00, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|███▉| 4.97G/4.98G [01:58<00:00, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|████| 4.98G/4.98G [01:58<00:00, 42.0MB/s]\u001b[A\n",
      "Downloading shards:  25%|██████                  | 1/4 [01:58<05:56, 118.82s/it]\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0%|    | 10.5M/5.00G [00:00<01:56, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0%|    | 21.0M/5.00G [00:00<01:56, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 31.5M/5.00G [00:00<02:45, 30.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 52.4M/5.00G [00:01<01:47, 45.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 62.9M/5.00G [00:01<01:50, 44.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 73.4M/5.00G [00:01<01:51, 44.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|    | 83.9M/5.00G [00:01<01:52, 43.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|    | 94.4M/5.00G [00:02<01:52, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|     | 105M/5.00G [00:02<01:53, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|     | 115M/5.00G [00:02<01:54, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 126M/5.00G [00:02<01:53, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 136M/5.00G [00:03<01:53, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 147M/5.00G [00:03<01:53, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 157M/5.00G [00:03<01:53, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 168M/5.00G [00:03<01:53, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 178M/5.00G [00:04<01:52, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 189M/5.00G [00:04<01:53, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 199M/5.00G [00:04<01:55, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 210M/5.00G [00:04<01:54, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 220M/5.00G [00:05<01:53, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▏    | 231M/5.00G [00:05<01:52, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▏    | 241M/5.00G [00:05<01:53, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎    | 252M/5.00G [00:05<01:52, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎    | 262M/5.00G [00:06<01:52, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎    | 273M/5.00G [00:06<01:51, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎    | 283M/5.00G [00:06<01:50, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎    | 294M/5.00G [00:06<01:50, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎    | 304M/5.00G [00:07<01:50, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎    | 315M/5.00G [00:07<01:50, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 325M/5.00G [00:07<01:50, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 336M/5.00G [00:07<01:51, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 346M/5.00G [00:08<01:50, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 357M/5.00G [00:08<01:50, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 367M/5.00G [00:08<01:52, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 377M/5.00G [00:08<01:51, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 388M/5.00G [00:09<01:50, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 398M/5.00G [00:09<01:49, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 409M/5.00G [00:09<01:48, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 419M/5.00G [00:09<01:48, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 430M/5.00G [00:10<01:48, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 440M/5.00G [00:10<01:48, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 451M/5.00G [00:10<01:47, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 461M/5.00G [00:10<01:47, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 472M/5.00G [00:11<01:46, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▍    | 482M/5.00G [00:11<01:46, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▍    | 493M/5.00G [00:11<01:46, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▌    | 503M/5.00G [00:11<01:45, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▌    | 514M/5.00G [00:12<01:45, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▌    | 524M/5.00G [00:12<01:45, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 535M/5.00G [00:12<01:45, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 545M/5.00G [00:12<01:45, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 556M/5.00G [00:13<01:45, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 566M/5.00G [00:13<01:44, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 577M/5.00G [00:13<01:43, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 587M/5.00G [00:13<01:43, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 598M/5.00G [00:14<01:42, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 608M/5.00G [00:14<01:42, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 619M/5.00G [00:14<01:43, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 629M/5.00G [00:14<01:50, 39.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 640M/5.00G [00:15<01:48, 40.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 650M/5.00G [00:15<01:46, 40.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 661M/5.00G [00:15<01:44, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 671M/5.00G [00:15<01:43, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 682M/5.00G [00:16<01:43, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 692M/5.00G [00:16<01:42, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 703M/5.00G [00:16<02:01, 35.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 724M/5.00G [00:17<01:36, 44.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▋    | 734M/5.00G [00:17<01:37, 43.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▋    | 744M/5.00G [00:17<01:37, 43.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▊    | 755M/5.00G [00:17<01:38, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▊    | 765M/5.00G [00:18<01:38, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 776M/5.00G [00:18<01:38, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 786M/5.00G [00:18<01:37, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 797M/5.00G [00:18<01:37, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 807M/5.00G [00:19<01:37, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 818M/5.00G [00:19<01:37, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 828M/5.00G [00:19<01:37, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 839M/5.00G [00:19<01:37, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 849M/5.00G [00:20<01:37, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 860M/5.00G [00:20<01:37, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 870M/5.00G [00:20<01:37, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 881M/5.00G [00:20<01:37, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 891M/5.00G [00:21<01:36, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 902M/5.00G [00:21<01:36, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 912M/5.00G [00:21<01:35, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 923M/5.00G [00:21<01:35, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 933M/5.00G [00:22<01:35, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 944M/5.00G [00:22<01:35, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 954M/5.00G [00:22<01:35, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 965M/5.00G [00:22<01:35, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 975M/5.00G [00:23<01:34, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 986M/5.00G [00:23<01:34, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 996M/5.00G [00:23<01:34, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▊   | 1.01G/5.00G [00:23<01:37, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▊   | 1.02G/5.00G [00:24<01:36, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.03G/5.00G [00:24<01:35, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.04G/5.00G [00:24<01:34, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.05G/5.00G [00:24<01:34, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.06G/5.00G [00:25<01:42, 38.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.07G/5.00G [00:25<01:39, 39.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▊   | 1.08G/5.00G [00:25<01:37, 40.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▊   | 1.09G/5.00G [00:25<01:35, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▉   | 1.10G/5.00G [00:26<01:34, 41.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▉   | 1.11G/5.00G [00:26<01:32, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▉   | 1.12G/5.00G [00:26<01:32, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.13G/5.00G [00:26<01:31, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.14G/5.00G [00:27<01:31, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.15G/5.00G [00:27<01:30, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.16G/5.00G [00:27<01:30, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.17G/5.00G [00:27<01:30, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|▉   | 1.18G/5.00G [00:28<01:29, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|▉   | 1.20G/5.00G [00:28<01:29, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|▉   | 1.21G/5.00G [00:28<01:29, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|▉   | 1.22G/5.00G [00:28<01:29, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|▉   | 1.23G/5.00G [00:29<01:28, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|▉   | 1.24G/5.00G [00:29<01:28, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|▉   | 1.25G/5.00G [00:29<01:28, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█   | 1.26G/5.00G [00:29<01:28, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█   | 1.27G/5.00G [00:30<01:27, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.28G/5.00G [00:30<01:27, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.29G/5.00G [00:30<01:27, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.30G/5.00G [00:30<01:27, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.31G/5.00G [00:31<01:27, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.32G/5.00G [00:31<01:27, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.33G/5.00G [00:31<01:26, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.34G/5.00G [00:31<01:25, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.35G/5.00G [00:32<01:25, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.36G/5.00G [00:32<01:25, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.37G/5.00G [00:32<01:25, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█   | 1.38G/5.00G [00:32<01:24, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█   | 1.39G/5.00G [00:33<01:35, 37.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█   | 1.41G/5.00G [00:33<01:20, 44.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█▏  | 1.42G/5.00G [00:33<01:21, 43.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.43G/5.00G [00:33<01:22, 43.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.44G/5.00G [00:34<01:22, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.45G/5.00G [00:34<01:22, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.46G/5.00G [00:34<01:22, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.47G/5.00G [00:34<01:22, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.48G/5.00G [00:35<01:22, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.49G/5.00G [00:35<01:25, 41.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.50G/5.00G [00:35<01:24, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.51G/5.00G [00:35<01:23, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.52G/5.00G [00:36<01:23, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.53G/5.00G [00:36<01:22, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.54G/5.00G [00:36<01:21, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.55G/5.00G [00:36<01:21, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.56G/5.00G [00:37<01:21, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▎  | 1.57G/5.00G [00:37<01:20, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.58G/5.00G [00:37<01:20, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.59G/5.00G [00:37<01:20, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.60G/5.00G [00:38<01:19, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.61G/5.00G [00:38<01:20, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.63G/5.00G [00:38<01:19, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.64G/5.00G [00:38<01:19, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.65G/5.00G [00:39<01:20, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.66G/5.00G [00:39<01:19, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.67G/5.00G [00:39<01:18, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▎  | 1.68G/5.00G [00:39<01:18, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▎  | 1.69G/5.00G [00:39<01:18, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▎  | 1.70G/5.00G [00:40<01:17, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▎  | 1.71G/5.00G [00:40<01:17, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▍  | 1.72G/5.00G [00:40<01:16, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.73G/5.00G [00:40<01:17, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.74G/5.00G [00:41<01:16, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.75G/5.00G [00:41<01:16, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.76G/5.00G [00:41<01:16, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.77G/5.00G [00:41<01:16, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.78G/5.00G [00:42<01:15, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.79G/5.00G [00:42<01:14, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.80G/5.00G [00:42<01:14, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.81G/5.00G [00:42<01:14, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.82G/5.00G [00:43<01:14, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.84G/5.00G [00:43<01:14, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.85G/5.00G [00:43<01:14, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.86G/5.00G [00:43<01:13, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.87G/5.00G [00:44<01:13, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.88G/5.00G [00:44<01:13, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.89G/5.00G [00:44<01:13, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.90G/5.00G [00:44<01:13, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.91G/5.00G [00:45<01:12, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.92G/5.00G [00:45<01:12, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.93G/5.00G [00:45<01:17, 39.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.94G/5.00G [00:45<01:15, 40.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.95G/5.00G [00:46<01:14, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.96G/5.00G [00:46<01:13, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.97G/5.00G [00:46<01:12, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 1.98G/5.00G [00:46<01:11, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 1.99G/5.00G [00:47<01:11, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 2.00G/5.00G [00:47<01:11, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 2.01G/5.00G [00:47<01:10, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 2.02G/5.00G [00:47<01:09, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.03G/5.00G [00:48<01:24, 35.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.06G/5.00G [00:48<01:05, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.07G/5.00G [00:48<01:06, 43.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.08G/5.00G [00:49<01:07, 43.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.09G/5.00G [00:49<01:06, 43.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.10G/5.00G [00:49<01:06, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.11G/5.00G [00:49<01:06, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.12G/5.00G [00:50<01:06, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.13G/5.00G [00:50<01:07, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.14G/5.00G [00:50<01:06, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.15G/5.00G [00:50<01:06, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.16G/5.00G [00:51<01:06, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.17G/5.00G [00:51<01:06, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▋  | 2.18G/5.00G [00:51<01:06, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.19G/5.00G [00:51<01:06, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.20G/5.00G [00:52<01:05, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.21G/5.00G [00:52<01:05, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.22G/5.00G [00:52<01:05, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.23G/5.00G [00:52<01:05, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.24G/5.00G [00:53<01:05, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.25G/5.00G [00:53<01:04, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.26G/5.00G [00:53<01:04, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.28G/5.00G [00:53<01:04, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.29G/5.00G [00:54<01:04, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.30G/5.00G [00:54<01:04, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.31G/5.00G [00:54<01:03, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.32G/5.00G [00:54<01:03, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▊  | 2.33G/5.00G [00:55<01:02, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▊  | 2.34G/5.00G [00:55<01:02, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▉  | 2.35G/5.00G [00:55<01:01, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▉  | 2.36G/5.00G [00:55<01:03, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▉  | 2.37G/5.00G [00:56<01:02, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.38G/5.00G [00:56<01:02, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.39G/5.00G [00:56<01:02, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.40G/5.00G [00:56<01:01, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.41G/5.00G [00:57<01:00, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.42G/5.00G [00:57<01:00, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.43G/5.00G [00:57<01:00, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.44G/5.00G [00:57<01:00, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.45G/5.00G [00:58<00:59, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.46G/5.00G [00:58<00:59, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.47G/5.00G [00:58<00:59, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|█▉  | 2.49G/5.00G [00:58<00:59, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|█▉  | 2.50G/5.00G [00:59<00:58, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|██  | 2.51G/5.00G [00:59<00:59, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|██  | 2.52G/5.00G [00:59<00:58, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.53G/5.00G [00:59<00:58, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.54G/5.00G [01:00<00:58, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.55G/5.00G [01:00<00:58, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.56G/5.00G [01:00<00:57, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.57G/5.00G [01:00<00:57, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.58G/5.00G [01:01<00:56, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.59G/5.00G [01:01<00:56, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.60G/5.00G [01:01<00:56, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.61G/5.00G [01:01<00:56, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.62G/5.00G [01:02<00:55, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██  | 2.63G/5.00G [01:02<00:55, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██  | 2.64G/5.00G [01:02<00:55, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██  | 2.65G/5.00G [01:02<00:55, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██▏ | 2.66G/5.00G [01:03<00:55, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██▏ | 2.67G/5.00G [01:03<00:54, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▏ | 2.68G/5.00G [01:03<00:54, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▏ | 2.69G/5.00G [01:03<00:54, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▏ | 2.71G/5.00G [01:04<01:02, 36.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▏ | 2.72G/5.00G [01:04<00:52, 43.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.73G/5.00G [01:04<00:52, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.74G/5.00G [01:04<00:52, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.75G/5.00G [01:05<00:52, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.76G/5.00G [01:05<00:52, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.77G/5.00G [01:05<00:52, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.78G/5.00G [01:05<00:52, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.79G/5.00G [01:06<00:53, 41.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.80G/5.00G [01:06<00:53, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.81G/5.00G [01:06<00:52, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▎ | 2.82G/5.00G [01:06<00:51, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.83G/5.00G [01:07<00:51, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.84G/5.00G [01:07<00:51, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.85G/5.00G [01:07<00:50, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.86G/5.00G [01:07<00:50, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.87G/5.00G [01:07<00:50, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▎ | 2.88G/5.00G [01:08<00:49, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▎ | 2.89G/5.00G [01:08<00:49, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▎ | 2.90G/5.00G [01:08<00:49, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▎ | 2.92G/5.00G [01:08<00:49, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.93G/5.00G [01:09<00:48, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.94G/5.00G [01:09<00:48, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.95G/5.00G [01:09<00:48, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.96G/5.00G [01:09<00:48, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.97G/5.00G [01:10<00:47, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 2.98G/5.00G [01:10<00:47, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 2.99G/5.00G [01:10<00:47, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 3.00G/5.00G [01:10<00:47, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 3.01G/5.00G [01:11<00:46, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 3.02G/5.00G [01:11<00:46, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.03G/5.00G [01:11<00:46, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.04G/5.00G [01:11<00:46, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.05G/5.00G [01:12<00:46, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.06G/5.00G [01:12<00:45, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.07G/5.00G [01:12<00:45, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.08G/5.00G [01:12<00:45, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.09G/5.00G [01:13<00:44, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.10G/5.00G [01:13<00:44, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.11G/5.00G [01:13<00:44, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.12G/5.00G [01:13<00:44, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.14G/5.00G [01:14<00:44, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.15G/5.00G [01:14<00:44, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.16G/5.00G [01:14<00:43, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.17G/5.00G [01:14<00:43, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.18G/5.00G [01:15<00:43, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.19G/5.00G [01:15<00:42, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.20G/5.00G [01:15<00:42, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.21G/5.00G [01:15<00:42, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.22G/5.00G [01:16<00:44, 40.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.23G/5.00G [01:16<00:43, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.24G/5.00G [01:16<00:42, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.25G/5.00G [01:16<00:41, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.26G/5.00G [01:17<00:41, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.27G/5.00G [01:17<00:40, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.28G/5.00G [01:17<00:40, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.29G/5.00G [01:17<00:40, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.30G/5.00G [01:18<00:39, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.31G/5.00G [01:18<00:39, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.32G/5.00G [01:18<00:39, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.33G/5.00G [01:18<00:38, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.34G/5.00G [01:19<00:38, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.36G/5.00G [01:19<00:50, 32.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.38G/5.00G [01:19<00:35, 45.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.39G/5.00G [01:20<00:35, 44.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.40G/5.00G [01:20<00:36, 44.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.41G/5.00G [01:20<00:36, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.42G/5.00G [01:20<00:36, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▋ | 3.43G/5.00G [01:21<00:36, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.44G/5.00G [01:21<00:36, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.45G/5.00G [01:21<00:36, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.46G/5.00G [01:21<00:36, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.47G/5.00G [01:22<00:36, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.48G/5.00G [01:22<00:36, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.49G/5.00G [01:22<00:35, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.50G/5.00G [01:22<00:35, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.51G/5.00G [01:23<00:35, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.52G/5.00G [01:23<00:34, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.53G/5.00G [01:23<00:34, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.54G/5.00G [01:23<00:34, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.55G/5.00G [01:24<00:34, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.57G/5.00G [01:24<00:33, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▊ | 3.58G/5.00G [01:24<00:33, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▊ | 3.59G/5.00G [01:24<00:33, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▉ | 3.60G/5.00G [01:25<00:33, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▉ | 3.61G/5.00G [01:25<00:32, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▉ | 3.62G/5.00G [01:25<00:32, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.63G/5.00G [01:25<00:32, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.64G/5.00G [01:26<00:31, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.65G/5.00G [01:26<00:33, 40.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.66G/5.00G [01:26<00:32, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.67G/5.00G [01:26<00:32, 41.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.68G/5.00G [01:27<00:31, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.69G/5.00G [01:27<00:31, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.70G/5.00G [01:27<00:30, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.71G/5.00G [01:27<00:30, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.72G/5.00G [01:28<00:30, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|██▉ | 3.73G/5.00G [01:28<00:29, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|██▉ | 3.74G/5.00G [01:28<00:29, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|███ | 3.75G/5.00G [01:28<00:29, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|███ | 3.76G/5.00G [01:29<00:29, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.77G/5.00G [01:29<00:28, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.79G/5.00G [01:29<00:28, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.80G/5.00G [01:29<00:28, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.81G/5.00G [01:30<00:27, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.82G/5.00G [01:30<00:27, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.83G/5.00G [01:30<00:27, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.84G/5.00G [01:30<00:27, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.85G/5.00G [01:31<00:27, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.86G/5.00G [01:31<00:26, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.87G/5.00G [01:31<00:26, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███ | 3.88G/5.00G [01:31<00:26, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███ | 3.89G/5.00G [01:32<00:26, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███ | 3.90G/5.00G [01:32<00:25, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▏| 3.91G/5.00G [01:32<00:25, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▏| 3.92G/5.00G [01:32<00:25, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.93G/5.00G [01:33<00:25, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.94G/5.00G [01:33<00:24, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.95G/5.00G [01:33<00:24, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.96G/5.00G [01:33<00:24, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.97G/5.00G [01:34<00:24, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 3.98G/5.00G [01:34<00:24, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 4.00G/5.00G [01:34<00:23, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 4.01G/5.00G [01:34<00:23, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 4.02G/5.00G [01:35<00:23, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.03G/5.00G [01:35<00:23, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.04G/5.00G [01:35<00:26, 36.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.05G/5.00G [01:35<00:21, 43.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.06G/5.00G [01:36<00:21, 43.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▎| 4.07G/5.00G [01:36<00:21, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.08G/5.00G [01:36<00:21, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.09G/5.00G [01:36<00:21, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.10G/5.00G [01:37<00:21, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.11G/5.00G [01:37<00:20, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.12G/5.00G [01:37<00:20, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.13G/5.00G [01:37<00:20, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.14G/5.00G [01:37<00:20, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.15G/5.00G [01:38<00:19, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.16G/5.00G [01:38<00:19, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.17G/5.00G [01:38<00:19, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.18G/5.00G [01:38<00:19, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.19G/5.00G [01:39<00:18, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.20G/5.00G [01:39<00:18, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.22G/5.00G [01:39<00:18, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.23G/5.00G [01:39<00:18, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.24G/5.00G [01:40<00:17, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.25G/5.00G [01:40<00:17, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.26G/5.00G [01:40<00:17, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.27G/5.00G [01:40<00:17, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.28G/5.00G [01:41<00:16, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.29G/5.00G [01:41<00:16, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.30G/5.00G [01:41<00:16, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.31G/5.00G [01:41<00:16, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.32G/5.00G [01:42<00:15, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.33G/5.00G [01:42<00:15, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.34G/5.00G [01:42<00:15, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.35G/5.00G [01:42<00:15, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.36G/5.00G [01:43<00:15, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.37G/5.00G [01:43<00:14, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.38G/5.00G [01:43<00:14, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.39G/5.00G [01:43<00:14, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.40G/5.00G [01:44<00:14, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.41G/5.00G [01:44<00:13, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.42G/5.00G [01:44<00:13, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.44G/5.00G [01:44<00:13, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.45G/5.00G [01:45<00:12, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.46G/5.00G [01:45<00:12, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.47G/5.00G [01:45<00:12, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.48G/5.00G [01:46<00:13, 37.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.49G/5.00G [01:46<00:11, 44.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.50G/5.00G [01:46<00:11, 43.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.51G/5.00G [01:46<00:11, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.52G/5.00G [01:46<00:11, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▌| 4.53G/5.00G [01:47<00:10, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.54G/5.00G [01:47<00:10, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.55G/5.00G [01:47<00:10, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.56G/5.00G [01:47<00:10, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.57G/5.00G [01:48<00:10, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.58G/5.00G [01:48<00:09, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.59G/5.00G [01:48<00:09, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.60G/5.00G [01:48<00:09, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.61G/5.00G [01:49<00:09, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.62G/5.00G [01:49<00:08, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.63G/5.00G [01:49<00:08, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.65G/5.00G [01:49<00:08, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.66G/5.00G [01:50<00:08, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.67G/5.00G [01:50<00:07, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▋| 4.68G/5.00G [01:50<00:07, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▋| 4.69G/5.00G [01:50<00:07, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 4.70G/5.00G [01:51<00:07, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 4.71G/5.00G [01:51<00:06, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 4.72G/5.00G [01:51<00:06, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.73G/5.00G [01:51<00:06, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.74G/5.00G [01:52<00:06, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.75G/5.00G [01:52<00:05, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.76G/5.00G [01:52<00:05, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.77G/5.00G [01:52<00:05, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.78G/5.00G [01:53<00:05, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.79G/5.00G [01:53<00:04, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.80G/5.00G [01:53<00:04, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.81G/5.00G [01:53<00:04, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.82G/5.00G [01:54<00:04, 41.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▊| 4.83G/5.00G [01:54<00:03, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▉| 4.84G/5.00G [01:54<00:03, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▉| 4.85G/5.00G [01:54<00:03, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▉| 4.87G/5.00G [01:55<00:03, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.88G/5.00G [01:55<00:02, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.89G/5.00G [01:55<00:02, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.90G/5.00G [01:55<00:02, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.91G/5.00G [01:56<00:02, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.92G/5.00G [01:56<00:01, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.93G/5.00G [01:56<00:01, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.94G/5.00G [01:56<00:01, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.95G/5.00G [01:57<00:01, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.96G/5.00G [01:57<00:00, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.97G/5.00G [01:57<00:00, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|███▉| 4.98G/5.00G [01:57<00:00, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|███▉| 4.99G/5.00G [01:58<00:00, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|████| 5.00G/5.00G [01:58<00:00, 42.3MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████            | 2/4 [03:57<03:57, 118.64s/it]\n",
      "model-00003-of-00004.safetensors:   0%|             | 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0%|    | 10.5M/4.92G [00:00<02:01, 40.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0%|    | 21.0M/4.92G [00:00<01:58, 41.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 31.5M/4.92G [00:00<01:56, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 41.9M/4.92G [00:00<01:54, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 52.4M/4.92G [00:01<01:54, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 62.9M/4.92G [00:01<01:53, 42.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 73.4M/4.92G [00:01<01:53, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|    | 83.9M/4.92G [00:01<01:53, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|    | 94.4M/4.92G [00:02<01:54, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|     | 105M/4.92G [00:02<01:53, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|     | 115M/4.92G [00:02<01:53, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 126M/4.92G [00:02<01:53, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 136M/4.92G [00:03<01:53, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 147M/4.92G [00:03<01:52, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 157M/4.92G [00:03<01:52, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 168M/4.92G [00:03<01:51, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 178M/4.92G [00:04<01:51, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 189M/4.92G [00:04<01:51, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 199M/4.92G [00:04<01:50, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 210M/4.92G [00:04<01:50, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 220M/4.92G [00:05<01:50, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▏    | 231M/4.92G [00:05<01:50, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▏    | 241M/4.92G [00:05<01:49, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▎    | 252M/4.92G [00:05<01:49, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▎    | 262M/4.92G [00:06<01:49, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 273M/4.92G [00:06<01:49, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 283M/4.92G [00:06<01:49, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 294M/4.92G [00:06<01:48, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 304M/4.92G [00:07<01:48, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 315M/4.92G [00:07<01:48, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 325M/4.92G [00:07<01:47, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 336M/4.92G [00:07<01:47, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 346M/4.92G [00:08<01:47, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 357M/4.92G [00:08<01:46, 42.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 367M/4.92G [00:08<02:00, 37.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 377M/4.92G [00:08<01:42, 44.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 388M/4.92G [00:09<01:42, 44.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 398M/4.92G [00:09<01:43, 43.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 409M/4.92G [00:09<01:43, 43.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 419M/4.92G [00:09<01:44, 43.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 430M/4.92G [00:10<01:44, 42.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 440M/4.92G [00:10<01:44, 42.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 451M/4.92G [00:10<01:44, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 461M/4.92G [00:10<01:44, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▍    | 472M/4.92G [00:11<01:43, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▍    | 482M/4.92G [00:11<01:44, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▌    | 493M/4.92G [00:11<01:44, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▌    | 503M/4.92G [00:11<01:43, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▌    | 514M/4.92G [00:12<01:43, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 524M/4.92G [00:12<01:42, 42.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 535M/4.92G [00:12<01:42, 42.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 545M/4.92G [00:12<01:42, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 556M/4.92G [00:13<01:42, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 566M/4.92G [00:13<01:43, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 577M/4.92G [00:13<01:42, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 587M/4.92G [00:13<01:42, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 598M/4.92G [00:14<01:42, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 608M/4.92G [00:14<01:41, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 619M/4.92G [00:14<01:41, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 629M/4.92G [00:14<01:41, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 640M/4.92G [00:15<01:40, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 650M/4.92G [00:15<01:40, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 661M/4.92G [00:15<01:41, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▋    | 671M/4.92G [00:15<01:40, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▋    | 682M/4.92G [00:16<01:39, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▋    | 692M/4.92G [00:16<01:39, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▋    | 703M/4.92G [00:16<01:39, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▋    | 713M/4.92G [00:16<01:38, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▋    | 724M/4.92G [00:17<01:38, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▋    | 734M/4.92G [00:17<01:38, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▊    | 744M/4.92G [00:17<01:37, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▊    | 755M/4.92G [00:17<01:37, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 765M/4.92G [00:18<01:37, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 776M/4.92G [00:18<01:37, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 786M/4.92G [00:18<01:37, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 797M/4.92G [00:18<01:37, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 807M/4.92G [00:19<01:37, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 818M/4.92G [00:19<01:36, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 828M/4.92G [00:19<01:36, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 839M/4.92G [00:19<01:36, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 849M/4.92G [00:20<01:36, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 860M/4.92G [00:20<01:25, 47.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 870M/4.92G [00:20<01:14, 54.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 881M/4.92G [00:20<01:33, 43.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 891M/4.92G [00:20<01:33, 43.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 902M/4.92G [00:21<01:33, 42.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 912M/4.92G [00:21<01:33, 42.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 923M/4.92G [00:21<01:35, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 933M/4.92G [00:21<01:35, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 944M/4.92G [00:22<01:34, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 954M/4.92G [00:22<01:34, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|▉    | 965M/4.92G [00:22<01:34, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|▉    | 975M/4.92G [00:22<01:32, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|█    | 986M/4.92G [00:23<01:32, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|█    | 996M/4.92G [00:23<01:32, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|▊   | 1.01G/4.92G [00:23<01:34, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.02G/4.92G [00:23<01:32, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.03G/4.92G [00:24<01:32, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.04G/4.92G [00:24<01:31, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.05G/4.92G [00:24<01:33, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▊   | 1.06G/4.92G [00:24<01:32, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▊   | 1.07G/4.92G [00:25<01:32, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.08G/4.92G [00:25<01:31, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.09G/4.92G [00:25<01:31, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.10G/4.92G [00:25<01:30, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.11G/4.92G [00:26<01:29, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.12G/4.92G [00:26<01:29, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.13G/4.92G [00:26<01:31, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.14G/4.92G [00:26<01:29, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.15G/4.92G [00:27<01:29, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.16G/4.92G [00:27<01:28, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.17G/4.92G [00:27<01:30, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.18G/4.92G [00:27<01:32, 40.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.20G/4.92G [00:28<01:27, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|▉   | 1.21G/4.92G [00:28<01:26, 42.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|▉   | 1.22G/4.92G [00:28<01:28, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|▉   | 1.23G/4.92G [00:28<01:27, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|█   | 1.24G/4.92G [00:29<01:27, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|█   | 1.25G/4.92G [00:29<01:26, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.26G/4.92G [00:29<01:28, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.27G/4.92G [00:29<01:26, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.28G/4.92G [00:30<01:25, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.29G/4.92G [00:30<01:25, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.30G/4.92G [00:30<01:27, 41.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█   | 1.31G/4.92G [00:30<01:26, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█   | 1.32G/4.92G [00:31<01:25, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█   | 1.33G/4.92G [00:31<01:24, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█   | 1.34G/4.92G [00:31<01:26, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█   | 1.35G/4.92G [00:31<01:33, 38.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█   | 1.36G/4.92G [00:32<01:28, 40.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█   | 1.37G/4.92G [00:32<01:26, 40.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█▏  | 1.38G/4.92G [00:32<01:25, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█▏  | 1.39G/4.92G [00:32<01:24, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.41G/4.92G [00:33<01:23, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.42G/4.92G [00:33<01:22, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.43G/4.92G [00:33<01:22, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.44G/4.92G [00:34<01:41, 34.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.45G/4.92G [00:34<01:20, 42.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▏  | 1.46G/4.92G [00:34<01:16, 45.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▏  | 1.47G/4.92G [00:34<01:17, 44.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▏  | 1.48G/4.92G [00:34<01:18, 43.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▏  | 1.49G/4.92G [00:35<01:18, 43.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▏  | 1.50G/4.92G [00:35<01:18, 43.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▏  | 1.51G/4.92G [00:35<01:18, 43.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▏  | 1.52G/4.92G [00:35<01:19, 42.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▏  | 1.53G/4.92G [00:36<01:19, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▎  | 1.54G/4.92G [00:36<01:19, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.55G/4.92G [00:36<01:19, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.56G/4.92G [00:36<01:18, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.57G/4.92G [00:37<01:18, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.58G/4.92G [00:37<01:18, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.59G/4.92G [00:37<01:19, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.60G/4.92G [00:37<01:18, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.61G/4.92G [00:38<01:17, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.63G/4.92G [00:38<01:17, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.64G/4.92G [00:38<01:19, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.65G/4.92G [00:38<01:18, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.66G/4.92G [00:39<01:17, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.67G/4.92G [00:39<01:16, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.68G/4.92G [00:39<01:18, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.69G/4.92G [00:39<01:16, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.70G/4.92G [00:40<01:16, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.71G/4.92G [00:40<01:16, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.72G/4.92G [00:40<01:17, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.73G/4.92G [00:40<01:15, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.74G/4.92G [00:41<01:15, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.75G/4.92G [00:41<01:14, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.76G/4.92G [00:41<01:16, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.77G/4.92G [00:41<01:15, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.78G/4.92G [00:42<01:14, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.79G/4.92G [00:42<01:14, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▍  | 1.80G/4.92G [00:42<01:15, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▍  | 1.81G/4.92G [00:42<01:17, 40.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▍  | 1.82G/4.92G [00:43<01:16, 40.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▍  | 1.84G/4.92G [00:43<01:15, 41.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.85G/4.92G [00:43<01:14, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.86G/4.92G [00:43<01:13, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.87G/4.92G [00:44<01:12, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.88G/4.92G [00:44<01:12, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.89G/4.92G [00:44<01:12, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.90G/4.92G [00:44<01:10, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.91G/4.92G [00:45<01:10, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.92G/4.92G [00:45<01:10, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.93G/4.92G [00:45<01:10, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.94G/4.92G [00:45<01:09, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|█▌  | 1.95G/4.92G [00:46<01:09, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|█▌  | 1.96G/4.92G [00:46<01:09, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|█▌  | 1.97G/4.92G [00:46<01:09, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|█▌  | 1.98G/4.92G [00:47<01:28, 33.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▌  | 1.99G/4.92G [00:47<01:11, 40.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.00G/4.92G [00:47<01:02, 46.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.01G/4.92G [00:47<01:05, 44.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.02G/4.92G [00:47<01:05, 44.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.03G/4.92G [00:48<01:05, 43.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.04G/4.92G [00:48<01:09, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.06G/4.92G [00:48<01:06, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.07G/4.92G [00:48<01:06, 42.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.08G/4.92G [00:49<01:06, 42.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.09G/4.92G [00:49<01:06, 42.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.10G/4.92G [00:49<01:07, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.11G/4.92G [00:49<01:06, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.12G/4.92G [00:50<01:06, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.13G/4.92G [00:50<01:05, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▋  | 2.14G/4.92G [00:50<01:06, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▋  | 2.15G/4.92G [00:50<01:06, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▊  | 2.16G/4.92G [00:51<01:05, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▊  | 2.17G/4.92G [00:51<01:04, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▊  | 2.18G/4.92G [00:51<01:05, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.19G/4.92G [00:51<01:04, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.20G/4.92G [00:52<01:04, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.21G/4.92G [00:52<01:04, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.22G/4.92G [00:52<01:04, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.23G/4.92G [00:52<01:03, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.24G/4.92G [00:53<01:05, 41.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.25G/4.92G [00:53<01:07, 39.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.26G/4.92G [00:53<01:06, 40.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.28G/4.92G [00:53<01:05, 40.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.29G/4.92G [00:54<01:04, 41.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|█▊  | 2.30G/4.92G [00:54<01:02, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 2.31G/4.92G [00:54<01:02, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 2.32G/4.92G [00:54<01:01, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 2.33G/4.92G [00:55<01:01, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.34G/4.92G [00:55<01:01, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.35G/4.92G [00:55<01:00, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.36G/4.92G [00:55<01:00, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.37G/4.92G [00:56<01:00, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.38G/4.92G [00:56<00:59, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.39G/4.92G [00:56<00:59, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.40G/4.92G [00:56<01:00, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.41G/4.92G [00:57<00:59, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.42G/4.92G [00:57<00:59, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.43G/4.92G [00:57<00:59, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|█▉  | 2.44G/4.92G [00:57<00:58, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|█▉  | 2.45G/4.92G [00:58<00:58, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|██  | 2.46G/4.92G [00:58<00:57, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|██  | 2.47G/4.92G [00:58<01:05, 37.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.49G/4.92G [00:58<00:54, 44.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.50G/4.92G [00:59<00:55, 43.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.51G/4.92G [00:59<00:55, 43.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.52G/4.92G [00:59<00:55, 43.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.53G/4.92G [00:59<00:55, 43.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.54G/4.92G [01:00<00:55, 42.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.55G/4.92G [01:00<00:55, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.56G/4.92G [01:00<00:56, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.57G/4.92G [01:00<00:55, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.58G/4.92G [01:01<00:55, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██  | 2.59G/4.92G [01:01<00:54, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██  | 2.60G/4.92G [01:01<00:55, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██  | 2.61G/4.92G [01:01<00:55, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██▏ | 2.62G/4.92G [01:02<00:53, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.63G/4.92G [01:02<00:53, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.64G/4.92G [01:02<00:54, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.65G/4.92G [01:02<00:53, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.66G/4.92G [01:03<00:53, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.67G/4.92G [01:03<00:52, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.68G/4.92G [01:03<00:53, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.69G/4.92G [01:03<00:53, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.71G/4.92G [01:04<00:58, 37.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.72G/4.92G [01:04<00:56, 38.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.73G/4.92G [01:04<00:54, 40.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▏ | 2.74G/4.92G [01:04<00:53, 40.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▏ | 2.75G/4.92G [01:05<00:52, 41.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▏ | 2.76G/4.92G [01:05<00:52, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▎ | 2.77G/4.92G [01:05<00:51, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.78G/4.92G [01:05<00:50, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.79G/4.92G [01:06<00:50, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.80G/4.92G [01:06<00:49, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.81G/4.92G [01:06<00:49, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.82G/4.92G [01:06<00:49, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.83G/4.92G [01:07<00:48, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.84G/4.92G [01:07<00:48, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.85G/4.92G [01:07<00:48, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.86G/4.92G [01:07<00:48, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.87G/4.92G [01:08<00:48, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.88G/4.92G [01:08<00:48, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.89G/4.92G [01:08<00:47, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.90G/4.92G [01:08<00:47, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.92G/4.92G [01:09<00:47, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.93G/4.92G [01:09<00:46, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.94G/4.92G [01:09<00:46, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.95G/4.92G [01:09<00:46, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.96G/4.92G [01:10<00:46, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.97G/4.92G [01:10<00:45, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 2.98G/4.92G [01:10<00:45, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 2.99G/4.92G [01:10<00:45, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.00G/4.92G [01:11<00:44, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.01G/4.92G [01:11<00:44, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.02G/4.92G [01:11<00:45, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.03G/4.92G [01:11<00:44, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.04G/4.92G [01:12<00:54, 34.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.05G/4.92G [01:12<00:45, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.06G/4.92G [01:12<00:41, 45.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.07G/4.92G [01:12<00:41, 44.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.08G/4.92G [01:13<00:41, 43.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.09G/4.92G [01:13<00:42, 43.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.10G/4.92G [01:13<00:42, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.11G/4.92G [01:13<00:42, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.12G/4.92G [01:14<00:42, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.14G/4.92G [01:14<00:41, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.15G/4.92G [01:14<00:42, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.16G/4.92G [01:14<00:41, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.17G/4.92G [01:15<00:41, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.18G/4.92G [01:15<00:40, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.19G/4.92G [01:15<00:41, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.20G/4.92G [01:15<00:40, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.21G/4.92G [01:16<00:40, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.22G/4.92G [01:16<00:39, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.23G/4.92G [01:16<00:40, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.24G/4.92G [01:16<00:39, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.25G/4.92G [01:17<00:39, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.26G/4.92G [01:17<00:39, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.27G/4.92G [01:17<00:39, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.28G/4.92G [01:17<00:38, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.29G/4.92G [01:18<00:38, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.30G/4.92G [01:18<00:38, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.31G/4.92G [01:18<00:38, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.32G/4.92G [01:18<00:38, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.33G/4.92G [01:19<00:37, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.34G/4.92G [01:19<00:37, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.36G/4.92G [01:19<00:37, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.37G/4.92G [01:19<00:36, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|██▋ | 3.38G/4.92G [01:20<00:36, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|██▊ | 3.39G/4.92G [01:20<00:36, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|██▊ | 3.40G/4.92G [01:20<00:36, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|██▊ | 3.41G/4.92G [01:20<00:36, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.42G/4.92G [01:21<00:35, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.43G/4.92G [01:21<00:35, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.44G/4.92G [01:21<00:35, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.45G/4.92G [01:21<00:35, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.46G/4.92G [01:22<00:34, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.47G/4.92G [01:22<00:34, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.48G/4.92G [01:22<00:34, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.49G/4.92G [01:22<00:34, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.50G/4.92G [01:23<00:33, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.51G/4.92G [01:23<00:33, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|██▊ | 3.52G/4.92G [01:23<00:33, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|██▉ | 3.53G/4.92G [01:23<00:32, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|██▉ | 3.54G/4.92G [01:24<00:32, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|██▉ | 3.55G/4.92G [01:24<00:32, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.57G/4.92G [01:24<00:32, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.58G/4.92G [01:24<00:32, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.59G/4.92G [01:25<00:31, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.60G/4.92G [01:25<00:31, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.61G/4.92G [01:25<00:31, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.62G/4.92G [01:25<00:31, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.63G/4.92G [01:26<00:30, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.64G/4.92G [01:26<00:30, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.65G/4.92G [01:26<00:30, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.66G/4.92G [01:26<00:29, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|██▉ | 3.67G/4.92G [01:27<00:29, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|██▉ | 3.68G/4.92G [01:27<00:35, 34.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|███ | 3.70G/4.92G [01:27<00:27, 44.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.71G/4.92G [01:28<00:27, 43.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.72G/4.92G [01:28<00:27, 43.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.73G/4.92G [01:28<00:28, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.74G/4.92G [01:28<00:27, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.75G/4.92G [01:29<00:27, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.76G/4.92G [01:29<00:26, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.77G/4.92G [01:29<00:27, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.79G/4.92G [01:29<00:26, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.80G/4.92G [01:30<00:26, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.81G/4.92G [01:30<00:26, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███ | 3.82G/4.92G [01:30<00:26, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███ | 3.83G/4.92G [01:30<00:26, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███ | 3.84G/4.92G [01:31<00:25, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███▏| 3.85G/4.92G [01:31<00:25, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███▏| 3.86G/4.92G [01:31<00:25, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▏| 3.88G/4.92G [01:31<00:18, 56.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▏| 3.89G/4.92G [01:32<00:19, 52.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▏| 3.90G/4.92G [01:32<00:28, 36.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.91G/4.92G [01:32<00:26, 37.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.92G/4.92G [01:33<00:25, 39.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.93G/4.92G [01:33<00:24, 40.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.94G/4.92G [01:33<00:24, 40.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.95G/4.92G [01:33<00:23, 41.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▏| 3.96G/4.92G [01:34<00:22, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▏| 3.97G/4.92G [01:34<00:22, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▏| 3.98G/4.92G [01:34<00:22, 40.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▎| 4.00G/4.92G [01:34<00:22, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▎| 4.01G/4.92G [01:35<00:21, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.02G/4.92G [01:35<00:21, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.03G/4.92G [01:35<00:21, 41.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.04G/4.92G [01:35<00:20, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.05G/4.92G [01:36<00:20, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.06G/4.92G [01:36<00:20, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.07G/4.92G [01:36<00:20, 41.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.08G/4.92G [01:36<00:19, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.09G/4.92G [01:37<00:19, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.10G/4.92G [01:37<00:19, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.11G/4.92G [01:37<00:19, 41.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.12G/4.92G [01:37<00:18, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.13G/4.92G [01:38<00:18, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.14G/4.92G [01:38<00:18, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▍| 4.15G/4.92G [01:38<00:18, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.16G/4.92G [01:38<00:17, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.17G/4.92G [01:39<00:17, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.18G/4.92G [01:39<00:17, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.19G/4.92G [01:39<00:17, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.20G/4.92G [01:39<00:16, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.22G/4.92G [01:40<00:16, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.23G/4.92G [01:40<00:16, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.24G/4.92G [01:40<00:16, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.25G/4.92G [01:40<00:16, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.26G/4.92G [01:41<00:15, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.27G/4.92G [01:41<00:15, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.28G/4.92G [01:41<00:15, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.29G/4.92G [01:41<00:14, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.30G/4.92G [01:42<00:14, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.31G/4.92G [01:42<00:14, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.32G/4.92G [01:42<00:14, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.33G/4.92G [01:42<00:13, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.34G/4.92G [01:43<00:15, 37.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.35G/4.92G [01:43<00:12, 43.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.36G/4.92G [01:43<00:13, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.37G/4.92G [01:43<00:12, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.38G/4.92G [01:44<00:12, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.39G/4.92G [01:44<00:12, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.40G/4.92G [01:44<00:12, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.41G/4.92G [01:44<00:11, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.42G/4.92G [01:45<00:11, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.44G/4.92G [01:45<00:11, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.45G/4.92G [01:45<00:11, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.46G/4.92G [01:45<00:10, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.47G/4.92G [01:46<00:10, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.48G/4.92G [01:46<00:10, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.49G/4.92G [01:46<00:10, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.50G/4.92G [01:46<00:10, 40.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.51G/4.92G [01:47<00:09, 40.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.52G/4.92G [01:47<00:09, 41.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.53G/4.92G [01:47<00:09, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.54G/4.92G [01:47<00:08, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.55G/4.92G [01:48<00:08, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.56G/4.92G [01:48<00:08, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.57G/4.92G [01:48<00:08, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.58G/4.92G [01:48<00:07, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.59G/4.92G [01:49<00:07, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▋| 4.60G/4.92G [01:49<00:07, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▊| 4.61G/4.92G [01:49<00:07, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▊| 4.62G/4.92G [01:49<00:06, 42.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▊| 4.63G/4.92G [01:50<00:06, 42.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▊| 4.65G/4.92G [01:50<00:06, 42.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|███▊| 4.66G/4.92G [01:50<00:06, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|███▊| 4.67G/4.92G [01:50<00:05, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|███▊| 4.68G/4.92G [01:51<00:05, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|███▊| 4.69G/4.92G [01:51<00:05, 42.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.70G/4.92G [01:51<00:05, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.71G/4.92G [01:51<00:04, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.72G/4.92G [01:52<00:04, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.73G/4.92G [01:52<00:04, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.74G/4.92G [01:52<00:04, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▊| 4.75G/4.92G [01:52<00:03, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▊| 4.76G/4.92G [01:53<00:03, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▉| 4.77G/4.92G [01:53<00:03, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▉| 4.78G/4.92G [01:53<00:03, 41.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▉| 4.79G/4.92G [01:53<00:02, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|███▉| 4.80G/4.92G [01:54<00:02, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|███▉| 4.81G/4.92G [01:54<00:02, 41.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|███▉| 4.82G/4.92G [01:54<00:02, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|███▉| 4.83G/4.92G [01:54<00:01, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.84G/4.92G [01:55<00:01, 42.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.85G/4.92G [01:55<00:01, 42.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.87G/4.92G [01:55<00:01, 41.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.88G/4.92G [01:55<00:00, 41.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.89G/4.92G [01:56<00:00, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|███▉| 4.90G/4.92G [01:56<00:00, 42.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|███▉| 4.91G/4.92G [01:56<00:00, 41.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|████| 4.92G/4.92G [01:56<00:00, 42.1MB/s]\u001b[A\n",
      "Downloading shards:  75%|██████████████████      | 3/4 [05:54<01:57, 117.88s/it]\n",
      "model-00004-of-00004.safetensors:   0%|             | 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   1%|    | 10.5M/1.17G [00:00<00:39, 29.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   2%|    | 21.0M/1.17G [00:00<00:32, 35.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   3%|    | 31.5M/1.17G [00:00<00:29, 38.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   4%|▏   | 41.9M/1.17G [00:01<00:28, 40.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   4%|▏   | 52.4M/1.17G [00:01<00:27, 41.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   5%|▏   | 62.9M/1.17G [00:01<00:26, 41.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   6%|▎   | 73.4M/1.17G [00:01<00:26, 41.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   7%|▎   | 83.9M/1.17G [00:02<00:25, 42.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   8%|▎   | 94.4M/1.17G [00:02<00:25, 42.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   9%|▍    | 105M/1.17G [00:02<00:25, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  10%|▍    | 115M/1.17G [00:02<00:24, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  11%|▌    | 126M/1.17G [00:03<00:24, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  12%|▌    | 136M/1.17G [00:03<00:24, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  13%|▋    | 147M/1.17G [00:03<00:23, 42.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  13%|▋    | 157M/1.17G [00:03<00:23, 42.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  14%|▋    | 168M/1.17G [00:04<00:23, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  15%|▊    | 178M/1.17G [00:04<00:23, 42.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  16%|▊    | 189M/1.17G [00:04<00:23, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  17%|▊    | 199M/1.17G [00:04<00:22, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  18%|▉    | 210M/1.17G [00:05<00:22, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  19%|▉    | 220M/1.17G [00:05<00:22, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  20%|▉    | 231M/1.17G [00:05<00:22, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  21%|█    | 241M/1.17G [00:05<00:21, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  22%|█    | 252M/1.17G [00:06<00:21, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  22%|█    | 262M/1.17G [00:06<00:21, 42.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  23%|█▏   | 273M/1.17G [00:06<00:21, 42.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  24%|█▏   | 283M/1.17G [00:06<00:20, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  25%|█▎   | 294M/1.17G [00:07<00:20, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  26%|█▎   | 304M/1.17G [00:07<00:20, 42.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  27%|█▎   | 315M/1.17G [00:07<00:19, 42.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  28%|█▍   | 325M/1.17G [00:07<00:19, 42.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  29%|█▍   | 336M/1.17G [00:07<00:19, 42.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  30%|█▍   | 346M/1.17G [00:08<00:19, 42.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  31%|█▌   | 357M/1.17G [00:08<00:19, 42.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  31%|█▌   | 367M/1.17G [00:08<00:18, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  32%|█▌   | 377M/1.17G [00:08<00:18, 42.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  33%|█▋   | 388M/1.17G [00:09<00:18, 42.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  34%|█▋   | 398M/1.17G [00:09<00:18, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  35%|█▊   | 409M/1.17G [00:09<00:17, 42.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  36%|█▊   | 419M/1.17G [00:09<00:17, 42.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  37%|█▊   | 430M/1.17G [00:10<00:17, 43.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  38%|█▉   | 440M/1.17G [00:10<00:18, 39.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  39%|█▉   | 451M/1.17G [00:10<00:19, 37.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  39%|█▉   | 461M/1.17G [00:11<00:18, 39.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  40%|██   | 472M/1.17G [00:11<00:17, 40.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  41%|██   | 482M/1.17G [00:11<00:16, 40.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  42%|██   | 493M/1.17G [00:11<00:16, 41.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  43%|██▏  | 503M/1.17G [00:12<00:15, 41.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  44%|██▏  | 514M/1.17G [00:12<00:15, 42.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  45%|██▏  | 524M/1.17G [00:12<00:15, 42.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  46%|██▎  | 535M/1.17G [00:12<00:14, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  47%|██▎  | 545M/1.17G [00:13<00:14, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  48%|██▍  | 556M/1.17G [00:13<00:14, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  48%|██▍  | 566M/1.17G [00:13<00:14, 42.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  49%|██▍  | 577M/1.17G [00:13<00:13, 42.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  50%|██▌  | 587M/1.17G [00:14<00:13, 42.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  51%|██▌  | 598M/1.17G [00:14<00:13, 42.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  52%|██▌  | 608M/1.17G [00:14<00:13, 42.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  53%|██▋  | 619M/1.17G [00:14<00:12, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  54%|██▋  | 629M/1.17G [00:15<00:12, 42.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  55%|██▋  | 640M/1.17G [00:15<00:12, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  56%|██▊  | 650M/1.17G [00:15<00:12, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  57%|██▊  | 661M/1.17G [00:15<00:11, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  57%|██▊  | 671M/1.17G [00:15<00:11, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  58%|██▉  | 682M/1.17G [00:16<00:11, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  59%|██▉  | 692M/1.17G [00:16<00:11, 42.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  60%|███  | 703M/1.17G [00:16<00:11, 42.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  61%|███  | 713M/1.17G [00:16<00:10, 42.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  62%|███  | 724M/1.17G [00:17<00:12, 34.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  63%|███▏ | 734M/1.17G [00:17<00:10, 40.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  64%|███▏ | 744M/1.17G [00:17<00:10, 40.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  65%|███▏ | 755M/1.17G [00:18<00:09, 41.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  66%|███▎ | 765M/1.17G [00:18<00:09, 41.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  66%|███▎ | 776M/1.17G [00:18<00:09, 41.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  67%|███▎ | 786M/1.17G [00:18<00:09, 42.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  68%|███▍ | 797M/1.17G [00:19<00:08, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  69%|███▍ | 807M/1.17G [00:19<00:08, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  70%|███▌ | 818M/1.17G [00:19<00:08, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  71%|███▌ | 828M/1.17G [00:19<00:08, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  72%|███▌ | 839M/1.17G [00:20<00:07, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  73%|███▋ | 849M/1.17G [00:20<00:07, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  74%|███▋ | 860M/1.17G [00:20<00:07, 42.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  75%|███▋ | 870M/1.17G [00:20<00:07, 40.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  75%|███▊ | 881M/1.17G [00:21<00:07, 39.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  76%|███▊ | 891M/1.17G [00:21<00:07, 37.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  77%|███▊ | 902M/1.17G [00:21<00:06, 38.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  78%|███▉ | 912M/1.17G [00:21<00:06, 39.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  79%|███▉ | 923M/1.17G [00:22<00:06, 40.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  80%|███▉ | 933M/1.17G [00:22<00:05, 41.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  81%|████ | 944M/1.17G [00:22<00:05, 41.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  82%|████ | 954M/1.17G [00:22<00:05, 42.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  83%|████▏| 965M/1.17G [00:23<00:04, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  83%|████▏| 975M/1.17G [00:23<00:04, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  84%|████▏| 986M/1.17G [00:23<00:04, 42.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  85%|████▎| 996M/1.17G [00:23<00:04, 42.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  86%|███▍| 1.01G/1.17G [00:24<00:03, 42.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  87%|███▍| 1.02G/1.17G [00:24<00:03, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  88%|███▌| 1.03G/1.17G [00:24<00:03, 42.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  89%|███▌| 1.04G/1.17G [00:24<00:03, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  90%|███▌| 1.05G/1.17G [00:25<00:02, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  91%|███▋| 1.06G/1.17G [00:25<00:02, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  92%|███▋| 1.07G/1.17G [00:25<00:02, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  92%|███▋| 1.08G/1.17G [00:25<00:02, 42.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  93%|███▋| 1.09G/1.17G [00:26<00:01, 42.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  94%|███▊| 1.10G/1.17G [00:26<00:01, 42.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  95%|███▊| 1.11G/1.17G [00:26<00:01, 42.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  96%|███▊| 1.12G/1.17G [00:26<00:01, 42.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  97%|███▉| 1.13G/1.17G [00:27<00:00, 42.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  98%|███▉| 1.14G/1.17G [00:27<00:00, 42.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  99%|███▉| 1.15G/1.17G [00:27<00:00, 42.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|████| 1.17G/1.17G [00:27<00:00, 41.8MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [06:22<00:00, 95.61s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [06:22<00:00, 95.63s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [06:22<00:00, 95.64s/it]\n",
      "[INFO|modeling_utils.py:1670] 2025-01-17 00:20:04,460 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2025-01-17 00:20:04,467 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"pad_token_id\": 128255\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:02<00:00,  1.55it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:02<00:00,  1.51it/s]\n",
      "generation_config.json: 100%|███████████████████| 121/121 [00:00<00:00, 717kB/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.20it/s]\n",
      "[INFO|modeling_utils.py:4800] 2025-01-17 00:20:07,882 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2025-01-17 00:20:07,882 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at allganize/Llama-3-Alpha-Ko-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1051] 2025-01-17 00:20:07,972 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2025-01-17 00:20:07,972 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "[INFO|2025-01-17 00:20:07] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-01-17 00:20:07] llamafactory.model.model_utils.attention:157 >> Using vanilla attention implementation.\n",
      "[INFO|2025-01-17 00:20:07] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-01-17 00:20:07] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-01-17 00:20:07] llamafactory.model.model_utils.misc:157 >> Found linear modules: v_proj,k_proj,q_proj,down_proj,gate_proj,o_proj,up_proj\n",
      "[INFO|2025-01-17 00:20:08] llamafactory.model.loader:157 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "[INFO|trainer.py:698] 2025-01-17 00:20:08,417 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2313] 2025-01-17 00:20:08,750 >> ***** Running training *****\n",
      "[INFO|trainer.py:2314] 2025-01-17 00:20:08,750 >>   Num examples = 450\n",
      "[INFO|trainer.py:2315] 2025-01-17 00:20:08,750 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:2316] 2025-01-17 00:20:08,750 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2319] 2025-01-17 00:20:08,750 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:2320] 2025-01-17 00:20:08,750 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2321] 2025-01-17 00:20:08,750 >>   Total optimization steps = 740\n",
      "[INFO|trainer.py:2322] 2025-01-17 00:20:08,753 >>   Number of trainable parameters = 20,971,520\n",
      "{'loss': 0.0726, 'grad_norm': 0.25885695219039917, 'learning_rate': 1.3513513513513515e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0484, 'grad_norm': 0.5971971750259399, 'learning_rate': 2.702702702702703e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0132, 'grad_norm': 0.0956939309835434, 'learning_rate': 4.0540540540540545e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0095, 'grad_norm': 0.09579328447580338, 'learning_rate': 5.405405405405406e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0058, 'grad_norm': 0.09342814981937408, 'learning_rate': 6.756756756756757e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0048, 'grad_norm': 0.09390052407979965, 'learning_rate': 8.108108108108109e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0063, 'grad_norm': 0.049057409167289734, 'learning_rate': 9.45945945945946e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0058, 'grad_norm': 0.0614580400288105, 'learning_rate': 9.99799753559161e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0038, 'grad_norm': 0.05258730426430702, 'learning_rate': 9.985766061140232e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0034, 'grad_norm': 0.044968828558921814, 'learning_rate': 9.962442770775675e-05, 'epoch': 2.67}\n",
      " 14%|█████▌                                   | 100/740 [03:47<24:12,  2.27s/it][INFO|trainer.py:4117] 2025-01-17 00:23:56,777 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-01-17 00:23:56,777 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-01-17 00:23:56,777 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▏                                      | 2/17 [00:00<00:01, 10.58it/s]\u001b[A\n",
      " 24%|██████████▎                                 | 4/17 [00:00<00:01,  7.57it/s]\u001b[A\n",
      " 29%|████████████▉                               | 5/17 [00:00<00:01,  6.24it/s]\u001b[A\n",
      " 35%|███████████████▌                            | 6/17 [00:00<00:01,  5.77it/s]\u001b[A\n",
      " 41%|██████████████████                          | 7/17 [00:01<00:01,  5.86it/s]\u001b[A\n",
      " 47%|████████████████████▋                       | 8/17 [00:01<00:01,  5.70it/s]\u001b[A\n",
      " 53%|███████████████████████▎                    | 9/17 [00:01<00:01,  5.41it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 10/17 [00:01<00:01,  5.57it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 11/17 [00:01<00:01,  5.12it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 12/17 [00:02<00:00,  5.06it/s]\u001b[A\n",
      " 76%|████████████████████████████████▉          | 13/17 [00:02<00:00,  4.97it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 14/17 [00:02<00:00,  4.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 15/17 [00:02<00:00,  4.62it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 16/17 [00:02<00:00,  4.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.00712813064455986, 'eval_runtime': 3.3479, 'eval_samples_per_second': 14.935, 'eval_steps_per_second': 5.078, 'epoch': 2.67}\n",
      " 14%|█████▌                                   | 100/740 [03:51<24:12,  2.27s/it]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:03<00:00,  5.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-01-17 00:24:00,126 >> Saving model checkpoint to deidentification/checkpoint/checkpoint-100\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:24:00,344 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:24:00,345 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-17 00:24:00,708 >> tokenizer config file saved in deidentification/checkpoint/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-17 00:24:00,712 >> Special tokens file saved in deidentification/checkpoint/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 0.0038, 'grad_norm': 0.046642184257507324, 'learning_rate': 9.928079551738543e-05, 'epoch': 2.93}\n",
      "{'loss': 0.0028, 'grad_norm': 0.03628137335181236, 'learning_rate': 9.882752851759248e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0023, 'grad_norm': 0.052115440368652344, 'learning_rate': 9.826563508985017e-05, 'epoch': 3.47}\n",
      "{'loss': 0.0031, 'grad_norm': 0.06576284766197205, 'learning_rate': 9.759636527645633e-05, 'epoch': 3.73}\n",
      "{'loss': 0.0037, 'grad_norm': 0.1118457242846489, 'learning_rate': 9.682120799956962e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0022, 'grad_norm': 0.04285147041082382, 'learning_rate': 9.594188774880982e-05, 'epoch': 4.27}\n",
      "{'loss': 0.0017, 'grad_norm': 0.029985172674059868, 'learning_rate': 9.496036074479184e-05, 'epoch': 4.53}\n",
      "{'loss': 0.0018, 'grad_norm': 0.04824485257267952, 'learning_rate': 9.387881058712888e-05, 'epoch': 4.8}\n",
      "{'loss': 0.0018, 'grad_norm': 0.029144275933504105, 'learning_rate': 9.269964339658605e-05, 'epoch': 5.07}\n",
      "{'loss': 0.001, 'grad_norm': 0.035372912883758545, 'learning_rate': 9.142548246219212e-05, 'epoch': 5.33}\n",
      " 27%|███████████                              | 200/740 [07:39<20:46,  2.31s/it][INFO|trainer.py:4117] 2025-01-17 00:27:48,649 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-01-17 00:27:48,649 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-01-17 00:27:48,649 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▏                                      | 2/17 [00:00<00:01, 10.62it/s]\u001b[A\n",
      " 24%|██████████▎                                 | 4/17 [00:00<00:01,  7.56it/s]\u001b[A\n",
      " 29%|████████████▉                               | 5/17 [00:00<00:01,  6.24it/s]\u001b[A\n",
      " 35%|███████████████▌                            | 6/17 [00:00<00:01,  5.76it/s]\u001b[A\n",
      " 41%|██████████████████                          | 7/17 [00:01<00:01,  5.85it/s]\u001b[A\n",
      " 47%|████████████████████▋                       | 8/17 [00:01<00:01,  5.69it/s]\u001b[A\n",
      " 53%|███████████████████████▎                    | 9/17 [00:01<00:01,  5.40it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 10/17 [00:01<00:01,  5.56it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 11/17 [00:01<00:01,  5.12it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 12/17 [00:02<00:00,  5.05it/s]\u001b[A\n",
      " 76%|████████████████████████████████▉          | 13/17 [00:02<00:00,  4.96it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 14/17 [00:02<00:00,  4.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 15/17 [00:02<00:00,  4.61it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 16/17 [00:02<00:00,  4.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.00753615889698267, 'eval_runtime': 3.3504, 'eval_samples_per_second': 14.924, 'eval_steps_per_second': 5.074, 'epoch': 5.33}\n",
      " 27%|███████████                              | 200/740 [07:43<20:46,  2.31s/it]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:03<00:00,  5.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-01-17 00:27:51,998 >> Saving model checkpoint to deidentification/checkpoint/checkpoint-200\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:27:52,245 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:27:52,246 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-17 00:27:52,640 >> tokenizer config file saved in deidentification/checkpoint/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-17 00:27:52,643 >> Special tokens file saved in deidentification/checkpoint/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.001, 'grad_norm': 0.038345251232385635, 'learning_rate': 9.005916240521788e-05, 'epoch': 5.6}\n",
      "{'loss': 0.0012, 'grad_norm': 0.041838519275188446, 'learning_rate': 8.860372287300431e-05, 'epoch': 5.87}\n",
      "{'loss': 0.0008, 'grad_norm': 0.015419741161167622, 'learning_rate': 8.706240177667003e-05, 'epoch': 6.13}\n",
      "{'loss': 0.0008, 'grad_norm': 0.017304949462413788, 'learning_rate': 8.543862808774192e-05, 'epoch': 6.4}\n",
      "{'loss': 0.0008, 'grad_norm': 0.012796919792890549, 'learning_rate': 8.373601420973464e-05, 'epoch': 6.67}\n",
      "{'loss': 0.0006, 'grad_norm': 0.028370914980769157, 'learning_rate': 8.195834794164925e-05, 'epoch': 6.93}\n",
      "{'loss': 0.0009, 'grad_norm': 0.062328536063432693, 'learning_rate': 8.010958405127048e-05, 'epoch': 7.2}\n",
      "{'loss': 0.0007, 'grad_norm': 0.04660516977310181, 'learning_rate': 7.81938354770089e-05, 'epoch': 7.47}\n",
      "{'loss': 0.0006, 'grad_norm': 0.03611364960670471, 'learning_rate': 7.621536417786159e-05, 'epoch': 7.73}\n",
      "{'loss': 0.0008, 'grad_norm': 0.010478202253580093, 'learning_rate': 7.417857165184723e-05, 'epoch': 8.0}\n",
      " 41%|████████████████▌                        | 300/740 [11:32<16:44,  2.28s/it][INFO|trainer.py:4117] 2025-01-17 00:31:41,309 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-01-17 00:31:41,309 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-01-17 00:31:41,309 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▏                                      | 2/17 [00:00<00:01, 10.63it/s]\u001b[A\n",
      " 24%|██████████▎                                 | 4/17 [00:00<00:01,  7.57it/s]\u001b[A\n",
      " 29%|████████████▉                               | 5/17 [00:00<00:01,  6.24it/s]\u001b[A\n",
      " 35%|███████████████▌                            | 6/17 [00:00<00:01,  5.77it/s]\u001b[A\n",
      " 41%|██████████████████                          | 7/17 [00:01<00:01,  5.86it/s]\u001b[A\n",
      " 47%|████████████████████▋                       | 8/17 [00:01<00:01,  5.70it/s]\u001b[A\n",
      " 53%|███████████████████████▎                    | 9/17 [00:01<00:01,  5.41it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 10/17 [00:01<00:01,  5.57it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 11/17 [00:01<00:01,  5.13it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 12/17 [00:02<00:00,  5.07it/s]\u001b[A\n",
      " 76%|████████████████████████████████▉          | 13/17 [00:02<00:00,  4.97it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 14/17 [00:02<00:00,  4.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 15/17 [00:02<00:00,  4.62it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 16/17 [00:02<00:00,  4.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.008458864875137806, 'eval_runtime': 3.3415, 'eval_samples_per_second': 14.963, 'eval_steps_per_second': 5.087, 'epoch': 8.0}\n",
      " 41%|████████████████▌                        | 300/740 [11:35<16:44,  2.28s/it]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:03<00:00,  5.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-01-17 00:31:44,651 >> Saving model checkpoint to deidentification/checkpoint/checkpoint-300\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:31:44,859 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:31:44,860 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-17 00:31:45,234 >> tokenizer config file saved in deidentification/checkpoint/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-17 00:31:45,239 >> Special tokens file saved in deidentification/checkpoint/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.0003, 'grad_norm': 0.007162842433899641, 'learning_rate': 7.208798914400916e-05, 'epoch': 8.27}\n",
      "{'loss': 0.0003, 'grad_norm': 0.010480719618499279, 'learning_rate': 6.994826756577082e-05, 'epoch': 8.53}\n",
      "{'loss': 0.0003, 'grad_norm': 0.005648079793900251, 'learning_rate': 6.776416714806969e-05, 'epoch': 8.8}\n",
      "{'loss': 0.0005, 'grad_norm': 0.01425427757203579, 'learning_rate': 6.554054685128856e-05, 'epoch': 9.07}\n",
      "{'loss': 0.0002, 'grad_norm': 0.009120668284595013, 'learning_rate': 6.328235355554382e-05, 'epoch': 9.33}\n",
      "{'loss': 0.0006, 'grad_norm': 0.038582026958465576, 'learning_rate': 6.099461105537889e-05, 'epoch': 9.6}\n",
      "{'loss': 0.0007, 'grad_norm': 0.004808368626981974, 'learning_rate': 5.868240888334653e-05, 'epoch': 9.87}\n",
      "{'loss': 0.0004, 'grad_norm': 0.027475552633404732, 'learning_rate': 5.6350890987343944e-05, 'epoch': 10.13}\n",
      "{'loss': 0.0003, 'grad_norm': 0.07282858341932297, 'learning_rate': 5.4005244286890354e-05, 'epoch': 10.4}\n",
      "{'loss': 0.0002, 'grad_norm': 0.022907717153429985, 'learning_rate': 5.1650687133805674e-05, 'epoch': 10.67}\n",
      " 54%|██████████████████████▏                  | 400/740 [15:24<12:47,  2.26s/it][INFO|trainer.py:4117] 2025-01-17 00:35:33,163 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-01-17 00:35:33,163 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-01-17 00:35:33,163 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▏                                      | 2/17 [00:00<00:01, 10.65it/s]\u001b[A\n",
      " 24%|██████████▎                                 | 4/17 [00:00<00:01,  7.59it/s]\u001b[A\n",
      " 29%|████████████▉                               | 5/17 [00:00<00:01,  6.25it/s]\u001b[A\n",
      " 35%|███████████████▌                            | 6/17 [00:00<00:01,  5.78it/s]\u001b[A\n",
      " 41%|██████████████████                          | 7/17 [00:01<00:01,  5.85it/s]\u001b[A\n",
      " 47%|████████████████████▋                       | 8/17 [00:01<00:01,  5.69it/s]\u001b[A\n",
      " 53%|███████████████████████▎                    | 9/17 [00:01<00:01,  5.41it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 10/17 [00:01<00:01,  5.57it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 11/17 [00:01<00:01,  5.12it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 12/17 [00:02<00:00,  5.07it/s]\u001b[A\n",
      " 76%|████████████████████████████████▉          | 13/17 [00:02<00:00,  4.97it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 14/17 [00:02<00:00,  4.98it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 15/17 [00:02<00:00,  4.62it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 16/17 [00:02<00:00,  4.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.00952452328056097, 'eval_runtime': 3.3414, 'eval_samples_per_second': 14.964, 'eval_steps_per_second': 5.088, 'epoch': 10.67}\n",
      " 54%|██████████████████████▏                  | 400/740 [15:27<12:47,  2.26s/it]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:03<00:00,  5.05it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-01-17 00:35:36,506 >> Saving model checkpoint to deidentification/checkpoint/checkpoint-400\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:35:36,703 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:35:36,704 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-17 00:35:37,112 >> tokenizer config file saved in deidentification/checkpoint/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-17 00:35:37,117 >> Special tokens file saved in deidentification/checkpoint/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.0002, 'grad_norm': 0.0345800556242466, 'learning_rate': 4.929245770296191e-05, 'epoch': 10.93}\n",
      "{'loss': 0.0001, 'grad_norm': 0.004283989313989878, 'learning_rate': 4.69358023389342e-05, 'epoch': 11.2}\n",
      "{'loss': 0.0001, 'grad_norm': 0.006937502883374691, 'learning_rate': 4.458596388447691e-05, 'epoch': 11.47}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0014644402544945478, 'learning_rate': 4.224817001679011e-05, 'epoch': 11.73}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0021945617627352476, 'learning_rate': 3.992762161752474e-05, 'epoch': 12.0}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001521083526313305, 'learning_rate': 3.762948120239988e-05, 'epoch': 12.27}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0007603589911013842, 'learning_rate': 3.5358861436172485e-05, 'epoch': 12.53}\n",
      "{'loss': 0.0001, 'grad_norm': 0.016549021005630493, 'learning_rate': 3.312081375851038e-05, 'epoch': 12.8}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0011915910290554166, 'learning_rate': 3.0920317146072576e-05, 'epoch': 13.07}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0014059556415304542, 'learning_rate': 2.876226703579761e-05, 'epoch': 13.33}\n",
      " 68%|███████████████████████████▋             | 500/740 [19:17<09:09,  2.29s/it][INFO|trainer.py:4117] 2025-01-17 00:39:26,500 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-01-17 00:39:26,500 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-01-17 00:39:26,500 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▏                                      | 2/17 [00:00<00:01, 10.67it/s]\u001b[A\n",
      " 24%|██████████▎                                 | 4/17 [00:00<00:01,  7.59it/s]\u001b[A\n",
      " 29%|████████████▉                               | 5/17 [00:00<00:01,  6.24it/s]\u001b[A\n",
      " 35%|███████████████▌                            | 6/17 [00:00<00:01,  5.77it/s]\u001b[A\n",
      " 41%|██████████████████                          | 7/17 [00:01<00:01,  5.86it/s]\u001b[A\n",
      " 47%|████████████████████▋                       | 8/17 [00:01<00:01,  5.70it/s]\u001b[A\n",
      " 53%|███████████████████████▎                    | 9/17 [00:01<00:01,  5.40it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 10/17 [00:01<00:01,  5.57it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 11/17 [00:01<00:01,  5.12it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 12/17 [00:02<00:00,  5.07it/s]\u001b[A\n",
      " 76%|████████████████████████████████▉          | 13/17 [00:02<00:00,  4.97it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 14/17 [00:02<00:00,  4.98it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 15/17 [00:02<00:00,  4.62it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 16/17 [00:02<00:00,  4.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.011054523289203644, 'eval_runtime': 3.3429, 'eval_samples_per_second': 14.957, 'eval_steps_per_second': 5.085, 'epoch': 13.33}\n",
      " 68%|███████████████████████████▋             | 500/740 [19:21<09:09,  2.29s/it]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:03<00:00,  5.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-01-17 00:39:29,843 >> Saving model checkpoint to deidentification/checkpoint/checkpoint-500\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:39:30,039 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:39:30,040 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-17 00:39:30,368 >> tokenizer config file saved in deidentification/checkpoint/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-17 00:39:30,372 >> Special tokens file saved in deidentification/checkpoint/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.0001, 'grad_norm': 0.0031265346333384514, 'learning_rate': 2.66514644340426e-05, 'epoch': 13.6}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0013263579457998276, 'learning_rate': 2.459260523580154e-05, 'epoch': 13.87}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001636509201489389, 'learning_rate': 2.2590269777764515e-05, 'epoch': 14.13}\n",
      "{'loss': 0.0001, 'grad_norm': 0.000489116704557091, 'learning_rate': 2.0648912648459074e-05, 'epoch': 14.4}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0009016190306283534, 'learning_rate': 1.8772852778143063e-05, 'epoch': 14.67}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0019193071639165282, 'learning_rate': 1.6966263830495936e-05, 'epoch': 14.93}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00042964218300767243, 'learning_rate': 1.5233164917484116e-05, 'epoch': 15.2}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0008116548415273428, 'learning_rate': 1.3577411658056966e-05, 'epoch': 15.47}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0005914101493544877, 'learning_rate': 1.2002687600565137e-05, 'epoch': 15.73}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0016582077369093895, 'learning_rate': 1.0512496027983714e-05, 'epoch': 16.0}\n",
      " 81%|█████████████████████████████████▏       | 600/740 [23:10<05:23,  2.31s/it][INFO|trainer.py:4117] 2025-01-17 00:43:18,840 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-01-17 00:43:18,841 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-01-17 00:43:18,841 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▏                                      | 2/17 [00:00<00:01, 10.61it/s]\u001b[A\n",
      " 24%|██████████▎                                 | 4/17 [00:00<00:01,  7.56it/s]\u001b[A\n",
      " 29%|████████████▉                               | 5/17 [00:00<00:01,  6.23it/s]\u001b[A\n",
      " 35%|███████████████▌                            | 6/17 [00:00<00:01,  5.76it/s]\u001b[A\n",
      " 41%|██████████████████                          | 7/17 [00:01<00:01,  5.84it/s]\u001b[A\n",
      " 47%|████████████████████▋                       | 8/17 [00:01<00:01,  5.69it/s]\u001b[A\n",
      " 53%|███████████████████████▎                    | 9/17 [00:01<00:01,  5.39it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 10/17 [00:01<00:01,  5.55it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 11/17 [00:01<00:01,  5.11it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 12/17 [00:02<00:00,  5.06it/s]\u001b[A\n",
      " 76%|████████████████████████████████▉          | 13/17 [00:02<00:00,  4.96it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 14/17 [00:02<00:00,  4.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 15/17 [00:02<00:00,  4.61it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 16/17 [00:02<00:00,  4.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.011525987647473812, 'eval_runtime': 3.3497, 'eval_samples_per_second': 14.927, 'eval_steps_per_second': 5.075, 'epoch': 16.0}\n",
      " 81%|█████████████████████████████████▏       | 600/740 [23:13<05:23,  2.31s/it]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:03<00:00,  5.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-01-17 00:43:22,191 >> Saving model checkpoint to deidentification/checkpoint/checkpoint-600\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:43:22,397 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:43:22,397 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-17 00:43:22,720 >> tokenizer config file saved in deidentification/checkpoint/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-17 00:43:22,725 >> Special tokens file saved in deidentification/checkpoint/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006409685011021793, 'learning_rate': 9.110152164171126e-06, 'epoch': 16.27}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001753088552504778, 'learning_rate': 7.798775798502483e-06, 'epoch': 16.53}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00038878549821674824, 'learning_rate': 6.581284345285372e-06, 'epoch': 16.8}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00035791590926237404, 'learning_rate': 5.460386353398583e-06, 'epoch': 17.07}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00034282065462321043, 'learning_rate': 4.43857548059321e-06, 'epoch': 17.33}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0012524164048954844, 'learning_rate': 3.51812494586114e-06, 'epoch': 17.6}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0007450383855029941, 'learning_rate': 2.701082472212879e-06, 'epoch': 17.87}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006294071790762246, 'learning_rate': 1.9892657311155248e-06, 'epoch': 18.13}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0009972461266443133, 'learning_rate': 1.3842582987255493e-06, 'epoch': 18.4}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0005560723948292434, 'learning_rate': 8.874061329125938e-07, 'epoch': 18.67}\n",
      " 95%|██████████████████████████████████████▊  | 700/740 [27:03<01:34,  2.36s/it][INFO|trainer.py:4117] 2025-01-17 00:47:12,112 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-01-17 00:47:12,112 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-01-17 00:47:12,112 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▏                                      | 2/17 [00:00<00:01, 10.66it/s]\u001b[A\n",
      " 24%|██████████▎                                 | 4/17 [00:00<00:01,  7.58it/s]\u001b[A\n",
      " 29%|████████████▉                               | 5/17 [00:00<00:01,  6.23it/s]\u001b[A\n",
      " 35%|███████████████▌                            | 6/17 [00:00<00:01,  5.76it/s]\u001b[A\n",
      " 41%|██████████████████                          | 7/17 [00:01<00:01,  5.85it/s]\u001b[A\n",
      " 47%|████████████████████▋                       | 8/17 [00:01<00:01,  5.69it/s]\u001b[A\n",
      " 53%|███████████████████████▎                    | 9/17 [00:01<00:01,  5.40it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 10/17 [00:01<00:01,  5.56it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 11/17 [00:01<00:01,  5.12it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 12/17 [00:02<00:00,  5.06it/s]\u001b[A\n",
      " 76%|████████████████████████████████▉          | 13/17 [00:02<00:00,  4.97it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 14/17 [00:02<00:00,  4.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 15/17 [00:02<00:00,  4.61it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 16/17 [00:02<00:00,  4.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.011689691804349422, 'eval_runtime': 3.3466, 'eval_samples_per_second': 14.94, 'eval_steps_per_second': 5.08, 'epoch': 18.67}\n",
      " 95%|██████████████████████████████████████▊  | 700/740 [27:06<01:34,  2.36s/it]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:03<00:00,  5.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2025-01-17 00:47:15,459 >> Saving model checkpoint to deidentification/checkpoint/checkpoint-700\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:47:15,659 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:47:15,660 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-17 00:47:15,993 >> tokenizer config file saved in deidentification/checkpoint/checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-17 00:47:15,996 >> Special tokens file saved in deidentification/checkpoint/checkpoint-700/special_tokens_map.json\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004111549351364374, 'learning_rate': 4.998145789118114e-07, 'epoch': 18.93}\n",
      "{'loss': 0.0001, 'grad_norm': 0.000977695221081376, 'learning_rate': 2.223459102662695e-07, 'epoch': 19.2}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004809124511666596, 'learning_rate': 5.561741053010661e-08, 'epoch': 19.47}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0005175435217097402, 'learning_rate': 0.0, 'epoch': 19.73}\n",
      "100%|█████████████████████████████████████████| 740/740 [28:38<00:00,  2.28s/it][INFO|trainer.py:3801] 2025-01-17 00:48:47,465 >> Saving model checkpoint to deidentification/checkpoint/checkpoint-740\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:48:47,673 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:48:47,674 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-17 00:48:47,952 >> tokenizer config file saved in deidentification/checkpoint/checkpoint-740/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-17 00:48:47,956 >> Special tokens file saved in deidentification/checkpoint/checkpoint-740/special_tokens_map.json\n",
      "[INFO|trainer.py:2584] 2025-01-17 00:48:48,722 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1719.9688, 'train_samples_per_second': 5.233, 'train_steps_per_second': 0.43, 'train_loss': 0.002868071574689443, 'epoch': 19.73}\n",
      "100%|█████████████████████████████████████████| 740/740 [28:39<00:00,  2.32s/it]\n",
      "[INFO|trainer.py:3801] 2025-01-17 00:48:48,729 >> Saving model checkpoint to deidentification/checkpoint\n",
      "[INFO|configuration_utils.py:679] 2025-01-17 00:48:48,994 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-17 00:48:48,995 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-17 00:48:49,279 >> tokenizer config file saved in deidentification/checkpoint/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-17 00:48:49,284 >> Special tokens file saved in deidentification/checkpoint/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     19.7333\n",
      "  total_flos               = 406840618GF\n",
      "  train_loss               =      0.0029\n",
      "  train_runtime            =  0:28:39.96\n",
      "  train_samples_per_second =       5.233\n",
      "  train_steps_per_second   =        0.43\n",
      "Figure saved at: deidentification/checkpoint/training_loss.png\n",
      "Figure saved at: deidentification/checkpoint/training_eval_loss.png\n",
      "[WARNING|2025-01-17 00:48:49] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
      "[INFO|trainer.py:4117] 2025-01-17 00:48:49,735 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2025-01-17 00:48:49,735 >>   Num examples = 50\n",
      "[INFO|trainer.py:4122] 2025-01-17 00:48:49,735 >>   Batch size = 1\n",
      "100%|███████████████████████████████████████████| 17/17 [00:03<00:00,  5.45it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =    19.7333\n",
      "  eval_loss               =     0.0117\n",
      "  eval_runtime            = 0:00:03.30\n",
      "  eval_samples_per_second =     15.127\n",
      "  eval_steps_per_second   =      5.143\n",
      "[INFO|modelcard.py:449] 2025-01-17 00:48:53,056 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train examples/train_lora/llama3_lora_deidentify.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b898b8c-25e3-46c3-a528-6423300c1b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: allganize/Llama-3-Alpha-Ko-8B-Instruct\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:09<00:00,  2.44s/it]\n",
      "Loading PEFT: ./deidentification/checkpoint/checkpoint-740\n",
      "Running merge_and_unload\n",
      "[2025-01-17 00:57:35,152] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Model saved to ./deidentification/model\n"
     ]
    }
   ],
   "source": [
    "!python merge.py \\\n",
    "    --base_model_name_or_path allganize/Llama-3-Alpha-Ko-8B-Instruct \\\n",
    "    --peft_model_path ./deidentification/checkpoint/checkpoint-740 \\\n",
    "    --output_dir ./deidentification/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82999cbe-b59f-4b7f-a0a5-c0ff66a0df7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/LLaMA-Factory'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa07214-d5ff-4445-8279-9c4fc7c4535a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8919c-7d47-4f96-b4fe-fa85efd8d1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
