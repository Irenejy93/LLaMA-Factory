{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507ac07a-e6ff-4d2e-8752-6c7fda408f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Irenejy93/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263806ab-d167-4777-ab6a-2ebee59f3655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/LLaMA-Factory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02eb9612-e718-4feb-abf4-a357eb8ec517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]    \n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3798 kB]\n",
      "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2692 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [57.9 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1237 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3962 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3003 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1535 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [65.7 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:21 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\n",
      "Get:22 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1378 kB]\n",
      "Fetched 38.2 MB in 2s (18.4 MB/s)                                              \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libgpm2 libsodium23 vim-common vim-runtime xxd\n",
      "Suggested packages:\n",
      "  gpm ctags vim-doc vim-scripts\n",
      "The following NEW packages will be installed:\n",
      "  libgpm2 libsodium23 vim vim-common vim-runtime xxd\n",
      "0 upgraded, 6 newly installed, 0 to remove and 129 not upgraded.\n",
      "Need to get 8878 kB of archives.\n",
      "After this operation, 38.8 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xxd amd64 2:8.2.3995-1ubuntu2.23 [51.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-common all 2:8.2.3995-1ubuntu2.23 [81.5 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgpm2 amd64 1.20.7-10build1 [15.3 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsodium23 amd64 1.0.18-1build2 [164 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-runtime all 2:8.2.3995-1ubuntu2.23 [6833 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim amd64 2:8.2.3995-1ubuntu2.23 [1732 kB]\n",
      "Fetched 8878 kB in 1s (15.9 MB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package xxd.\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../0-xxd_2%3a8.2.3995-1ubuntu2.23_amd64.deb ...\n",
      "Unpacking xxd (2:8.2.3995-1ubuntu2.23) ...\n",
      "Selecting previously unselected package vim-common.\n",
      "Preparing to unpack .../1-vim-common_2%3a8.2.3995-1ubuntu2.23_all.deb ...\n",
      "Unpacking vim-common (2:8.2.3995-1ubuntu2.23) ...\n",
      "Selecting previously unselected package libgpm2:amd64.\n",
      "Preparing to unpack .../2-libgpm2_1.20.7-10build1_amd64.deb ...\n",
      "Unpacking libgpm2:amd64 (1.20.7-10build1) ...\n",
      "Selecting previously unselected package libsodium23:amd64.\n",
      "Preparing to unpack .../3-libsodium23_1.0.18-1build2_amd64.deb ...\n",
      "Unpacking libsodium23:amd64 (1.0.18-1build2) ...\n",
      "Selecting previously unselected package vim-runtime.\n",
      "Preparing to unpack .../4-vim-runtime_2%3a8.2.3995-1ubuntu2.23_all.deb ...\n",
      "Adding 'diversion of /usr/share/vim/vim82/doc/help.txt to /usr/share/vim/vim82/doc/help.txt.vim-tiny by vim-runtime'\n",
      "Adding 'diversion of /usr/share/vim/vim82/doc/tags to /usr/share/vim/vim82/doc/tags.vim-tiny by vim-runtime'\n",
      "Unpacking vim-runtime (2:8.2.3995-1ubuntu2.23) ...\n",
      "Selecting previously unselected package vim.\n",
      "Preparing to unpack .../5-vim_2%3a8.2.3995-1ubuntu2.23_amd64.deb ...\n",
      "Unpacking vim (2:8.2.3995-1ubuntu2.23) ...\n",
      "Setting up libsodium23:amd64 (1.0.18-1build2) ...\n",
      "Setting up libgpm2:amd64 (1.20.7-10build1) ...\n",
      "Setting up xxd (2:8.2.3995-1ubuntu2.23) ...\n",
      "Setting up vim-common (2:8.2.3995-1ubuntu2.23) ...\n",
      "Setting up vim-runtime (2:8.2.3995-1ubuntu2.23) ...\n",
      "Setting up vim (2:8.2.3995-1ubuntu2.23) ...\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/vi.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/vi.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/vi.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/vi.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/vi.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/vi.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/vi.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/vi.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/view.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/view.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/view.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/view.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/view.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/view.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/view.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/view.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/ex.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/ex.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/ex.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/ex.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/ex.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/ex.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/ex.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/ex.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/editor (editor) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/editor.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/editor.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/editor.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/editor.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/editor.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/editor.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/editor.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/editor.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install vim -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e7d989-a3d0-4330-984d-756f58bf909e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring transformers: markers 'python_version < \"3.10\"' don't match your environment\n",
      "Collecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2 (from -r requirements.txt (line 2))\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets<=3.2.0,>=2.16.0 (from -r requirements.txt (line 3))\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate<=1.2.1,>=0.34.0 (from -r requirements.txt (line 4))\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<=0.12.0,>=0.11.1 (from -r requirements.txt (line 5))\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 6))\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<=0.21.0,>=0.19.0 (from -r requirements.txt (line 7))\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting gradio<=5.18.0,>=4.38.0 (from -r requirements.txt (line 8))\n",
      "  Downloading gradio-5.18.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pandas>=2.0.0 (from -r requirements.txt (line 9))\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from -r requirements.txt (line 10))\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting einops (from -r requirements.txt (line 11))\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 12))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 13))\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting protobuf (from -r requirements.txt (line 14))\n",
      "  Downloading protobuf-6.30.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting uvicorn (from -r requirements.txt (line 15))\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic (from -r requirements.txt (line 16))\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting fastapi (from -r requirements.txt (line 17))\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting sse-starlette (from -r requirements.txt (line 18))\n",
      "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting matplotlib>=3.7.0 (from -r requirements.txt (line 19))\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting fire (from -r requirements.txt (line 20))\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (1.24.1)\n",
      "Collecting av (from -r requirements.txt (line 24))\n",
      "  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting librosa (from -r requirements.txt (line 25))\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting tyro<0.9.0 (from -r requirements.txt (line 26))\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2))\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3)) (2023.4.0)\n",
      "Collecting aiohttp (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.2.1,>=0.34.0->-r requirements.txt (line 4)) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.2.1,>=0.34.0->-r requirements.txt (line 4)) (2.1.0+cu118)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8)) (4.0.0)\n",
      "Collecting ffmpy (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.7.2 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8)) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8)) (2.1.2)\n",
      "Collecting orjson~=3.0 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8)) (9.3.0)\n",
      "Collecting pydub (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8)) (4.4.0)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.7.2->gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 9)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting click>=7.0 (from uvicorn->-r requirements.txt (line 15))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 15))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->-r requirements.txt (line 16))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic->-r requirements.txt (line 16))\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions~=4.0 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->-r requirements.txt (line 19))\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.7.0->-r requirements.txt (line 19))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->-r requirements.txt (line 19))\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->-r requirements.txt (line 19))\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (2.4.7)\n",
      "Collecting termcolor (from fire->-r requirements.txt (line 20))\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa->-r requirements.txt (line 25))\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa->-r requirements.txt (line 25))\n",
      "  Downloading numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa->-r requirements.txt (line 25))\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting joblib>=1.0 (from librosa->-r requirements.txt (line 25))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 25)) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa->-r requirements.txt (line 25))\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa->-r requirements.txt (line 25))\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa->-r requirements.txt (line 25))\n",
      "  Downloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting lazy_loader>=0.1 (from librosa->-r requirements.txt (line 25))\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa->-r requirements.txt (line 25))\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->-r requirements.txt (line 26))\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro<0.9.0->-r requirements.txt (line 26))\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->-r requirements.txt (line 26))\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8)) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8)) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3)) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8)) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0 (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa->-r requirements.txt (line 25))\n",
      "  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->-r requirements.txt (line 25)) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (1.26.13)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (2.16.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 25))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 25)) (1.16.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->-r requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->-r requirements.txt (line 4)) (3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->-r requirements.txt (line 4)) (2.1.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio<=5.18.0,>=4.38.0->-r requirements.txt (line 8))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 25)) (2.21)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio-5.18.0-py3-none-any.whl (62.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m686.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m456.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.30.1-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m505.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m782.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
      "Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m848.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m797.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m854.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m592.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m676.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m840.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m858.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m568.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m627.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m863.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m910.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m689.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m677.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m461.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m751.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m675.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m641.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m428.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m649.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=e070b06b093ef9d3d8635aaecc939510ebe7356d93f14b3d819b0838f91a15f2\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
      "Successfully built fire\n",
      "Installing collected packages: sentencepiece, pytz, pydub, xxhash, websockets, tzdata, typing-extensions, tqdm, tomlkit, threadpoolctl, termcolor, soxr, shtab, shellingham, semantic-version, scipy, safetensors, ruff, requests, regex, python-multipart, pyarrow, protobuf, propcache, orjson, msgpack, mdurl, llvmlite, lazy_loader, kiwisolver, joblib, h11, fsspec, frozenlist, fonttools, ffmpy, einops, docstring-parser, dill, cycler, contourpy, click, av, audioread, async-timeout, annotated-types, aiohappyeyeballs, aiofiles, uvicorn, tiktoken, soundfile, scikit-learn, pydantic-core, pooch, pandas, numba, multiprocess, multidict, matplotlib, markdown-it-py, huggingface-hub, httpcore, fire, anyio, aiosignal, yarl, tokenizers, starlette, rich, pydantic, librosa, httpx, accelerate, tyro, typer, transformers, sse-starlette, safehttpx, gradio-client, fastapi, aiohttp, peft, gradio, datasets, trl\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "Successfully installed accelerate-1.2.1 aiofiles-23.2.1 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.9.0 async-timeout-5.0.1 audioread-3.0.1 av-14.2.0 click-8.1.8 contourpy-1.3.1 cycler-0.12.1 datasets-3.2.0 dill-0.3.8 docstring-parser-0.16 einops-0.8.1 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 fonttools-4.56.0 frozenlist-1.5.0 fsspec-2024.9.0 gradio-5.18.0 gradio-client-1.7.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.29.3 joblib-1.4.2 kiwisolver-1.4.8 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.44.0 markdown-it-py-3.0.0 matplotlib-3.10.1 mdurl-0.1.2 msgpack-1.1.0 multidict-6.2.0 multiprocess-0.70.16 numba-0.61.0 orjson-3.10.15 pandas-2.2.3 peft-0.12.0 pooch-1.8.2 propcache-0.3.0 protobuf-6.30.1 pyarrow-19.0.1 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 pytz-2025.1 regex-2024.11.6 requests-2.32.3 rich-13.9.4 ruff-0.11.2 safehttpx-0.1.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 semantic-version-2.10.0 sentencepiece-0.2.0 shellingham-1.5.4 shtab-1.7.1 soundfile-0.13.1 soxr-0.5.0.post1 sse-starlette-2.2.1 starlette-0.46.1 termcolor-2.5.0 threadpoolctl-3.6.0 tiktoken-0.9.0 tokenizers-0.21.0 tomlkit-0.13.2 tqdm-4.67.1 transformers-4.49.0 trl-0.9.6 typer-0.15.2 typing-extensions-4.12.2 tyro-0.8.14 tzdata-2025.2 uvicorn-0.34.0 websockets-15.0.1 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting deepspeed==0.14.0\n",
      "  Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m659.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson (from deepspeed==0.14.0)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting ninja (from deepspeed==0.14.0)\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed==0.14.0)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (2.10.6)\n",
      "Collecting pynvml (from deepspeed==0.14.0)\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0) (4.12.2)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->deepspeed==0.14.0)\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (2024.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed==0.14.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.14.0) (1.3.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400398 sha256=df9d7b10a32c2a0208f0396b2b3413390b97b3f63c0f325ce5c45d41ee49c57f\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, pynvml, ninja, deepspeed\n",
      "Successfully installed deepspeed-0.14.0 hjson-3.1.0 ninja-1.11.1.4 nvidia-ml-py-12.570.86 py-cpuinfo-9.0.0 pynvml-12.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Obtaining file:///workspace/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (4.49.0)\n",
      "Requirement already satisfied: datasets<=3.2.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Requirement already satisfied: accelerate<=1.2.1,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
      "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (0.9.6)\n",
      "Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
      "Requirement already satisfied: gradio<=5.18.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (5.18.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.15.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (6.30.1)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.34.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.10.6)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.12)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (3.10.1)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (14.2.0)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
      "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (0.8.14)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m590.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-chinese\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.2.1,>=0.34.0) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.2.1,>=0.34.0) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.2.1,>=0.34.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.2.0,>=2.16.0) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0) (3.11.14)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (4.9.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (1.7.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (2.1.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (3.10.15)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (9.3.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (0.11.2)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (0.46.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0) (4.12.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.2->gradio<=5.18.0,>=4.38.0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1) (3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2) (2024.11.6)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0) (1.7.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.5.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge-chinese) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (1.18.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0) (1.26.13)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0) (2.16.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.18.0,>=4.38.0) (1.5.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0) (0.1.2)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m567.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba, llamafactory\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=d80eaba9c399362e7625e391dff76fadfcd6eee3f733ac2ff498856fa6139ae5\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/69/31/d56d90b22a1777b0b231e234b00302a55be255930f8bd92dcd\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=25826 sha256=af3a6c8822d9da826eed717207746b0ea4c4680586412e9df2521cc29e6b7a11\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0uek6jvh/wheels/6b/fb/2e/068b37b77399a386b5777cf8b247cb07a69197b4baef3732a2\n",
      "Successfully built jieba llamafactory\n",
      "Installing collected packages: jieba, rouge-chinese, nltk, llamafactory\n",
      "Successfully installed jieba-0.42.1 llamafactory-0.9.2.dev0 nltk-3.9.1 rouge-chinese-1.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install deepspeed==0.14.0\n",
    "!pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61602ad9-132d-4882-8154-ae0ea725f736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `llamaf` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llamaf`\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74127708-56d5-4a87-91d0-6a7ac282f0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf675c9-81d1-4c85-a918-837b35b0f645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-20 00:37:45,961] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO|2025-03-20 00:37:48] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:29527\n",
      "[2025-03-20 00:37:50,122] torch.distributed.run: [WARNING] \n",
      "[2025-03-20 00:37:50,122] torch.distributed.run: [WARNING] *****************************************\n",
      "[2025-03-20 00:37:50,122] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2025-03-20 00:37:50,122] torch.distributed.run: [WARNING] *****************************************\n",
      "[2025-03-20 00:37:55,524] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-03-20 00:37:55,664] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO|2025-03-20 00:37:56] llamafactory.hparams.parser:384 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[WARNING|2025-03-20 00:37:56] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-03-20 00:37:56] llamafactory.hparams.parser:384 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "config.json: 100%|█████████████████████████████| 772/772 [00:00<00:00, 4.43MB/s]\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 00:37:56,765 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 00:37:56,766 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████| 51.0k/51.0k [00:00<00:00, 2.60MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 36.4MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 350/350 [00:00<00:00, 2.92MB/s]\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:37:59,078 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:37:59,078 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:37:59,078 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:37:59,078 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:37:59,078 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:37:59,078 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-20 00:37:59,524 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 00:38:00,297 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 00:38:00,299 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:38:00,377 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:38:00,377 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:38:00,378 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:38:00,378 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:38:00,378 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-03-20 00:38:00,378 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2313] 2025-03-20 00:38:00,789 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-03-20 00:38:00] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-03-20 00:38:00] llamafactory.data.loader:157 >> Loading dataset irene93/deidentification-chat-ko...\n",
      "README.md: 100%|███████████████████████████████| 454/454 [00:00<00:00, 7.56MB/s]\n",
      "train-00000-of-00001.parquet: 100%|████████| 12.1M/12.1M [00:00<00:00, 48.7MB/s]\n",
      "test-00000-of-00001.parquet: 100%|█████████| 1.36M/1.36M [00:00<00:00, 19.4MB/s]\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 100%|█████| 8460/8460 [00:00<00:00, 81779.16 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 100%|████████| 940/940 [00:00<00:00, 93814.44 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1538.5\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 500/500 [00:03<00:00, 127.66\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 65895, 83628, 34804, 111097, 105922, 18918, 103185, 54596, 108, 119087, 72115, 103850, 229, 80052, 382, 567, 67890, 30426, 126840, 48826, 16, 13, 55430, 125441, 62060, 57390, 57575, 117559, 64254, 18359, 510, 94283, 16, 1145, 510, 94283, 17, 60, 78102, 43139, 126912, 106248, 27796, 19954, 106725, 62060, 50643, 101360, 11, 121731, 24486, 87134, 13094, 64857, 98934, 113191, 50152, 105718, 62060, 60798, 32179, 18918, 41820, 61938, 627, 17, 13, 101347, 105316, 102657, 11, 23955, 85767, 33177, 11, 56773, 44690, 1174, 95303, 113002, 73148, 49085, 127141, 510, 74471, 16, 1145, 510, 74471, 17, 60, 78102, 11, 510, 37004, 16, 15304, 37004, 17, 60, 78102, 11, 510, 62105, 16, 15304, 62105, 17, 60, 102278, 1174, 510, 74483, 16, 1145, 510, 74483, 17, 60, 78102, 38164, 120, 17835, 62060, 60798, 101360, 121731, 24486, 61139, 20565, 64857, 98934, 107205, 50152, 102772, 105718, 62060, 60798, 32179, 18918, 41820, 61938, 627, 18, 13, 67945, 60798, 32179, 18918, 114839, 48936, 106745, 107285, 103843, 29102, 55216, 48424, 61415, 11, 74618, 55055, 77437, 75908, 118156, 108533, 22035, 104508, 35495, 101971, 52688, 43139, 23955, 108503, 108533, 119978, 720, 19, 13, 82001, 111850, 117035, 34804, 62060, 57390, 110078, 19954, 105701, 110218, 84656, 101106, 65219, 58901, 115839, 61938, 13, 720, 65895, 83628, 13094, 111097, 105922, 18918, 103185, 71023, 62060, 57390, 96318, 101577, 80052, 13, 720, 25, 43449, 25, 128009, 128006, 882, 128007, 271, 35495, 108583, 25, 96270, 124409, 11, 119929, 19954, 103236, 30446, 105519, 89359, 27796, 18918, 106562, 97, 103170, 105718, 107744, 88708, 13094, 103405, 85721, 105519, 89359, 53400, 72208, 79474, 112737, 13, 74959, 110979, 61816, 55430, 101272, 29833, 36439, 114067, 1980, 126862, 56154, 25, 96270, 124409, 11, 116534, 102424, 13, 102678, 105204, 20565, 101703, 81673, 124365, 21121, 126308, 16969, 116534, 119673, 111097, 105922, 20565, 108289, 61938, 13, 102132, 79053, 18359, 116023, 113161, 103373, 105807, 1980, 35495, 108583, 25, 103315, 11, 63171, 87134, 34804, 102155, 101607, 24140, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 102155, 101607, 24140, 116534, 102424, 11, 116534, 119673, 126100, 73148, 81673, 56773, 44690, 49085, 123660, 34983, 113161, 33390, 117078, 16582, 115284, 382, 35495, 108583, 25, 126100, 73148, 16969, 220, 7755, 12, 4513, 19, 12, 19282, 23, 109816, 11, 56773, 44690, 16969, 106010, 30426, 124091, 89359, 107573, 125519, 103272, 17835, 220, 4513, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 116534, 102424, 95303, 113002, 73148, 49085, 108289, 61938, 13, 23955, 115155, 34804, 121916, 116281, 102893, 82158, 113191, 123263, 382, 35495, 108583, 25, 103315, 11, 95303, 113002, 73148, 16969, 220, 4513, 12, 10961, 22, 12, 21381, 16, 80052, 382, 126862, 56154, 25, 74959, 34983, 55430, 110311, 27796, 117078, 61938, 13, 116534, 102424, 127737, 123660, 124933, 107744, 88708, 18359, 74959, 34983, 101948, 99901, 11, 105718, 107744, 88708, 13094, 103405, 85721, 105519, 89359, 53400, 107387, 74959, 116304, 13, 102678, 105204, 113866, 57575, 116786, 30426, 104613, 106138, 82158, 34983, 124365, 115284, 382, 35495, 108583, 25, 116334, 35243, 45780, 52375, 13094, 108231, 13, 122545, 168, 107, 97, 104613, 106138, 13094, 107123, 64356, 113191, 117677, 1980, 126862, 56154, 25, 64432, 102233, 104613, 106138, 82158, 102772, 101603, 101096, 33177, 111902, 43139, 220, 18, 93, 20, 33177, 111297, 101228, 36811, 114409, 13, 106792, 95415, 101015, 113002, 18918, 74959, 34983, 56773, 51402, 382, 35495, 108583, 25, 103315, 11, 102066, 115284, 13, 119083, 30426, 105642, 107651, 16969, 47782, 114067, 1980, 126862, 56154, 25, 103315, 11, 117717, 103521, 34804, 23955, 17835, 115954, 82158, 65219, 13879, 90463, 13, 105642, 108515, 126840, 13094, 36439, 34609, 30426, 33390, 122545, 82776, 22035, 127808, 56773, 119978, 382, 35495, 108583, 25, 117078, 61938, 382, 126862, 56154, 25, 103315, 11, 117078, 61938, 13, 111937, 123106, 98243, 51402, 13, 128009, 128006, 78191, 128007, 35495, 108583, 25, 96270, 124409, 11, 119929, 19954, 103236, 30446, 105519, 89359, 27796, 18918, 106562, 97, 103170, 105718, 107744, 88708, 13094, 103405, 85721, 105519, 89359, 53400, 72208, 79474, 112737, 13, 74959, 110979, 61816, 55430, 101272, 29833, 36439, 114067, 1980, 126862, 56154, 25, 96270, 124409, 11, 116534, 102424, 13, 102678, 105204, 20565, 101703, 81673, 124365, 21121, 126308, 16969, 116534, 119673, 111097, 105922, 20565, 108289, 61938, 13, 102132, 79053, 18359, 116023, 113161, 103373, 105807, 1980, 35495, 108583, 25, 103315, 11, 63171, 87134, 34804, 510, 94283, 16, 60, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 510, 94283, 16, 60, 116534, 102424, 11, 116534, 119673, 126100, 73148, 81673, 56773, 44690, 49085, 123660, 34983, 113161, 33390, 117078, 16582, 115284, 382, 35495, 108583, 25, 126100, 73148, 16969, 510, 74471, 16, 60, 109816, 11, 56773, 44690, 16969, 510, 62105, 16, 60, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 116534, 102424, 95303, 113002, 73148, 49085, 108289, 61938, 13, 23955, 115155, 34804, 121916, 116281, 102893, 82158, 113191, 123263, 382, 35495, 108583, 25, 103315, 11, 95303, 113002, 73148, 16969, 510, 74483, 16, 60, 80052, 382, 126862, 56154, 25, 74959, 34983, 55430, 110311, 27796, 117078, 61938, 13, 116534, 102424, 127737, 123660, 124933, 107744, 88708, 18359, 74959, 34983, 101948, 99901, 11, 105718, 107744, 88708, 13094, 103405, 85721, 105519, 89359, 53400, 107387, 74959, 116304, 13, 102678, 105204, 113866, 57575, 116786, 30426, 104613, 106138, 82158, 34983, 124365, 115284, 382, 35495, 108583, 25, 116334, 35243, 45780, 52375, 13094, 108231, 13, 122545, 168, 107, 97, 104613, 106138, 13094, 107123, 64356, 113191, 117677, 1980, 126862, 56154, 25, 64432, 102233, 104613, 106138, 82158, 102772, 101603, 101096, 33177, 111902, 43139, 220, 18, 93, 20, 33177, 111297, 101228, 36811, 114409, 13, 106792, 95415, 101015, 113002, 18918, 74959, 34983, 56773, 51402, 382, 35495, 108583, 25, 103315, 11, 102066, 115284, 13, 119083, 30426, 105642, 107651, 16969, 47782, 114067, 1980, 126862, 56154, 25, 103315, 11, 117717, 103521, 34804, 23955, 17835, 115954, 82158, 65219, 13879, 90463, 13, 105642, 108515, 126840, 13094, 36439, 34609, 30426, 33390, 122545, 82776, 22035, 127808, 56773, 119978, 382, 35495, 108583, 25, 117078, 61938, 382, 126862, 56154, 25, 103315, 11, 117078, 61938, 13, 111937, 123106, 98243, 51402, 13, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 개인정보를 감춰주는 로봇입니다.\n",
      "\n",
      "## 지시 사항 ##\n",
      "1.주어진 대화에서 사람이름을 [PERSON1], [PERSON2] 등으로 등장 순서에 따라 대체하고, 동일한 이름이 반복될 경우 같은 대치어를 사용합니다.\n",
      "2.연락처, 이메일, 주소, 계좌번호도 각각 [CONTACT1], [CONTACT2] 등, [EMAIL1],[EMAIL2] 등, [ADDRESS1],[ADDRESS2]등, [ACCOUNT1], [ACCOUNT2] 등 으로 대치하고 동일한 정보가 반복되는 경우에는 같은 대치어를 사용합니다.\n",
      "3.대치어를 작성할때 글머리 기호나, 나열식 방식을 쓰지말고 평문으로 이어서 쓰십시오 \n",
      "4.위 규칙은 대화 전체에 걸쳐 일관되게 적용합니다. \n",
      "당신이 개인정보를 감출 대화내역입니다. \n",
      ": 입력:<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "고객: 안녕하세요, 최근에 카드 청구서를 봤는데 같은 항목이 두 번 청구된 것 같아요. 확인 좀 해주실 수 있나요?\n",
      "\n",
      "상담사: 안녕하세요, 고객님. 저희가 도와드리기 위해서는 고객님의 개인정보가 필요합니다. 성함을 알려주시겠어요?\n",
      "\n",
      "고객: 네, 제 이름은 김민수입니다.\n",
      "\n",
      "상담사: 감사합니다. 김민수 고객님, 고객님의 전화번호와 주소도 말씀해주시면 감사하겠습니다.\n",
      "\n",
      "고객: 전화번호는 010-1234-5678이고, 주소는 서울시 강남구 테헤란로 123입니다.\n",
      "\n",
      "상담사: 감사합니다. 고객님 계좌번호도 필요합니다. 이 부분은 꼭 안전하게 처리될 것입니다.\n",
      "\n",
      "고객: 네, 계좌번호는 123-4567-8901입니다.\n",
      "\n",
      "상담사: 확인해주셔서 감사합니다. 고객님께서 말씀하신 항목을 확인해본 결과, 같은 항목이 두 번 청구된 것을 확인했습니다. 저희 쪽에서 즉시 환불 처리해드리겠습니다.\n",
      "\n",
      "고객: 정말 다행이네요. 언제쯤 환불이 완료될까요?\n",
      "\n",
      "상담사: 보통 환불 처리에는 영업일 기준으로 3~5일 정도 소요됩니다. 거래 후계좌를 확인해 주세요.\n",
      "\n",
      "고객: 네, 알겠습니다. 혹시 다른 문제는 없나요?\n",
      "\n",
      "상담사: 네, 이번 건은 이로써 처리되었습니다. 다른 문의 사항이 있으시면 언제든지 연락 주십시오.\n",
      "\n",
      "고객: 감사합니다.\n",
      "\n",
      "상담사: 네, 감사합니다. 좋은 하루 되세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>고객: 안녕하세요, 최근에 카드 청구서를 봤는데 같은 항목이 두 번 청구된 것 같아요. 확인 좀 해주실 수 있나요?\n",
      "\n",
      "상담사: 안녕하세요, 고객님. 저희가 도와드리기 위해서는 고객님의 개인정보가 필요합니다. 성함을 알려주시겠어요?\n",
      "\n",
      "고객: 네, 제 이름은 [PERSON1]입니다.\n",
      "\n",
      "상담사: 감사합니다. [PERSON1] 고객님, 고객님의 전화번호와 주소도 말씀해주시면 감사하겠습니다.\n",
      "\n",
      "고객: 전화번호는 [CONTACT1]이고, 주소는 [ADDRESS1]입니다.\n",
      "\n",
      "상담사: 감사합니다. 고객님 계좌번호도 필요합니다. 이 부분은 꼭 안전하게 처리될 것입니다.\n",
      "\n",
      "고객: 네, 계좌번호는 [ACCOUNT1]입니다.\n",
      "\n",
      "상담사: 확인해주셔서 감사합니다. 고객님께서 말씀하신 항목을 확인해본 결과, 같은 항목이 두 번 청구된 것을 확인했습니다. 저희 쪽에서 즉시 환불 처리해드리겠습니다.\n",
      "\n",
      "고객: 정말 다행이네요. 언제쯤 환불이 완료될까요?\n",
      "\n",
      "상담사: 보통 환불 처리에는 영업일 기준으로 3~5일 정도 소요됩니다. 거래 후계좌를 확인해 주세요.\n",
      "\n",
      "고객: 네, 알겠습니다. 혹시 다른 문제는 없나요?\n",
      "\n",
      "상담사: 네, 이번 건은 이로써 처리되었습니다. 다른 문의 사항이 있으시면 언제든지 연락 주십시오.\n",
      "\n",
      "고객: 감사합니다.\n",
      "\n",
      "상담사: 네, 감사합니다. 좋은 하루 되세요.<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 35495, 108583, 25, 96270, 124409, 11, 119929, 19954, 103236, 30446, 105519, 89359, 27796, 18918, 106562, 97, 103170, 105718, 107744, 88708, 13094, 103405, 85721, 105519, 89359, 53400, 72208, 79474, 112737, 13, 74959, 110979, 61816, 55430, 101272, 29833, 36439, 114067, 1980, 126862, 56154, 25, 96270, 124409, 11, 116534, 102424, 13, 102678, 105204, 20565, 101703, 81673, 124365, 21121, 126308, 16969, 116534, 119673, 111097, 105922, 20565, 108289, 61938, 13, 102132, 79053, 18359, 116023, 113161, 103373, 105807, 1980, 35495, 108583, 25, 103315, 11, 63171, 87134, 34804, 510, 94283, 16, 60, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 510, 94283, 16, 60, 116534, 102424, 11, 116534, 119673, 126100, 73148, 81673, 56773, 44690, 49085, 123660, 34983, 113161, 33390, 117078, 16582, 115284, 382, 35495, 108583, 25, 126100, 73148, 16969, 510, 74471, 16, 60, 109816, 11, 56773, 44690, 16969, 510, 62105, 16, 60, 80052, 382, 126862, 56154, 25, 117078, 61938, 13, 116534, 102424, 95303, 113002, 73148, 49085, 108289, 61938, 13, 23955, 115155, 34804, 121916, 116281, 102893, 82158, 113191, 123263, 382, 35495, 108583, 25, 103315, 11, 95303, 113002, 73148, 16969, 510, 74483, 16, 60, 80052, 382, 126862, 56154, 25, 74959, 34983, 55430, 110311, 27796, 117078, 61938, 13, 116534, 102424, 127737, 123660, 124933, 107744, 88708, 18359, 74959, 34983, 101948, 99901, 11, 105718, 107744, 88708, 13094, 103405, 85721, 105519, 89359, 53400, 107387, 74959, 116304, 13, 102678, 105204, 113866, 57575, 116786, 30426, 104613, 106138, 82158, 34983, 124365, 115284, 382, 35495, 108583, 25, 116334, 35243, 45780, 52375, 13094, 108231, 13, 122545, 168, 107, 97, 104613, 106138, 13094, 107123, 64356, 113191, 117677, 1980, 126862, 56154, 25, 64432, 102233, 104613, 106138, 82158, 102772, 101603, 101096, 33177, 111902, 43139, 220, 18, 93, 20, 33177, 111297, 101228, 36811, 114409, 13, 106792, 95415, 101015, 113002, 18918, 74959, 34983, 56773, 51402, 382, 35495, 108583, 25, 103315, 11, 102066, 115284, 13, 119083, 30426, 105642, 107651, 16969, 47782, 114067, 1980, 126862, 56154, 25, 103315, 11, 117717, 103521, 34804, 23955, 17835, 115954, 82158, 65219, 13879, 90463, 13, 105642, 108515, 126840, 13094, 36439, 34609, 30426, 33390, 122545, 82776, 22035, 127808, 56773, 119978, 382, 35495, 108583, 25, 117078, 61938, 382, 126862, 56154, 25, 103315, 11, 117078, 61938, 13, 111937, 123106, 98243, 51402, 13, 128009]\n",
      "labels:\n",
      "고객: 안녕하세요, 최근에 카드 청구서를 봤는데 같은 항목이 두 번 청구된 것 같아요. 확인 좀 해주실 수 있나요?\n",
      "\n",
      "상담사: 안녕하세요, 고객님. 저희가 도와드리기 위해서는 고객님의 개인정보가 필요합니다. 성함을 알려주시겠어요?\n",
      "\n",
      "고객: 네, 제 이름은 [PERSON1]입니다.\n",
      "\n",
      "상담사: 감사합니다. [PERSON1] 고객님, 고객님의 전화번호와 주소도 말씀해주시면 감사하겠습니다.\n",
      "\n",
      "고객: 전화번호는 [CONTACT1]이고, 주소는 [ADDRESS1]입니다.\n",
      "\n",
      "상담사: 감사합니다. 고객님 계좌번호도 필요합니다. 이 부분은 꼭 안전하게 처리될 것입니다.\n",
      "\n",
      "고객: 네, 계좌번호는 [ACCOUNT1]입니다.\n",
      "\n",
      "상담사: 확인해주셔서 감사합니다. 고객님께서 말씀하신 항목을 확인해본 결과, 같은 항목이 두 번 청구된 것을 확인했습니다. 저희 쪽에서 즉시 환불 처리해드리겠습니다.\n",
      "\n",
      "고객: 정말 다행이네요. 언제쯤 환불이 완료될까요?\n",
      "\n",
      "상담사: 보통 환불 처리에는 영업일 기준으로 3~5일 정도 소요됩니다. 거래 후계좌를 확인해 주세요.\n",
      "\n",
      "고객: 네, 알겠습니다. 혹시 다른 문제는 없나요?\n",
      "\n",
      "상담사: 네, 이번 건은 이로써 처리되었습니다. 다른 문의 사항이 있으시면 언제든지 연락 주십시오.\n",
      "\n",
      "고객: 감사합니다.\n",
      "\n",
      "상담사: 네, 감사합니다. 좋은 하루 되세요.<|eot_id|>\n",
      "model.safetensors.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 71.9MB/s]\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0%|             | 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 10.5M/4.98G [00:00<01:53, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 21.0M/4.98G [00:00<01:26, 57.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 31.5M/4.98G [00:00<01:10, 69.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 52.4M/4.98G [00:00<00:51, 96.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|     | 73.4M/4.98G [00:00<00:41, 119MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|     | 94.4M/4.98G [00:00<00:36, 133MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|▏     | 115M/4.98G [00:01<00:33, 145MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏     | 136M/4.98G [00:01<00:30, 158MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏     | 157M/4.98G [00:01<00:28, 171MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏     | 178M/4.98G [00:01<00:26, 182MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏     | 199M/4.98G [00:01<00:25, 189MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▎     | 220M/4.98G [00:01<00:24, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎     | 241M/4.98G [00:01<00:24, 192MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎     | 273M/4.98G [00:01<00:28, 166MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎     | 294M/4.98G [00:02<00:28, 164MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▍     | 315M/4.98G [00:02<00:27, 170MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▍     | 346M/4.98G [00:02<00:24, 187MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▍     | 367M/4.98G [00:02<00:24, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍     | 398M/4.98G [00:02<00:23, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▌     | 419M/4.98G [00:02<00:24, 189MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▌     | 440M/4.98G [00:02<00:24, 185MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▌     | 461M/4.98G [00:02<00:24, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▌     | 482M/4.98G [00:02<00:23, 188MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▌     | 503M/4.98G [00:03<00:23, 187MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▋     | 535M/4.98G [00:03<00:23, 189MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▋     | 556M/4.98G [00:03<00:23, 190MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▋     | 577M/4.98G [00:03<00:23, 191MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▋     | 598M/4.98G [00:03<00:23, 187MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▋     | 619M/4.98G [00:03<00:24, 175MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▊     | 640M/4.98G [00:03<00:24, 175MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▊     | 661M/4.98G [00:03<00:23, 181MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▊     | 682M/4.98G [00:04<00:22, 187MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▊     | 713M/4.98G [00:04<00:21, 199MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▉     | 744M/4.98G [00:04<00:21, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▉     | 776M/4.98G [00:04<00:21, 200MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▉     | 797M/4.98G [00:04<00:21, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▉     | 818M/4.98G [00:04<00:22, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|█     | 839M/4.98G [00:04<00:22, 187MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|█     | 860M/4.98G [00:04<00:22, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|█     | 881M/4.98G [00:05<00:21, 192MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|█     | 902M/4.98G [00:05<00:21, 189MB/s]\u001b[A[INFO|configuration_utils.py:699] 2025-03-20 00:38:20,055 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 00:38:20,056 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3982] 2025-03-20 00:38:20,114 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/model.safetensors.index.json\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:  19%|█     | 923M/4.98G [00:05<00:21, 189MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|█▏    | 944M/4.98G [00:05<00:20, 193MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|█▏    | 965M/4.98G [00:05<00:20, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|█▏    | 986M/4.98G [00:05<00:22, 175MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|█    | 1.01G/4.98G [00:05<00:21, 183MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|█    | 1.04G/4.98G [00:05<00:20, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|█    | 1.06G/4.98G [00:06<00:20, 188MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|█    | 1.08G/4.98G [00:06<00:20, 192MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|█    | 1.10G/4.98G [00:06<00:19, 195MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|█▏   | 1.12G/4.98G [00:06<00:19, 195MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|█▏   | 1.15G/4.98G [00:06<00:19, 199MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|█▏   | 1.17G/4.98G [00:06<00:19, 199MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|█▏   | 1.20G/4.98G [00:06<00:19, 191MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|█▏   | 1.22G/4.98G [00:06<00:20, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█▎   | 1.25G/4.98G [00:07<00:19, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█▎   | 1.27G/4.98G [00:07<00:19, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█▎   | 1.30G/4.98G [00:07<00:18, 197MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█▎   | 1.32G/4.98G [00:07<00:18, 199MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█▎   | 1.34G/4.98G [00:07<00:18, 199MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█▎   | 1.36G/4.98G [00:07<00:17, 201MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█▍   | 1.39G/4.98G [00:07<00:17, 205MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█▍   | 1.42G/4.98G [00:07<00:17, 206MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▍   | 1.44G/4.98G [00:07<00:17, 201MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▍   | 1.46G/4.98G [00:08<00:17, 197MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▍   | 1.48G/4.98G [00:08<00:17, 198MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▌   | 1.50G/4.98G [00:08<00:17, 199MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▌   | 1.52G/4.98G [00:08<00:17, 198MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▌   | 1.54G/4.98G [00:08<00:17, 200MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▌   | 1.57G/4.98G [00:08<00:17, 197MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▌   | 1.59G/4.98G [00:08<00:16, 199MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▌   | 1.61G/4.98G [00:08<00:17, 190MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▋   | 1.64G/4.98G [00:08<00:17, 192MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▋   | 1.66G/4.98G [00:09<00:16, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▋   | 1.68G/4.98G [00:09<00:16, 200MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▋   | 1.70G/4.98G [00:09<00:16, 198MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▋   | 1.72G/4.98G [00:09<00:17, 191MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▋   | 1.74G/4.98G [00:09<00:16, 195MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▊   | 1.76G/4.98G [00:09<00:16, 197MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▊   | 1.79G/4.98G [00:09<00:15, 201MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▊   | 1.81G/4.98G [00:09<00:16, 198MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▊   | 1.84G/4.98G [00:09<00:15, 200MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▉   | 1.87G/4.98G [00:10<00:19, 159MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▉   | 1.89G/4.98G [00:10<00:18, 164MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▉   | 1.91G/4.98G [00:10<00:17, 174MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▉   | 1.93G/4.98G [00:10<00:20, 146MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▉   | 1.95G/4.98G [00:10<00:19, 157MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▉   | 1.97G/4.98G [00:10<00:21, 140MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|██   | 1.99G/4.98G [00:11<00:24, 123MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|██   | 2.01G/4.98G [00:11<00:25, 118MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|██   | 2.03G/4.98G [00:11<00:22, 133MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|██   | 2.06G/4.98G [00:11<00:25, 113MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|██   | 2.08G/4.98G [00:11<00:23, 124MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|██   | 2.10G/4.98G [00:11<00:20, 140MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|██▏  | 2.12G/4.98G [00:12<00:18, 154MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|██▏  | 2.14G/4.98G [00:12<00:17, 164MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|██▏  | 2.16G/4.98G [00:12<00:17, 165MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|██▏  | 2.18G/4.98G [00:12<00:16, 174MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|██▏  | 2.20G/4.98G [00:12<00:15, 182MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|██▏  | 2.23G/4.98G [00:12<00:14, 192MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|██▎  | 2.25G/4.98G [00:12<00:14, 188MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|██▎  | 2.28G/4.98G [00:12<00:14, 190MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|██▎  | 2.30G/4.98G [00:12<00:14, 191MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|██▎  | 2.32G/4.98G [00:13<00:13, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|██▎  | 2.34G/4.98G [00:13<00:13, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|██▎  | 2.36G/4.98G [00:13<00:13, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|██▍  | 2.38G/4.98G [00:13<00:14, 178MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|██▍  | 2.40G/4.98G [00:13<00:17, 147MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|██▍  | 2.42G/4.98G [00:13<00:23, 111MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|██▍  | 2.44G/4.98G [00:14<00:25, 101MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.46G/4.98G [00:14<00:28, 87.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.47G/4.98G [00:14<00:31, 78.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██  | 2.50G/4.98G [00:15<00:33, 74.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██  | 2.51G/4.98G [00:15<00:35, 69.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.53G/4.98G [00:15<00:33, 72.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.55G/4.98G [00:15<00:26, 92.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.57G/4.98G [00:15<00:28, 85.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.58G/4.98G [00:16<00:32, 74.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.60G/4.98G [00:16<00:28, 84.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██▋  | 2.62G/4.98G [00:16<00:22, 104MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██▋  | 2.64G/4.98G [00:16<00:18, 123MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▋  | 2.66G/4.98G [00:16<00:16, 141MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▋  | 2.68G/4.98G [00:16<00:14, 154MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▋  | 2.71G/4.98G [00:16<00:13, 166MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▋  | 2.73G/4.98G [00:16<00:12, 176MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▊  | 2.75G/4.98G [00:17<00:12, 182MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▊  | 2.77G/4.98G [00:17<00:12, 175MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▊  | 2.79G/4.98G [00:17<00:13, 167MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▊  | 2.81G/4.98G [00:17<00:12, 177MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▊  | 2.83G/4.98G [00:17<00:11, 184MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▊  | 2.85G/4.98G [00:17<00:11, 184MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▉  | 2.87G/4.98G [00:17<00:11, 185MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▉  | 2.89G/4.98G [00:17<00:11, 176MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▉  | 2.92G/4.98G [00:17<00:11, 181MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▉  | 2.94G/4.98G [00:18<00:10, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▉  | 2.96G/4.98G [00:18<00:11, 181MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▉  | 2.98G/4.98G [00:18<00:11, 181MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|███  | 3.00G/4.98G [00:18<00:11, 175MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|███  | 3.02G/4.98G [00:18<00:11, 178MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|███  | 3.04G/4.98G [00:18<00:10, 182MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|███  | 3.06G/4.98G [00:18<00:10, 184MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|███  | 3.08G/4.98G [00:18<00:09, 191MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|███▏ | 3.11G/4.98G [00:19<00:09, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|███▏ | 3.14G/4.98G [00:19<00:09, 193MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|███▏ | 3.16G/4.98G [00:19<00:09, 193MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|███▏ | 3.18G/4.98G [00:19<00:09, 190MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|███▏ | 3.20G/4.98G [00:19<00:14, 119MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|███▏ | 3.23G/4.98G [00:19<00:12, 142MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|███▎ | 3.25G/4.98G [00:19<00:11, 155MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|███▎ | 3.27G/4.98G [00:20<00:10, 166MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|███▎ | 3.29G/4.98G [00:20<00:09, 176MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|███▎ | 3.31G/4.98G [00:20<00:09, 184MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|███▎ | 3.33G/4.98G [00:20<00:08, 190MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|███▍ | 3.37G/4.98G [00:20<00:08, 198MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|███▍ | 3.39G/4.98G [00:20<00:09, 174MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|███▍ | 3.41G/4.98G [00:20<00:09, 165MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|███▍ | 3.43G/4.98G [00:20<00:09, 164MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|███▍ | 3.45G/4.98G [00:21<00:08, 172MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|███▍ | 3.48G/4.98G [00:21<00:08, 173MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|███▌ | 3.50G/4.98G [00:21<00:08, 172MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|███▌ | 3.52G/4.98G [00:21<00:08, 179MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|███▌ | 3.54G/4.98G [00:21<00:07, 183MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|███▌ | 3.57G/4.98G [00:21<00:07, 188MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|███▌ | 3.60G/4.98G [00:21<00:07, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|███▋ | 3.62G/4.98G [00:21<00:06, 199MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|███▋ | 3.64G/4.98G [00:22<00:06, 198MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|███▋ | 3.66G/4.98G [00:22<00:06, 198MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|███▋ | 3.68G/4.98G [00:22<00:06, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|███▋ | 3.70G/4.98G [00:22<00:06, 195MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███▊ | 3.73G/4.98G [00:22<00:06, 197MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███▊ | 3.76G/4.98G [00:22<00:05, 202MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███▊ | 3.79G/4.98G [00:22<00:05, 203MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███▊ | 3.81G/4.98G [00:22<00:05, 204MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███▊ | 3.83G/4.98G [00:22<00:05, 204MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███▊ | 3.85G/4.98G [00:23<00:05, 189MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███▉ | 3.87G/4.98G [00:23<00:05, 188MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███▉ | 3.89G/4.98G [00:23<00:05, 185MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▉ | 3.91G/4.98G [00:23<00:05, 189MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▉ | 3.93G/4.98G [00:23<00:05, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▉ | 3.95G/4.98G [00:23<00:05, 198MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▉ | 3.97G/4.98G [00:23<00:05, 172MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|████ | 4.00G/4.98G [00:23<00:05, 176MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|████ | 4.02G/4.98G [00:24<00:05, 184MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|████ | 4.04G/4.98G [00:24<00:05, 184MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|████ | 4.06G/4.98G [00:24<00:04, 189MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|████ | 4.08G/4.98G [00:24<00:04, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|████ | 4.10G/4.98G [00:24<00:04, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|████▏| 4.12G/4.98G [00:24<00:05, 160MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|████▏| 4.14G/4.98G [00:24<00:05, 143MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|████▏| 4.16G/4.98G [00:24<00:06, 134MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|████▏| 4.18G/4.98G [00:25<00:05, 134MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|████▏| 4.20G/4.98G [00:25<00:06, 124MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|████▏| 4.23G/4.98G [00:25<00:05, 127MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|████▎| 4.25G/4.98G [00:25<00:05, 131MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|████▎| 4.27G/4.98G [00:25<00:05, 123MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|████▎| 4.29G/4.98G [00:25<00:05, 127MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|████▎| 4.31G/4.98G [00:26<00:05, 131MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|████▎| 4.33G/4.98G [00:26<00:04, 133MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|████▎| 4.35G/4.98G [00:26<00:05, 121MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|████▍| 4.37G/4.98G [00:26<00:05, 107MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|████▍| 4.39G/4.98G [00:26<00:05, 106MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|████▍| 4.41G/4.98G [00:27<00:05, 103MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|████▍| 4.44G/4.98G [00:27<00:05, 100MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.45G/4.98G [00:27<00:05, 93.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.46G/4.98G [00:27<00:05, 91.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.47G/4.98G [00:27<00:06, 84.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.48G/4.98G [00:27<00:05, 87.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.50G/4.98G [00:28<00:05, 88.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.52G/4.98G [00:28<00:05, 85.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.53G/4.98G [00:28<00:05, 78.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.55G/4.98G [00:28<00:04, 85.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.57G/4.98G [00:28<00:04, 98.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|████▌| 4.59G/4.98G [00:29<00:03, 109MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|████▋| 4.61G/4.98G [00:29<00:03, 116MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|████▋| 4.63G/4.98G [00:29<00:02, 121MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|████▋| 4.66G/4.98G [00:29<00:02, 124MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.68G/4.98G [00:29<00:03, 86.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.70G/4.98G [00:30<00:02, 96.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.72G/4.98G [00:30<00:02, 86.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.73G/4.98G [00:30<00:03, 69.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.74G/4.98G [00:30<00:03, 65.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.75G/4.98G [00:31<00:03, 68.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.77G/4.98G [00:31<00:02, 82.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.79G/4.98G [00:31<00:02, 88.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.80G/4.98G [00:31<00:02, 63.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.82G/4.98G [00:31<00:02, 74.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.84G/4.98G [00:32<00:01, 88.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.87G/4.98G [00:32<00:01, 99.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|████▉| 4.89G/4.98G [00:32<00:00, 103MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|████▉| 4.91G/4.98G [00:32<00:00, 110MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|████▉| 4.93G/4.98G [00:32<00:00, 117MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|████▉| 4.95G/4.98G [00:32<00:00, 115MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|█████| 4.98G/4.98G [00:33<00:00, 150MB/s]\u001b[A\n",
      "Downloading shards:  25%|██████▎                  | 1/4 [00:27<01:23, 27.95s/it]\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0%|    | 10.5M/5.00G [00:00<01:41, 49.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 31.5M/5.00G [00:00<00:57, 86.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|     | 52.4M/5.00G [00:00<00:48, 102MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|     | 73.4M/5.00G [00:00<00:44, 112MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|     | 94.4M/5.00G [00:00<00:44, 109MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|▏     | 115M/5.00G [00:01<00:42, 115MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏     | 136M/5.00G [00:01<00:39, 125MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏     | 157M/5.00G [00:01<00:36, 133MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏     | 178M/5.00G [00:01<00:34, 141MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏     | 199M/5.00G [00:01<00:32, 147MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▎     | 220M/5.00G [00:01<00:32, 149MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎     | 241M/5.00G [00:01<00:31, 153MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎     | 262M/5.00G [00:02<00:30, 157MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎     | 283M/5.00G [00:02<00:29, 162MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎     | 304M/5.00G [00:02<00:30, 155MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▍     | 325M/5.00G [00:02<00:30, 155MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▍     | 346M/5.00G [00:02<00:30, 154MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▍     | 367M/5.00G [00:02<00:28, 160MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍     | 388M/5.00G [00:02<00:27, 167MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍     | 409M/5.00G [00:02<00:32, 143MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▌     | 430M/5.00G [00:03<00:31, 143MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▌     | 451M/5.00G [00:03<00:29, 156MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▌     | 472M/5.00G [00:03<00:28, 162MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▌     | 493M/5.00G [00:03<00:26, 173MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▋     | 524M/5.00G [00:03<00:24, 186MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▋     | 545M/5.00G [00:03<00:25, 178MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▋     | 577M/5.00G [00:03<00:24, 182MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▋     | 598M/5.00G [00:04<00:24, 183MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▊     | 629M/5.00G [00:04<00:22, 194MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▊     | 650M/5.00G [00:04<00:22, 189MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▊     | 671M/5.00G [00:04<00:22, 194MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▊     | 703M/5.00G [00:04<00:21, 202MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▉     | 734M/5.00G [00:04<00:20, 205MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▉     | 765M/5.00G [00:04<00:20, 208MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▉     | 786M/5.00G [00:04<00:20, 206MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▉     | 818M/5.00G [00:05<00:19, 210MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|█     | 849M/5.00G [00:05<00:19, 211MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|█     | 881M/5.00G [00:05<00:21, 196MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|█     | 912M/5.00G [00:05<00:20, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|█▏    | 944M/5.00G [00:05<00:19, 206MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|█▏    | 975M/5.00G [00:05<00:19, 208MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|█    | 1.01G/5.00G [00:05<00:19, 209MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|█    | 1.04G/5.00G [00:06<00:18, 209MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|█    | 1.07G/5.00G [00:06<00:18, 209MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|█    | 1.10G/5.00G [00:06<00:18, 212MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|█▏   | 1.13G/5.00G [00:06<00:18, 205MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|█▏   | 1.15G/5.00G [00:06<00:18, 204MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|█▏   | 1.18G/5.00G [00:06<00:18, 209MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|█▏   | 1.21G/5.00G [00:06<00:18, 207MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█▏   | 1.24G/5.00G [00:07<00:18, 209MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█▎   | 1.26G/5.00G [00:07<00:18, 205MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█▎   | 1.28G/5.00G [00:07<00:18, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█▎   | 1.30G/5.00G [00:07<00:18, 202MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█▎   | 1.33G/5.00G [00:07<00:17, 208MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█▎   | 1.36G/5.00G [00:07<00:17, 211MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█▍   | 1.39G/5.00G [00:07<00:16, 213MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▍   | 1.43G/5.00G [00:08<00:16, 212MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▍   | 1.46G/5.00G [00:08<00:16, 214MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▍   | 1.49G/5.00G [00:08<00:16, 216MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▌   | 1.52G/5.00G [00:08<00:16, 217MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▌   | 1.55G/5.00G [00:08<00:16, 208MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▌   | 1.58G/5.00G [00:08<00:16, 208MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▌   | 1.61G/5.00G [00:08<00:16, 211MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▋   | 1.65G/5.00G [00:09<00:15, 210MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▋   | 1.68G/5.00G [00:09<00:15, 208MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▋   | 1.71G/5.00G [00:09<00:15, 210MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▋   | 1.74G/5.00G [00:09<00:15, 211MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▊   | 1.77G/5.00G [00:09<00:15, 203MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▊   | 1.79G/5.00G [00:09<00:17, 187MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▊   | 1.81G/5.00G [00:09<00:18, 175MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▊   | 1.84G/5.00G [00:10<00:25, 123MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▊   | 1.86G/5.00G [00:10<00:24, 127MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▉   | 1.88G/5.00G [00:10<00:24, 125MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▉   | 1.90G/5.00G [00:10<00:29, 104MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.92G/5.00G [00:11<00:33, 92.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▉   | 1.94G/5.00G [00:11<00:29, 103MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▉   | 1.96G/5.00G [00:11<00:28, 105MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 1.98G/5.00G [00:11<00:31, 95.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 1.99G/5.00G [00:11<00:35, 84.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 2.01G/5.00G [00:12<00:30, 99.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.03G/5.00G [00:12<00:33, 89.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.04G/5.00G [00:12<00:37, 79.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.07G/5.00G [00:12<00:32, 90.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.09G/5.00G [00:13<00:35, 81.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.11G/5.00G [00:13<00:36, 79.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.12G/5.00G [00:13<00:35, 80.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.13G/5.00G [00:13<00:34, 82.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.14G/5.00G [00:13<00:41, 69.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.16G/5.00G [00:14<00:38, 73.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.17G/5.00G [00:14<00:40, 69.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.19G/5.00G [00:14<00:36, 77.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.21G/5.00G [00:14<00:30, 90.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.22G/5.00G [00:14<00:36, 76.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.23G/5.00G [00:14<00:34, 81.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.24G/5.00G [00:15<00:37, 73.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.25G/5.00G [00:15<00:39, 68.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.28G/5.00G [00:15<00:30, 89.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.29G/5.00G [00:15<00:30, 88.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.30G/5.00G [00:15<00:32, 83.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.32G/5.00G [00:16<00:33, 79.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▊  | 2.34G/5.00G [00:16<00:36, 72.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|██▎  | 2.37G/5.00G [00:16<00:25, 103MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|██▍  | 2.40G/5.00G [00:16<00:19, 131MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|██▍  | 2.43G/5.00G [00:16<00:16, 153MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|██▍  | 2.46G/5.00G [00:16<00:15, 168MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|██▍  | 2.50G/5.00G [00:17<00:13, 183MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|██▌  | 2.52G/5.00G [00:17<00:13, 185MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██▌  | 2.54G/5.00G [00:17<00:15, 161MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██▌  | 2.56G/5.00G [00:17<00:16, 151MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██▌  | 2.58G/5.00G [00:17<00:15, 152MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██▌  | 2.60G/5.00G [00:17<00:14, 164MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██▌  | 2.62G/5.00G [00:17<00:15, 155MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██▋  | 2.64G/5.00G [00:18<00:14, 162MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██▋  | 2.66G/5.00G [00:18<00:13, 173MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▋  | 2.68G/5.00G [00:18<00:13, 177MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▋  | 2.71G/5.00G [00:18<00:15, 144MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▋  | 2.73G/5.00G [00:18<00:14, 152MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▋  | 2.75G/5.00G [00:18<00:13, 163MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▊  | 2.78G/5.00G [00:18<00:12, 182MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▊  | 2.81G/5.00G [00:18<00:11, 193MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▊  | 2.84G/5.00G [00:19<00:10, 200MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▊  | 2.86G/5.00G [00:19<00:11, 190MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▉  | 2.88G/5.00G [00:19<00:10, 195MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▉  | 2.90G/5.00G [00:19<00:10, 191MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▉  | 2.94G/5.00G [00:19<00:10, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▉  | 2.96G/5.00G [00:19<00:10, 197MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▉  | 2.98G/5.00G [00:19<00:10, 192MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▉  | 3.00G/5.00G [00:19<00:10, 185MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|███  | 3.02G/5.00G [00:20<00:10, 185MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|███  | 3.05G/5.00G [00:20<00:10, 195MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|███  | 3.08G/5.00G [00:20<00:09, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|███  | 3.10G/5.00G [00:20<00:09, 192MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|███  | 3.12G/5.00G [00:20<00:10, 182MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|███▏ | 3.16G/5.00G [00:20<00:09, 195MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|███▏ | 3.19G/5.00G [00:20<00:08, 204MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|███▏ | 3.22G/5.00G [00:21<00:08, 209MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|███▎ | 3.25G/5.00G [00:21<00:08, 213MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|███▎ | 3.28G/5.00G [00:21<00:08, 194MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|███▎ | 3.30G/5.00G [00:21<00:08, 194MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|███▎ | 3.32G/5.00G [00:21<00:09, 183MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|███▎ | 3.36G/5.00G [00:21<00:08, 193MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|███▍ | 3.39G/5.00G [00:21<00:08, 199MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|███▍ | 3.41G/5.00G [00:22<00:07, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|███▍ | 3.44G/5.00G [00:22<00:07, 206MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|███▍ | 3.47G/5.00G [00:22<00:07, 210MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|███▍ | 3.49G/5.00G [00:22<00:07, 200MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|███▌ | 3.51G/5.00G [00:22<00:08, 172MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|███▌ | 3.53G/5.00G [00:22<00:09, 160MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|███▌ | 3.55G/5.00G [00:22<00:08, 164MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|███▌ | 3.58G/5.00G [00:22<00:08, 165MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|███▌ | 3.60G/5.00G [00:23<00:08, 174MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|███▌ | 3.62G/5.00G [00:23<00:07, 180MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|███▋ | 3.64G/5.00G [00:23<00:07, 179MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|███▋ | 3.66G/5.00G [00:23<00:07, 181MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|███▋ | 3.68G/5.00G [00:23<00:08, 161MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|███▋ | 3.71G/5.00G [00:23<00:07, 179MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|███▋ | 3.73G/5.00G [00:23<00:06, 184MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|███▊ | 3.75G/5.00G [00:23<00:06, 185MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███▊ | 3.77G/5.00G [00:24<00:06, 186MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███▊ | 3.81G/5.00G [00:24<00:06, 196MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███▊ | 3.84G/5.00G [00:24<00:05, 204MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███▊ | 3.86G/5.00G [00:24<00:05, 199MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▉ | 3.89G/5.00G [00:24<00:05, 207MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▉ | 3.92G/5.00G [00:24<00:05, 212MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▉ | 3.95G/5.00G [00:24<00:05, 207MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▉ | 3.97G/5.00G [00:25<00:05, 186MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|████ | 4.01G/5.00G [00:25<00:05, 196MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|████ | 4.04G/5.00G [00:25<00:04, 202MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|████ | 4.06G/5.00G [00:25<00:04, 202MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|████ | 4.08G/5.00G [00:25<00:04, 199MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|████ | 4.10G/5.00G [00:25<00:04, 196MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|████ | 4.12G/5.00G [00:25<00:04, 178MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|████▏| 4.14G/5.00G [00:25<00:04, 176MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|████▏| 4.16G/5.00G [00:26<00:04, 184MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|████▏| 4.18G/5.00G [00:26<00:04, 171MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|████▏| 4.20G/5.00G [00:26<00:05, 150MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|████▏| 4.23G/5.00G [00:26<00:06, 113MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|████▏| 4.25G/5.00G [00:26<00:06, 114MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|████▎| 4.27G/5.00G [00:27<00:06, 118MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.29G/5.00G [00:27<00:07, 96.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.31G/5.00G [00:27<00:09, 74.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.32G/5.00G [00:28<00:10, 63.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.33G/5.00G [00:28<00:10, 61.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.34G/5.00G [00:28<00:10, 62.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.36G/5.00G [00:28<00:08, 76.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.38G/5.00G [00:28<00:06, 94.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.40G/5.00G [00:28<00:06, 86.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.42G/5.00G [00:29<00:06, 93.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|████▍| 4.46G/5.00G [00:29<00:05, 102MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|████▍| 4.48G/5.00G [00:29<00:05, 104MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.50G/5.00G [00:29<00:05, 90.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.51G/5.00G [00:30<00:06, 73.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▌| 4.53G/5.00G [00:30<00:06, 73.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.55G/5.00G [00:30<00:05, 77.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.56G/5.00G [00:30<00:05, 78.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.57G/5.00G [00:31<00:05, 72.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.58G/5.00G [00:31<00:05, 73.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.59G/5.00G [00:31<00:06, 63.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.61G/5.00G [00:31<00:04, 88.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.63G/5.00G [00:31<00:04, 81.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.65G/5.00G [00:31<00:04, 81.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.66G/5.00G [00:32<00:04, 74.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▋| 4.68G/5.00G [00:32<00:03, 90.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|████▋| 4.71G/5.00G [00:32<00:02, 119MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|████▋| 4.73G/5.00G [00:32<00:02, 114MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|████▊| 4.75G/5.00G [00:32<00:02, 121MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|████▊| 4.77G/5.00G [00:32<00:01, 127MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.79G/5.00G [00:33<00:02, 97.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|████▊| 4.82G/5.00G [00:33<00:01, 124MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|████▊| 4.84G/5.00G [00:33<00:01, 136MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|████▊| 4.87G/5.00G [00:33<00:01, 125MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|████▉| 4.89G/5.00G [00:33<00:00, 129MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|████▉| 4.92G/5.00G [00:34<00:00, 154MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|████▉| 4.94G/5.00G [00:34<00:00, 165MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|████▉| 4.97G/5.00G [00:34<00:00, 180MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|█████| 5.00G/5.00G [00:34<00:00, 145MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 2/4 [01:02<01:03, 31.84s/it]\n",
      "model-00003-of-00004.safetensors:   0%|             | 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0%|    | 10.5M/4.92G [00:00<01:55, 42.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0%|    | 21.0M/4.92G [00:00<01:24, 57.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 31.5M/4.92G [00:00<01:07, 72.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 52.4M/4.92G [00:00<00:51, 95.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|     | 73.4M/4.92G [00:00<00:40, 119MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|     | 94.4M/4.92G [00:00<00:33, 142MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|▏     | 115M/4.92G [00:00<00:31, 151MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏     | 136M/4.92G [00:01<00:31, 153MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏     | 157M/4.92G [00:01<00:28, 168MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏     | 178M/4.92G [00:01<00:27, 170MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▎     | 210M/4.92G [00:01<00:25, 185MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▎     | 241M/4.92G [00:01<00:24, 194MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎     | 273M/4.92G [00:01<00:23, 200MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎     | 304M/4.92G [00:01<00:22, 204MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▍     | 325M/4.92G [00:02<00:22, 204MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▍     | 357M/4.92G [00:02<00:22, 206MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍     | 388M/4.92G [00:02<00:21, 211MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▌     | 419M/4.92G [00:02<00:21, 211MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▌     | 451M/4.92G [00:02<00:21, 212MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▌     | 482M/4.92G [00:02<00:20, 214MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▋     | 514M/4.92G [00:02<00:20, 216MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▋     | 545M/4.92G [00:03<00:20, 216MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▋     | 577M/4.92G [00:03<00:19, 218MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▋     | 608M/4.92G [00:03<00:19, 218MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▊     | 640M/4.92G [00:03<00:19, 218MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▊     | 671M/4.92G [00:03<00:19, 218MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▊     | 703M/4.92G [00:03<00:19, 216MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▉     | 734M/4.92G [00:03<00:19, 217MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▉     | 765M/4.92G [00:04<00:19, 212MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▉     | 797M/4.92G [00:04<00:19, 215MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|█     | 828M/4.92G [00:04<00:19, 215MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|█     | 860M/4.92G [00:04<00:18, 216MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|█     | 891M/4.92G [00:04<00:18, 218MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|█▏    | 923M/4.92G [00:04<00:18, 219MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|█▏    | 954M/4.92G [00:04<00:18, 216MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|█▏    | 986M/4.92G [00:05<00:18, 217MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|█    | 1.02G/4.92G [00:05<00:18, 215MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|█    | 1.05G/4.92G [00:05<00:17, 215MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|█    | 1.08G/4.92G [00:05<00:17, 214MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|█▏   | 1.11G/4.92G [00:05<00:17, 217MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|█▏   | 1.14G/4.92G [00:05<00:17, 219MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|█▏   | 1.17G/4.92G [00:06<00:19, 193MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|█▏   | 1.20G/4.92G [00:06<00:21, 172MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|█▏   | 1.23G/4.92G [00:06<00:20, 184MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|█▎   | 1.25G/4.92G [00:06<00:19, 186MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█▎   | 1.27G/4.92G [00:06<00:19, 188MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█▎   | 1.30G/4.92G [00:06<00:18, 198MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█▎   | 1.33G/4.92G [00:06<00:17, 205MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█▍   | 1.36G/4.92G [00:07<00:17, 206MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█▍   | 1.39G/4.92G [00:07<00:16, 210MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▍   | 1.43G/4.92G [00:07<00:16, 211MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▍   | 1.46G/4.92G [00:07<00:16, 214MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▌   | 1.49G/4.92G [00:07<00:15, 217MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▌   | 1.52G/4.92G [00:07<00:15, 218MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▌   | 1.55G/4.92G [00:07<00:15, 219MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▌   | 1.58G/4.92G [00:08<00:15, 220MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▋   | 1.61G/4.92G [00:08<00:15, 219MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▋   | 1.65G/4.92G [00:08<00:15, 214MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▋   | 1.68G/4.92G [00:08<00:15, 214MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▋   | 1.71G/4.92G [00:08<00:15, 213MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▊   | 1.74G/4.92G [00:08<00:15, 209MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▊   | 1.77G/4.92G [00:08<00:14, 213MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▊   | 1.80G/4.92G [00:09<00:14, 209MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▊   | 1.84G/4.92G [00:09<00:14, 211MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▉   | 1.87G/4.92G [00:09<00:14, 212MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▉   | 1.90G/4.92G [00:09<00:14, 213MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▉   | 1.93G/4.92G [00:09<00:13, 214MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|█▉   | 1.96G/4.92G [00:09<00:13, 216MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|██   | 1.99G/4.92G [00:09<00:13, 218MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|██   | 2.02G/4.92G [00:10<00:13, 219MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|██   | 2.06G/4.92G [00:10<00:23, 122MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|██   | 2.09G/4.92G [00:10<00:21, 133MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.11G/4.92G [00:11<00:28, 98.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|██▏  | 2.13G/4.92G [00:11<00:24, 112MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|██▏  | 2.16G/4.92G [00:11<00:21, 130MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|██▏  | 2.18G/4.92G [00:11<00:20, 133MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|██▏  | 2.20G/4.92G [00:11<00:20, 130MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|██▎  | 2.23G/4.92G [00:11<00:17, 153MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|██▎  | 2.26G/4.92G [00:12<00:15, 170MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|██▎  | 2.29G/4.92G [00:12<00:15, 170MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|██▎  | 2.31G/4.92G [00:12<00:14, 175MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|██▎  | 2.33G/4.92G [00:12<00:25, 102MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.35G/4.92G [00:12<00:26, 97.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|██▍  | 2.37G/4.92G [00:13<00:23, 110MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|██▍  | 2.39G/4.92G [00:13<00:22, 114MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|██▍  | 2.41G/4.92G [00:13<00:21, 117MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|██▍  | 2.43G/4.92G [00:13<00:23, 107MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|█▉  | 2.45G/4.92G [00:14<00:31, 77.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|██  | 2.46G/4.92G [00:14<00:33, 72.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.49G/4.92G [00:14<00:30, 80.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.51G/4.92G [00:14<00:24, 98.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.53G/4.92G [00:15<00:32, 74.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.54G/4.92G [00:15<00:34, 68.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.55G/4.92G [00:15<00:33, 71.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.57G/4.92G [00:15<00:32, 71.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.58G/4.92G [00:15<00:31, 75.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██  | 2.60G/4.92G [00:15<00:23, 97.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▋  | 2.63G/4.92G [00:16<00:17, 129MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▋  | 2.65G/4.92G [00:16<00:15, 145MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▋  | 2.67G/4.92G [00:16<00:14, 153MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▋  | 2.69G/4.92G [00:16<00:13, 165MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▊  | 2.72G/4.92G [00:16<00:20, 110MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▊  | 2.74G/4.92G [00:16<00:17, 127MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▊  | 2.76G/4.92G [00:16<00:14, 144MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▊  | 2.78G/4.92G [00:17<00:14, 152MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▊  | 2.81G/4.92G [00:17<00:12, 171MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▉  | 2.83G/4.92G [00:17<00:12, 169MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▉  | 2.85G/4.92G [00:17<00:14, 142MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▉  | 2.87G/4.92G [00:17<00:14, 138MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▉  | 2.89G/4.92G [00:17<00:15, 133MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▉  | 2.92G/4.92G [00:18<00:15, 132MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▉  | 2.94G/4.92G [00:18<00:15, 127MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|███  | 2.96G/4.92G [00:18<00:14, 132MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|███  | 2.98G/4.92G [00:18<00:15, 129MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|███  | 3.00G/4.92G [00:18<00:14, 134MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|███  | 3.02G/4.92G [00:18<00:14, 135MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|███  | 3.04G/4.92G [00:18<00:13, 137MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|███  | 3.06G/4.92G [00:19<00:14, 131MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|███▏ | 3.08G/4.92G [00:19<00:13, 138MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|███▏ | 3.10G/4.92G [00:19<00:12, 142MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|███▏ | 3.12G/4.92G [00:19<00:12, 143MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|███▏ | 3.15G/4.92G [00:19<00:12, 145MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|███▏ | 3.17G/4.92G [00:19<00:11, 146MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|███▏ | 3.19G/4.92G [00:19<00:11, 145MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|███▎ | 3.21G/4.92G [00:20<00:11, 149MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|███▎ | 3.23G/4.92G [00:20<00:11, 149MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|███▎ | 3.25G/4.92G [00:20<00:11, 151MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|███▎ | 3.27G/4.92G [00:20<00:10, 153MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|███▎ | 3.29G/4.92G [00:20<00:10, 154MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|███▎ | 3.31G/4.92G [00:20<00:11, 138MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|███▍ | 3.33G/4.92G [00:20<00:11, 140MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|███▍ | 3.36G/4.92G [00:21<00:10, 146MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|███▍ | 3.38G/4.92G [00:21<00:10, 147MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|███▍ | 3.40G/4.92G [00:21<00:10, 148MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|███▍ | 3.42G/4.92G [00:21<00:10, 148MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|███▍ | 3.44G/4.92G [00:21<00:09, 152MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|███▌ | 3.46G/4.92G [00:21<00:09, 154MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|███▌ | 3.48G/4.92G [00:21<00:10, 143MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|███▌ | 3.50G/4.92G [00:22<00:09, 146MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|███▌ | 3.52G/4.92G [00:22<00:09, 149MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|███▌ | 3.54G/4.92G [00:22<00:08, 155MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|███▋ | 3.57G/4.92G [00:22<00:08, 157MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|███▋ | 3.59G/4.92G [00:22<00:08, 157MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|███▋ | 3.61G/4.92G [00:22<00:08, 159MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|███▋ | 3.63G/4.92G [00:22<00:08, 157MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|███▋ | 3.65G/4.92G [00:23<00:07, 160MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|███▋ | 3.67G/4.92G [00:23<00:07, 161MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|███▊ | 3.69G/4.92G [00:23<00:07, 160MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███▊ | 3.71G/4.92G [00:23<00:07, 163MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███▊ | 3.73G/4.92G [00:23<00:07, 163MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███▊ | 3.75G/4.92G [00:23<00:07, 164MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███▊ | 3.77G/4.92G [00:23<00:06, 165MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███▊ | 3.80G/4.92G [00:23<00:07, 158MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███▉ | 3.82G/4.92G [00:24<00:07, 156MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███▉ | 3.84G/4.92G [00:24<00:06, 162MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███▉ | 3.86G/4.92G [00:24<00:06, 156MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▉ | 3.88G/4.92G [00:24<00:06, 156MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▉ | 3.90G/4.92G [00:24<00:06, 162MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▉ | 3.92G/4.92G [00:24<00:06, 164MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|████ | 3.94G/4.92G [00:24<00:05, 164MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|████ | 3.96G/4.92G [00:24<00:05, 165MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|████ | 3.98G/4.92G [00:25<00:05, 167MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|████ | 4.01G/4.92G [00:25<00:05, 165MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|████ | 4.03G/4.92G [00:25<00:05, 165MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|████ | 4.05G/4.92G [00:25<00:05, 168MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|████▏| 4.07G/4.92G [00:25<00:05, 168MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|████▏| 4.09G/4.92G [00:25<00:04, 169MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|████▏| 4.11G/4.92G [00:25<00:06, 123MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|████▏| 4.13G/4.92G [00:26<00:07, 105MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▍| 4.15G/4.92G [00:26<00:07, 99.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|████▏| 4.17G/4.92G [00:26<00:07, 103MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.19G/4.92G [00:26<00:07, 91.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.20G/4.92G [00:27<00:08, 82.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.22G/4.92G [00:27<00:09, 71.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.23G/4.92G [00:27<00:09, 71.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.24G/4.92G [00:27<00:09, 74.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.25G/4.92G [00:27<00:09, 72.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.26G/4.92G [00:28<00:10, 65.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.27G/4.92G [00:28<00:11, 57.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.29G/4.92G [00:28<00:08, 70.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.30G/4.92G [00:28<00:08, 70.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.31G/4.92G [00:28<00:08, 70.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.32G/4.92G [00:28<00:07, 76.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.33G/4.92G [00:29<00:08, 67.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.35G/4.92G [00:29<00:07, 75.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.36G/4.92G [00:29<00:06, 79.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.37G/4.92G [00:29<00:07, 74.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.38G/4.92G [00:29<00:06, 77.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.39G/4.92G [00:29<00:07, 71.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.41G/4.92G [00:30<00:05, 89.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|████▌| 4.44G/4.92G [00:30<00:04, 109MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|████▌| 4.46G/4.92G [00:30<00:03, 124MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|████▌| 4.48G/4.92G [00:30<00:03, 137MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|████▌| 4.50G/4.92G [00:30<00:02, 146MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|████▌| 4.52G/4.92G [00:30<00:02, 152MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|████▌| 4.54G/4.92G [00:30<00:02, 153MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|████▋| 4.56G/4.92G [00:30<00:02, 134MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|████▋| 4.58G/4.92G [00:31<00:02, 139MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|████▋| 4.60G/4.92G [00:31<00:03, 100MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|████▋| 4.63G/4.92G [00:31<00:02, 128MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|████▋| 4.66G/4.92G [00:31<00:01, 140MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|████▊| 4.68G/4.92G [00:31<00:01, 147MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|████▊| 4.70G/4.92G [00:31<00:01, 152MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|████▊| 4.72G/4.92G [00:32<00:01, 156MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|████▊| 4.74G/4.92G [00:32<00:01, 160MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|████▊| 4.76G/4.92G [00:32<00:00, 162MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|████▊| 4.78G/4.92G [00:32<00:00, 165MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|████▉| 4.80G/4.92G [00:32<00:00, 164MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|████▉| 4.82G/4.92G [00:32<00:00, 158MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|████▉| 4.84G/4.92G [00:32<00:00, 160MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|████▉| 4.87G/4.92G [00:32<00:00, 164MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|████▉| 4.89G/4.92G [00:33<00:00, 159MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|█████| 4.92G/4.92G [00:33<00:00, 148MB/s]\u001b[A\n",
      "Downloading shards:  75%|██████████████████▊      | 3/4 [01:35<00:32, 32.56s/it]\n",
      "model-00004-of-00004.safetensors:   0%|             | 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   1%|     | 10.5M/1.17G [00:00<00:11, 101MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   3%|▏    | 31.5M/1.17G [00:00<00:07, 151MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   4%|▏    | 52.4M/1.17G [00:00<00:07, 148MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   6%|▎    | 73.4M/1.17G [00:00<00:06, 169MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   8%|▍    | 94.4M/1.17G [00:00<00:06, 157MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  10%|▌     | 115M/1.17G [00:00<00:06, 154MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  13%|▊     | 147M/1.17G [00:00<00:05, 173MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  14%|▊     | 168M/1.17G [00:01<00:05, 172MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  16%|▉     | 189M/1.17G [00:01<00:05, 176MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  18%|█     | 210M/1.17G [00:01<00:05, 182MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  21%|█▏    | 241M/1.17G [00:01<00:04, 196MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  23%|█▍    | 273M/1.17G [00:01<00:04, 205MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  26%|█▌    | 304M/1.17G [00:01<00:04, 210MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  29%|█▋    | 336M/1.17G [00:01<00:03, 212MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  31%|█▉    | 367M/1.17G [00:01<00:03, 210MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  34%|██    | 398M/1.17G [00:02<00:03, 212MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  37%|██▏   | 430M/1.17G [00:02<00:03, 212MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  39%|██▎   | 461M/1.17G [00:02<00:03, 205MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  42%|██▌   | 493M/1.17G [00:02<00:03, 209MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  45%|██▋   | 524M/1.17G [00:02<00:03, 206MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  48%|██▊   | 556M/1.17G [00:02<00:02, 210MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  50%|███   | 587M/1.17G [00:03<00:02, 214MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  53%|███▏  | 619M/1.17G [00:03<00:02, 217MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  56%|███▎  | 650M/1.17G [00:03<00:02, 218MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  58%|███▌  | 682M/1.17G [00:03<00:02, 208MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  61%|███▋  | 713M/1.17G [00:03<00:02, 213MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  64%|███▊  | 744M/1.17G [00:03<00:01, 216MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  66%|███▉  | 776M/1.17G [00:03<00:01, 218MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  69%|████▏ | 807M/1.17G [00:04<00:01, 220MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  72%|████▎ | 839M/1.17G [00:04<00:01, 220MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  75%|████▍ | 870M/1.17G [00:04<00:01, 220MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  77%|████▋ | 902M/1.17G [00:04<00:01, 219MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  80%|████▊ | 933M/1.17G [00:04<00:01, 221MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  83%|████▉ | 965M/1.17G [00:04<00:00, 222MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  85%|█████ | 996M/1.17G [00:04<00:00, 212MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  88%|████▍| 1.03G/1.17G [00:05<00:00, 216MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  91%|████▌| 1.06G/1.17G [00:05<00:00, 216MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  93%|████▋| 1.09G/1.17G [00:05<00:00, 218MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  96%|████▊| 1.12G/1.17G [00:05<00:00, 219MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|█████| 1.17G/1.17G [00:05<00:00, 206MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [01:47<00:00, 26.81s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [01:41<00:00, 25.43s/it]\n",
      "[INFO|modeling_utils.py:1633] 2025-03-20 00:40:01,841 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1140] 2025-03-20 00:40:01,846 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"pad_token_id\": 128255\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.20it/s]\n",
      "[INFO|modeling_utils.py:4970] 2025-03-20 00:40:05,258 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4978] 2025-03-20 00:40:05,258 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at allganize/Llama-3-Alpha-Ko-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "generation_config.json: 100%|██████████████████| 121/121 [00:00<00:00, 2.01MB/s]\n",
      "[INFO|configuration_utils.py:1095] 2025-03-20 00:40:05,484 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-03-20 00:40:05,484 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "[INFO|2025-03-20 00:40:05] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-03-20 00:40:05] llamafactory.model.model_utils.attention:157 >> Using vanilla attention implementation.\n",
      "[INFO|2025-03-20 00:40:05] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-03-20 00:40:05] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-03-20 00:40:05] llamafactory.model.model_utils.misc:157 >> Found linear modules: q_proj,down_proj,v_proj,gate_proj,up_proj,k_proj,o_proj\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.04it/s]\n",
      "[INFO|2025-03-20 00:40:05] llamafactory.model.loader:157 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "[INFO|trainer.py:746] 2025-03-20 00:40:05,907 >> Using auto half precision backend\n",
      "[WARNING|trainer.py:781] 2025-03-20 00:40:05,907 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[INFO|trainer.py:2405] 2025-03-20 00:40:06,583 >> ***** Running training *****\n",
      "[INFO|trainer.py:2406] 2025-03-20 00:40:06,583 >>   Num examples = 450\n",
      "[INFO|trainer.py:2407] 2025-03-20 00:40:06,583 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:2408] 2025-03-20 00:40:06,583 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2411] 2025-03-20 00:40:06,583 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2412] 2025-03-20 00:40:06,583 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2413] 2025-03-20 00:40:06,583 >>   Total optimization steps = 1,120\n",
      "[INFO|trainer.py:2414] 2025-03-20 00:40:06,587 >>   Number of trainable parameters = 20,971,520\n",
      "{'loss': 0.0707, 'grad_norm': 0.24357804656028748, 'learning_rate': 8.92857142857143e-06, 'epoch': 0.18}\n",
      "{'loss': 0.0636, 'grad_norm': 0.35543641448020935, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0202, 'grad_norm': 0.13085633516311646, 'learning_rate': 2.6785714285714288e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0118, 'grad_norm': 0.17206336557865143, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0087, 'grad_norm': 0.0813165158033371, 'learning_rate': 4.464285714285715e-05, 'epoch': 0.89}\n",
      "{'loss': 0.006, 'grad_norm': 0.09643755853176117, 'learning_rate': 5.3571428571428575e-05, 'epoch': 1.05}\n",
      "{'loss': 0.006, 'grad_norm': 0.15116727352142334, 'learning_rate': 6.25e-05, 'epoch': 1.23}\n",
      "{'loss': 0.007, 'grad_norm': 0.14849591255187988, 'learning_rate': 7.142857142857143e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0056, 'grad_norm': 0.11302405595779419, 'learning_rate': 8.035714285714287e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0048, 'grad_norm': 0.08010058850049973, 'learning_rate': 8.92857142857143e-05, 'epoch': 1.76}\n",
      "  9%|███▌                                    | 100/1120 [03:38<36:05,  2.12s/it][INFO|trainer.py:4258] 2025-03-20 00:43:44,677 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 00:43:44,677 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 00:43:44,677 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.72it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.78it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.60it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.76it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.36it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.46it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.64it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.82it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.87it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.91it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.87it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.92it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.70it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.66it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.63it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.91it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.97it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.83it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.88it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.004894661251455545, 'eval_runtime': 4.3518, 'eval_samples_per_second': 11.49, 'eval_steps_per_second': 5.745, 'epoch': 1.76}\n",
      "  9%|███▌                                    | 100/1120 [03:42<36:05,  2.12s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.76it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 00:43:49,028 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-100\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 00:43:49,208 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 00:43:49,210 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 00:43:49,597 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 00:43:49,600 >> Special tokens file saved in de-identification/checkpoint/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 0.0044, 'grad_norm': 0.04428166151046753, 'learning_rate': 9.821428571428572e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0036, 'grad_norm': 0.09349347651004791, 'learning_rate': 9.998445910004082e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0028, 'grad_norm': 0.06071062386035919, 'learning_rate': 9.992134075089084e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0032, 'grad_norm': 0.04532066732645035, 'learning_rate': 9.980973490458728e-05, 'epoch': 2.46}\n",
      "{'loss': 0.003, 'grad_norm': 0.06208094581961632, 'learning_rate': 9.964974996142698e-05, 'epoch': 2.64}\n",
      "{'loss': 0.0033, 'grad_norm': 0.05703926458954811, 'learning_rate': 9.944154131125642e-05, 'epoch': 2.82}\n",
      "{'loss': 0.0037, 'grad_norm': 0.0421142540872097, 'learning_rate': 9.918531118254507e-05, 'epoch': 3.0}\n",
      "{'loss': 0.0031, 'grad_norm': 0.06011830270290375, 'learning_rate': 9.888130844596524e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0024, 'grad_norm': 0.06173485144972801, 'learning_rate': 9.852982837266955e-05, 'epoch': 3.34}\n",
      "{'loss': 0.0019, 'grad_norm': 0.0596347339451313, 'learning_rate': 9.81312123475006e-05, 'epoch': 3.52}\n",
      " 18%|███████▏                                | 200/1120 [07:20<33:33,  2.19s/it][INFO|trainer.py:4258] 2025-03-20 00:47:27,527 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 00:47:27,527 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 00:47:27,527 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.67it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.78it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.60it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.75it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.35it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.45it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.63it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.81it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.87it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.90it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.90it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.79it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.85it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.90it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.69it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.66it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.63it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.91it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.005168447270989418, 'eval_runtime': 4.3494, 'eval_samples_per_second': 11.496, 'eval_steps_per_second': 5.748, 'epoch': 3.52}\n",
      " 18%|███████▏                                | 200/1120 [07:25<33:33,  2.19s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.75it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 00:47:31,877 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-200\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 919ab7ea-96a0-4745-b1a2-14063086f82e)') - silently ignoring the lookup for the file config.json in allganize/Llama-3-Alpha-Ko-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in allganize/Llama-3-Alpha-Ko-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 00:47:42,278 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 00:47:42,280 >> Special tokens file saved in de-identification/checkpoint/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.0017, 'grad_norm': 0.025282155722379684, 'learning_rate': 9.768584753741134e-05, 'epoch': 3.69}\n",
      "{'loss': 0.0014, 'grad_norm': 0.08941849321126938, 'learning_rate': 9.719416651541839e-05, 'epoch': 3.87}\n",
      "{'loss': 0.0026, 'grad_norm': 0.04998939484357834, 'learning_rate': 9.665664684045333e-05, 'epoch': 4.04}\n",
      "{'loss': 0.0028, 'grad_norm': 0.07282345741987228, 'learning_rate': 9.607381059352038e-05, 'epoch': 4.21}\n",
      "{'loss': 0.0017, 'grad_norm': 0.037698451429605484, 'learning_rate': 9.544622387061055e-05, 'epoch': 4.39}\n",
      "{'loss': 0.0015, 'grad_norm': 0.18410754203796387, 'learning_rate': 9.477449623286505e-05, 'epoch': 4.57}\n",
      "{'loss': 0.0012, 'grad_norm': 0.08122694492340088, 'learning_rate': 9.405928011452211e-05, 'epoch': 4.75}\n",
      "{'loss': 0.0013, 'grad_norm': 0.026883261278271675, 'learning_rate': 9.330127018922194e-05, 'epoch': 4.92}\n",
      "{'loss': 0.0015, 'grad_norm': 0.05659131705760956, 'learning_rate': 9.250120269528546e-05, 'epoch': 5.09}\n",
      "{'loss': 0.0016, 'grad_norm': 0.12232646346092224, 'learning_rate': 9.165985472062246e-05, 'epoch': 5.27}\n",
      " 27%|██████████▋                             | 300/1120 [11:14<30:53,  2.26s/it][INFO|trainer.py:4258] 2025-03-20 00:51:21,310 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 00:51:21,310 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 00:51:21,310 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.73it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.78it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.59it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.75it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.36it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.45it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.64it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.82it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.87it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.91it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.85it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.91it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.69it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.65it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.62it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.89it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.88it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0073740845546126366, 'eval_runtime': 4.3484, 'eval_samples_per_second': 11.499, 'eval_steps_per_second': 5.749, 'epoch': 5.27}\n",
      " 27%|██████████▋                             | 300/1120 [11:19<30:53,  2.26s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.75it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 00:51:25,659 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-300\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 00:51:25,916 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 00:51:25,918 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 00:51:26,421 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 00:51:26,425 >> Special tokens file saved in de-identification/checkpoint/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.0015, 'grad_norm': 0.09524253010749817, 'learning_rate': 9.077804344796302e-05, 'epoch': 5.44}\n",
      "{'loss': 0.0009, 'grad_norm': 0.06760397553443909, 'learning_rate': 8.985662536114613e-05, 'epoch': 5.62}\n",
      "{'loss': 0.0012, 'grad_norm': 0.03545444831252098, 'learning_rate': 8.889649541323574e-05, 'epoch': 5.8}\n",
      "{'loss': 0.0013, 'grad_norm': 0.02500808984041214, 'learning_rate': 8.789858615727265e-05, 'epoch': 5.98}\n",
      "{'loss': 0.0007, 'grad_norm': 0.01982911117374897, 'learning_rate': 8.68638668405062e-05, 'epoch': 6.14}\n",
      "{'loss': 0.001, 'grad_norm': 0.04236343130469322, 'learning_rate': 8.579334246298593e-05, 'epoch': 6.32}\n",
      "{'loss': 0.0005, 'grad_norm': 0.017217418178915977, 'learning_rate': 8.468805280142709e-05, 'epoch': 6.5}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0054360926151275635, 'learning_rate': 8.354907139929851e-05, 'epoch': 6.68}\n",
      "{'loss': 0.0006, 'grad_norm': 0.15927860140800476, 'learning_rate': 8.237750452411353e-05, 'epoch': 6.85}\n",
      "{'loss': 0.0005, 'grad_norm': 0.00402940483763814, 'learning_rate': 8.117449009293668e-05, 'epoch': 7.02}\n",
      " 36%|██████████████▎                         | 400/1120 [14:57<21:42,  1.81s/it][INFO|trainer.py:4258] 2025-03-20 00:55:03,708 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 00:55:03,708 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 00:55:03,708 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.71it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.78it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.60it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.76it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.36it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.45it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.64it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.82it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.87it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.90it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.86it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.91it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.69it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.66it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.63it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.91it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.006152356043457985, 'eval_runtime': 4.3544, 'eval_samples_per_second': 11.483, 'eval_steps_per_second': 5.741, 'epoch': 7.02}\n",
      " 36%|██████████████▎                         | 400/1120 [15:01<21:42,  1.81s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.75it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 00:55:08,063 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-400\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 00:55:08,350 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 00:55:08,351 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 00:55:08,734 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 00:55:08,738 >> Special tokens file saved in de-identification/checkpoint/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.0003, 'grad_norm': 0.004758661147207022, 'learning_rate': 7.994119656715002e-05, 'epoch': 7.2}\n",
      "{'loss': 0.0004, 'grad_norm': 0.03593524917960167, 'learning_rate': 7.86788218175523e-05, 'epoch': 7.37}\n",
      "{'loss': 0.0002, 'grad_norm': 0.02716290019452572, 'learning_rate': 7.738859196089358e-05, 'epoch': 7.55}\n",
      "{'loss': 0.0006, 'grad_norm': 0.020106887444853783, 'learning_rate': 7.60717601689749e-05, 'epoch': 7.73}\n",
      "{'loss': 0.001, 'grad_norm': 0.028348904103040695, 'learning_rate': 7.472960545147038e-05, 'epoch': 7.91}\n",
      "{'loss': 0.0004, 'grad_norm': 0.006367434281855822, 'learning_rate': 7.33634314136531e-05, 'epoch': 8.07}\n",
      "{'loss': 0.0004, 'grad_norm': 0.004981459584087133, 'learning_rate': 7.197456499023225e-05, 'epoch': 8.25}\n",
      "{'loss': 0.0003, 'grad_norm': 0.009038057178258896, 'learning_rate': 7.056435515653059e-05, 'epoch': 8.43}\n",
      "{'loss': 0.0002, 'grad_norm': 0.005900462623685598, 'learning_rate': 6.91341716182545e-05, 'epoch': 8.6}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0012615586165338755, 'learning_rate': 6.768540348112907e-05, 'epoch': 8.78}\n",
      " 45%|█████████████████▊                      | 500/1120 [18:41<23:27,  2.27s/it][INFO|trainer.py:4258] 2025-03-20 00:58:47,640 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 00:58:47,640 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 00:58:47,640 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.67it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.73it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.55it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.71it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.33it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.41it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.62it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.80it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.86it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.89it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.89it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.79it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.85it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.90it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.68it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.65it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.62it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.90it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.008071374148130417, 'eval_runtime': 4.3576, 'eval_samples_per_second': 11.474, 'eval_steps_per_second': 5.737, 'epoch': 8.78}\n",
      " 45%|█████████████████▊                      | 500/1120 [18:45<23:27,  2.27s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.75it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 00:58:52,000 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-500\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 00:58:52,185 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 00:58:52,186 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 00:58:52,661 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 00:58:52,665 >> Special tokens file saved in de-identification/checkpoint/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.0002, 'grad_norm': 0.006905020214617252, 'learning_rate': 6.621945790169036e-05, 'epoch': 8.96}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0005304886144585907, 'learning_rate': 6.473775872054521e-05, 'epoch': 9.12}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0006629870622418821, 'learning_rate': 6.324174507942637e-05, 'epoch': 9.3}\n",
      "{'loss': 0.0002, 'grad_norm': 0.005160952918231487, 'learning_rate': 6.173287002338577e-05, 'epoch': 9.48}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0012376838130876422, 'learning_rate': 6.021259908948402e-05, 'epoch': 9.66}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0021066709887236357, 'learning_rate': 5.868240888334653e-05, 'epoch': 9.84}\n",
      "{'loss': 0.0002, 'grad_norm': 0.002558816922828555, 'learning_rate': 5.714378564496901e-05, 'epoch': 10.0}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0037699600216001272, 'learning_rate': 5.559822380516539e-05, 'epoch': 10.18}\n",
      "{'loss': 0.0001, 'grad_norm': 0.002849855227395892, 'learning_rate': 5.404722453406017e-05, 'epoch': 10.36}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0017650047084316611, 'learning_rate': 5.249229428303486e-05, 'epoch': 10.53}\n",
      " 54%|█████████████████████▍                  | 600/1120 [22:24<18:53,  2.18s/it][INFO|trainer.py:4258] 2025-03-20 01:02:30,862 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 01:02:30,862 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 01:02:30,862 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.74it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.79it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.60it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.75it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.36it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.45it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.64it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.82it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.87it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.91it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.85it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.90it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.69it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.66it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.62it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.91it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.97it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.83it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.88it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.010278438217937946, 'eval_runtime': 4.3456, 'eval_samples_per_second': 11.506, 'eval_steps_per_second': 5.753, 'epoch': 10.53}\n",
      " 54%|█████████████████████▍                  | 600/1120 [22:28<18:53,  2.18s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.76it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 01:02:35,208 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-600\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 01:02:35,395 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 01:02:35,396 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 01:02:35,785 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 01:02:35,788 >> Special tokens file saved in de-identification/checkpoint/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 0.0001, 'grad_norm': 0.000844712951220572, 'learning_rate': 5.0934943321545115e-05, 'epoch': 10.71}\n",
      "{'loss': 0.0002, 'grad_norm': 0.00222709565423429, 'learning_rate': 4.9376684270229254e-05, 'epoch': 10.89}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0007279654964804649, 'learning_rate': 4.781903063173321e-05, 'epoch': 11.05}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0007752863457426429, 'learning_rate': 4.626349532067879e-05, 'epoch': 11.23}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0008137731929309666, 'learning_rate': 4.471158919420312e-05, 'epoch': 11.41}\n",
      "{'loss': 0.0001, 'grad_norm': 0.000559603504370898, 'learning_rate': 4.316481958449634e-05, 'epoch': 11.59}\n",
      "{'loss': 0.0001, 'grad_norm': 0.021664686501026154, 'learning_rate': 4.162468883476319e-05, 'epoch': 11.76}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0016407676739618182, 'learning_rate': 4.0092692840030134e-05, 'epoch': 11.94}\n",
      "{'loss': 0.0001, 'grad_norm': 0.017734341323375702, 'learning_rate': 3.857031959421553e-05, 'epoch': 12.11}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00038741427124477923, 'learning_rate': 3.705904774487396e-05, 'epoch': 12.28}\n",
      " 62%|█████████████████████████               | 700/1120 [26:06<15:15,  2.18s/it][INFO|trainer.py:4258] 2025-03-20 01:06:13,029 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 01:06:13,029 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 01:06:13,029 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.73it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.79it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.60it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.75it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.36it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.44it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.63it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.81it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.86it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.88it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.89it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.86it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.90it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.68it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.65it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.62it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.91it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.95it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.010861375369131565, 'eval_runtime': 4.3602, 'eval_samples_per_second': 11.467, 'eval_steps_per_second': 5.734, 'epoch': 12.28}\n",
      " 62%|█████████████████████████               | 700/1120 [26:10<15:15,  2.18s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.75it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 01:06:17,388 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-700\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 01:06:17,571 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 01:06:17,572 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 01:06:17,942 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 01:06:17,946 >> Special tokens file saved in de-identification/checkpoint/checkpoint-700/special_tokens_map.json\n",
      "{'loss': 0.0001, 'grad_norm': 0.0009971784893423319, 'learning_rate': 3.556034515701852e-05, 'epoch': 12.46}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00519386725500226, 'learning_rate': 3.4075667487415785e-05, 'epoch': 12.64}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0007074885070323944, 'learning_rate': 3.2606456770738636e-05, 'epoch': 12.82}\n",
      "{'loss': 0.0001, 'grad_norm': 0.002316838363185525, 'learning_rate': 3.115414001894974e-05, 'epoch': 13.0}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006299330852925777, 'learning_rate': 2.9720127835276256e-05, 'epoch': 13.16}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00030699855415150523, 'learning_rate': 2.8305813044122097e-05, 'epoch': 13.34}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006799550610594451, 'learning_rate': 2.6912569338248315e-05, 'epoch': 13.52}\n",
      "{'loss': 0.0001, 'grad_norm': 0.015648122876882553, 'learning_rate': 2.5541749944535554e-05, 'epoch': 13.69}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001482878578826785, 'learning_rate': 2.4194686309624663e-05, 'epoch': 13.87}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0019092317670583725, 'learning_rate': 2.2872686806712035e-05, 'epoch': 14.04}\n",
      " 71%|████████████████████████████▌           | 800/1120 [29:48<10:23,  1.95s/it][INFO|trainer.py:4258] 2025-03-20 01:09:55,451 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 01:09:55,451 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 01:09:55,451 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.73it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.79it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.60it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.76it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.36it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.46it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.64it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.82it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.87it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.91it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.90it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.86it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.91it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.69it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.66it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.63it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.91it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.83it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.88it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.01152093056589365, 'eval_runtime': 4.353, 'eval_samples_per_second': 11.486, 'eval_steps_per_second': 5.743, 'epoch': 14.04}\n",
      " 71%|████████████████████████████▌           | 800/1120 [29:53<10:23,  1.95s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.76it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 01:09:59,805 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-800\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 01:10:00,035 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 01:10:00,037 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 01:10:00,373 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 01:10:00,375 >> Special tokens file saved in de-identification/checkpoint/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0, 'grad_norm': 0.0006247717537917197, 'learning_rate': 2.157703546475539e-05, 'epoch': 14.21}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0031449163798242807, 'learning_rate': 2.0308990721324927e-05, 'epoch': 14.39}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004198220558464527, 'learning_rate': 1.906978420031059e-05, 'epoch': 14.57}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005823877872899175, 'learning_rate': 1.7860619515673033e-05, 'epoch': 14.75}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006067781359888613, 'learning_rate': 1.6682671102399805e-05, 'epoch': 14.92}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004874294681940228, 'learning_rate': 1.553708307580265e-05, 'epoch': 15.09}\n",
      "{'loss': 0.0001, 'grad_norm': 0.005319693591445684, 'learning_rate': 1.4424968120263504e-05, 'epoch': 15.27}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0003825488965958357, 'learning_rate': 1.3347406408508695e-05, 'epoch': 15.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.000293243327178061, 'learning_rate': 1.230544455246101e-05, 'epoch': 15.62}\n",
      "{'loss': 0.0, 'grad_norm': 0.00038774291169829667, 'learning_rate': 1.130009458668863e-05, 'epoch': 15.8}\n",
      " 80%|████████████████████████████████▏       | 900/1120 [33:33<08:04,  2.20s/it][INFO|trainer.py:4258] 2025-03-20 01:13:40,519 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 01:13:40,519 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 01:13:40,519 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.73it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.79it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.59it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.76it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.36it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.45it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.62it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.81it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.86it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.90it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.90it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.79it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.85it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.90it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.68it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.65it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.62it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.89it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.94it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.94it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.81it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.01196268480271101, 'eval_runtime': 4.3527, 'eval_samples_per_second': 11.487, 'eval_steps_per_second': 5.744, 'epoch': 15.8}\n",
      " 80%|████████████████████████████████▏       | 900/1120 [33:38<08:04,  2.20s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.74it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 01:13:44,872 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-900\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 01:13:45,052 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 01:13:45,054 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 01:13:45,391 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 01:13:45,394 >> Special tokens file saved in de-identification/checkpoint/checkpoint-900/special_tokens_map.json\n",
      "{'loss': 0.0, 'grad_norm': 0.0004358863807283342, 'learning_rate': 1.0332332985438248e-05, 'epoch': 15.98}\n",
      "{'loss': 0.0, 'grad_norm': 0.00055363227147609, 'learning_rate': 9.403099714207175e-06, 'epoch': 16.14}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0008080883999355137, 'learning_rate': 8.513297316775625e-06, 'epoch': 16.32}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003641496296040714, 'learning_rate': 7.663790038585793e-06, 'epoch': 16.5}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00030465732561424375, 'learning_rate': 6.855402987319348e-06, 'epoch': 16.68}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003318952221889049, 'learning_rate': 6.088921331488568e-06, 'epoch': 16.85}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006074660923331976, 'learning_rate': 5.365089537819434e-06, 'epoch': 17.02}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0003980757319368422, 'learning_rate': 4.684610648167503e-06, 'epoch': 17.2}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006677820347249508, 'learning_rate': 4.048145596668967e-06, 'epoch': 17.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003648579877335578, 'learning_rate': 3.4563125677897932e-06, 'epoch': 17.55}\n",
      " 89%|██████████████████████████████████▊    | 1000/1120 [37:16<04:43,  2.36s/it][INFO|trainer.py:4258] 2025-03-20 01:17:23,594 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 01:17:23,594 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 01:17:23,594 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.74it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.79it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.61it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.77it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.37it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.45it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.64it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.82it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.88it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.90it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.87it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.92it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.70it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.67it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.63it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.92it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.98it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.83it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.88it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.012192649766802788, 'eval_runtime': 4.3422, 'eval_samples_per_second': 11.515, 'eval_steps_per_second': 5.757, 'epoch': 17.55}\n",
      " 89%|██████████████████████████████████▊    | 1000/1120 [37:21<04:43,  2.36s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.76it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 01:17:27,937 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-1000\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 01:17:28,168 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 01:17:28,169 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 01:17:28,632 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 01:17:28,639 >> Special tokens file saved in de-identification/checkpoint/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004894732846878469, 'learning_rate': 2.9096863958968268e-06, 'epoch': 17.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014337317552417517, 'learning_rate': 2.408798006933882e-06, 'epoch': 17.91}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005148624186404049, 'learning_rate': 1.9541339027450256e-06, 'epoch': 18.07}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005337645416148007, 'learning_rate': 1.5461356885461075e-06, 'epoch': 18.25}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0003562851925380528, 'learning_rate': 1.1851996440033319e-06, 'epoch': 18.43}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014204744948074222, 'learning_rate': 8.716763383355864e-07, 'epoch': 18.6}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004686762986239046, 'learning_rate': 6.058702898142643e-07, 'epoch': 18.78}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0038351716939359903, 'learning_rate': 3.8803966999139684e-07, 'epoch': 18.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006194544257596135, 'learning_rate': 2.1839605294330933e-07, 'epoch': 19.12}\n",
      "{'loss': 0.0, 'grad_norm': 0.0011689765378832817, 'learning_rate': 9.710420977340762e-08, 'epoch': 19.3}\n",
      " 98%|██████████████████████████████████████▎| 1100/1120 [40:59<00:43,  2.19s/it][INFO|trainer.py:4258] 2025-03-20 01:21:05,707 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 01:21:05,707 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 01:21:05,708 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:02, 10.71it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:03,  6.77it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:03,  6.58it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.74it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:02,  6.34it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  6.44it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  5.63it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  5.81it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:01<00:02,  5.87it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:02,  5.90it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:02<00:02,  5.90it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  5.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:02<00:01,  5.86it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  5.91it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  5.69it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  5.65it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:03<00:01,  5.62it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:03<00:00,  5.90it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.01207562442868948, 'eval_runtime': 4.3565, 'eval_samples_per_second': 11.477, 'eval_steps_per_second': 5.739, 'epoch': 19.3}\n",
      " 98%|██████████████████████████████████████▎| 1100/1120 [41:03<00:43,  2.19s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  5.75it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3942] 2025-03-20 01:21:10,064 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-1100\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 01:21:10,246 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 01:21:10,247 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 01:21:10,925 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-1100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 01:21:10,930 >> Special tokens file saved in de-identification/checkpoint/checkpoint-1100/special_tokens_map.json\n",
      "{'loss': 0.0, 'grad_norm': 0.0012501769233494997, 'learning_rate': 2.4281948573617874e-08, 'epoch': 19.48}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0015719993971288204, 'learning_rate': 0.0, 'epoch': 19.66}\n",
      "100%|███████████████████████████████████████| 1120/1120 [41:50<00:00,  2.28s/it][INFO|trainer.py:3942] 2025-03-20 01:21:57,527 >> Saving model checkpoint to de-identification/checkpoint/checkpoint-1120\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 01:21:57,720 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 01:21:57,721 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 01:21:58,041 >> tokenizer config file saved in de-identification/checkpoint/checkpoint-1120/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 01:21:58,044 >> Special tokens file saved in de-identification/checkpoint/checkpoint-1120/special_tokens_map.json\n",
      "[INFO|trainer.py:2657] 2025-03-20 01:21:59,205 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2512.6184, 'train_samples_per_second': 3.582, 'train_steps_per_second': 0.446, 'train_loss': 0.002414846396225455, 'epoch': 19.66}\n",
      "100%|███████████████████████████████████████| 1120/1120 [41:52<00:00,  2.24s/it]\n",
      "[INFO|trainer.py:3942] 2025-03-20 01:21:59,212 >> Saving model checkpoint to de-identification/checkpoint\n",
      "[INFO|configuration_utils.py:699] 2025-03-20 01:21:59,409 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allganize--Llama-3-Alpha-Ko-8B-Instruct/snapshots/d294a56f7b3d1128178a75148f14a00964e961b1/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-03-20 01:21:59,410 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"allganize/Llama-3-Alpha-Ko-8B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"unsloth_version\": \"2024.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-20 01:21:59,747 >> tokenizer config file saved in de-identification/checkpoint/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-03-20 01:21:59,750 >> Special tokens file saved in de-identification/checkpoint/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     19.6578\n",
      "  total_flos               = 405015466GF\n",
      "  train_loss               =      0.0024\n",
      "  train_runtime            =  0:41:52.61\n",
      "  train_samples_per_second =       3.582\n",
      "  train_steps_per_second   =       0.446\n",
      "Figure saved at: de-identification/checkpoint/training_loss.png\n",
      "Figure saved at: de-identification/checkpoint/training_eval_loss.png\n",
      "[WARNING|2025-03-20 01:22:00] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
      "[INFO|trainer.py:4258] 2025-03-20 01:22:00,266 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-03-20 01:22:00,266 >>   Num examples = 50\n",
      "[INFO|trainer.py:4263] 2025-03-20 01:22:00,266 >>   Batch size = 1\n",
      "100%|███████████████████████████████████████████| 25/25 [00:04<00:00,  6.01it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =    19.6578\n",
      "  eval_loss               =      0.012\n",
      "  eval_runtime            = 0:00:04.37\n",
      "  eval_samples_per_second =     11.434\n",
      "  eval_steps_per_second   =      5.717\n",
      "[INFO|modelcard.py:449] 2025-03-20 01:22:04,655 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train examples/train_lora/llama3_lora_deidentify.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b898b8c-25e3-46c3-a528-6423300c1b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: allganize/Llama-3-Alpha-Ko-8B-Instruct\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:08<00:00,  2.12s/it]\n",
      "Loading PEFT: ./de-identification/checkpoint/checkpoint-1120\n",
      "Running merge_and_unload\n",
      "[2025-03-20 01:25:20,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Model saved to ./de-identification/model\n"
     ]
    }
   ],
   "source": [
    "!python merge.py \\\n",
    "    --base_model_name_or_path allganize/Llama-3-Alpha-Ko-8B-Instruct \\\n",
    "    --peft_model_path ./de-identification/checkpoint/checkpoint-1120 \\\n",
    "    --output_dir ./de-identification/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82999cbe-b59f-4b7f-a0a5-c0ff66a0df7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/LLaMA-Factory'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa07214-d5ff-4445-8279-9c4fc7c4535a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8919c-7d47-4f96-b4fe-fa85efd8d1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
