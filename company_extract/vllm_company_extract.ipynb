{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75139723-86d5-4b54-9b57-06231c4966c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vllm==0.6.0\n",
      "  Using cached vllm-0.6.0-cp38-abi3-manylinux1_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (5.9.6)\n",
      "Collecting sentencepiece (from vllm==0.6.0)\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (1.24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (2.31.0)\n",
      "Collecting tqdm (from vllm==0.6.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting py-cpuinfo (from vllm==0.6.0)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting transformers>=4.43.2 (from vllm==0.6.0)\n",
      "  Using cached transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting tokenizers>=0.19.1 (from vllm==0.6.0)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting protobuf (from vllm==0.6.0)\n",
      "  Using cached protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting fastapi (from vllm==0.6.0)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting aiohttp (from vllm==0.6.0)\n",
      "  Using cached aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting openai>=1.0 (from vllm==0.6.0)\n",
      "  Using cached openai-1.68.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn[standard] (from vllm==0.6.0)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic>=2.8 (from vllm==0.6.0)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (9.3.0)\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (0.18.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.0)\n",
      "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm==0.6.0)\n",
      "  Using cached tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer==0.10.6 (from vllm==0.6.0)\n",
      "  Using cached lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting outlines<0.1,>=0.0.43 (from vllm==0.6.0)\n",
      "  Using cached outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting typing-extensions>=4.10 (from vllm==0.6.0)\n",
      "  Using cached typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting filelock>=3.10.4 (from vllm==0.6.0)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting partial-json-parser (from vllm==0.6.0)\n",
      "  Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (24.0.1)\n",
      "Collecting msgspec (from vllm==0.6.0)\n",
      "  Using cached msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf==0.9.1 (from vllm==0.6.0)\n",
      "  Using cached gguf-0.9.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from vllm==0.6.0) (4.6.4)\n",
      "Collecting mistral-common>=1.3.4 (from vllm==0.6.0)\n",
      "  Using cached mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (6.0.1)\n",
      "Collecting ray>=2.9 (from vllm==0.6.0)\n",
      "  Using cached ray-2.44.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting nvidia-ml-py (from vllm==0.6.0)\n",
      "  Using cached nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting torch==2.4.0 (from vllm==0.6.0)\n",
      "  Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision==0.19 (from vllm==0.6.0)\n",
      "  Using cached torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting xformers==0.0.27.post2 (from vllm==0.6.0)\n",
      "  Using cached xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting vllm-flash-attn==2.6.1 (from vllm==0.6.0)\n",
      "  Using cached vllm_flash_attn-2.6.1-cp310-cp310-manylinux1_x86_64.whl.metadata (476 bytes)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.10.6->vllm==0.6.0)\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.10.6->vllm==0.6.0) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm==0.6.0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting jsonschema>=4.21.1 (from mistral-common>=1.3.4->vllm==0.6.0)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting numpy<2.0.0 (from vllm==0.6.0)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pillow (from vllm==0.6.0)\n",
      "  Using cached pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0->vllm==0.6.0) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.0->vllm==0.6.0)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.0->vllm==0.6.0)\n",
      "  Using cached jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (1.3.0)\n",
      "Collecting lark (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (1.5.8)\n",
      "Collecting cloudpickle (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting numba (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Using cached numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (0.30.2)\n",
      "Collecting datasets (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Using cached datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Using cached pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting starlette<1.0.0,>=0.30.0 (from prometheus-fastapi-instrumentator>=7.0.0->vllm==0.6.0)\n",
      "  Using cached starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.8->vllm==0.6.0)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.8->vllm==0.6.0)\n",
      "  Using cached pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting click>=7.0 (from ray>=2.9->vllm==0.6.0)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.9->vllm==0.6.0)\n",
      "  Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vllm==0.6.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7d0778-2e56-4a23-8505-1bc8e9a6e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1857731c-855a-45cf-b340-bd7acd0f3659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cbc8bad7ba400c97f494fed4bba23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/927 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-27 04:13:26 arg_utils.py:862] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 03-27 04:13:26 config.py:999] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 03-27 04:13:26 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='irene93/Llama-3-Company-Extract', speculative_config=None, tokenizer='irene93/Llama-3-Company-Extract', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=irene93/Llama-3-Company-Extract, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4da25feebb4ec6b94fe9a93f17f3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035420cc6fff400695af3dd38b7425fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b59d1bc4d94780a9d13ec5c7126cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d197324df17d45ad92976d0d414a7e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-27 04:13:28 model_runner.py:915] Starting to load model irene93/Llama-3-Company-Extract...\n",
      "INFO 03-27 04:13:28 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f7d0feb6184222ad3b93e17e0de0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d8d8af63cb4f10a31cf25bb3292ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50caf081521c4d089a5b92c88a209279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabf99d35ca2498ba17724d1c47f39ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-27 04:15:31 model_runner.py:926] Loading model weights took 6.0160 GB\n",
      "INFO 03-27 04:15:31 gpu_executor.py:122] # GPU blocks: 37314, # CPU blocks: 2340\n",
      "INFO 03-27 04:15:36 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-27 04:15:36 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-27 04:15:48 model_runner.py:1335] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# LLM\n",
    "llm = LLM(model=\"irene93/Llama-3-Company-Extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f7b051-7ba8-4ed1-a718-3dab7d750143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 준비\n",
    "template = string.Template(\"\"\"<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "${system_content}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "\\n 뉴스데이터 :${user_content}<|eot_id|><|start_header_id|>assistant<|end_header_id|> \n",
    "\"\"\")\n",
    "\n",
    "system_prompt = '''1. 주어진 입력에 나와있는 회사명을 찾으세요 \n",
    "2. 회사명을 찾을수 없다면, 빈 파이썬 리스트를 출력하고, 임의로 답변하려고 시도하지 마세요. \n",
    "3. 출력은 반드시 파이썬의 리스트 형태로 생성하십시오.\n",
    "4. 뉴스데이터:이 주어지면 회사리스트: 다음에 파이썬 리스트 형태로 작성하세요.'''\n",
    "\n",
    "user_prompt = '''LG에너지솔루션, 배터리 관리 분야 새 얼굴 '비 어라운드' 만들었다 LG에너지솔루션의 배터리 관리 토털 설루션(BMTS) 브랜드 'B.around'(비.어라운드) 로고. LG에너지솔루션 제공  잇단 화재로 전기차 배터리에 불안과 공포(전기차 포비아)가 확산한 가운데 LG에너지솔루션(LG엔솔)이 배터리 관리 토털 설루션(BMTS) 브랜드를 만들고 사업 확장에 나선다. 전기차 배터리 생산을 넘어 소프트웨어 부문 강화로 수익원을 넓히려는 움직임으로 풀이된다.  LG엔솔은 25일 '항상 고객의 곁에'(Be around your side)란 뜻의 BMTS 브랜드 'B.around'(비.어라운드)를 공개했다. 어떤 상황에서든 배터리 상태를 실시간 모니터링하는 LG엔솔 BMTS의 기술 경쟁력을 상징한다는 설명이다.  비.어라운드 제품에는 기존 배터리 관리 시스템(BMS)에 클라우드와 인공지능(AI) 기술을 결합해 화재방지 등을 위한 안전 진단, 퇴화·수명 예측 등을 더 쉽게 할 계획이다. 배터리 불량 유형을 사전에 진단하고 퇴화 상태를 점검하는 소프트웨어를 통해 오랫동안 쓸 수 있게 할 방침이다.  이달훈 LG에너지솔루션 BMS개발센터장 상무는 \"오랜 기간 안전 진단 시스템을 위해 노력했고 이제는 배터리 관리 소프트웨어 사업으로 확장해 배터리의 건강한 사용을 위해 설루션을 제공하는 기업이 되려고 한다\"고 말했다.  김청환 기자 chk@hankookilbo.com'''\n",
    "\n",
    "# 메시지 준비\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# 프롬프트 생성\n",
    "prompt = template.safe_substitute({\n",
    "    \"system_content\": messages[0][\"content\"],\n",
    "    \"user_content\": messages[1][\"content\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed0aba1a-8229-4ba9-baad-299402bc37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt.strip() + '회사리스트:' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b25969-5266-4fd6-a85d-3991bc4e0327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "1. 주어진 입력에 나와있는 회사명을 찾으세요 \n",
      "2. 회사명을 찾을수 없다면, 빈 파이썬 리스트를 출력하고, 임의로 답변하려고 시도하지 마세요. \n",
      "3. 출력은 반드시 파이썬의 리스트 형태로 생성하십시오.\n",
      "4. 뉴스데이터:이 주어지면 회사리스트: 다음에 파이썬 리스트 형태로 작성하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "\n",
      " 뉴스데이터 :LG에너지솔루션, 배터리 관리 분야 새 얼굴 '비 어라운드' 만들었다 LG에너지솔루션의 배터리 관리 토털 설루션(BMTS) 브랜드 'B.around'(비.어라운드) 로고. LG에너지솔루션 제공  잇단 화재로 전기차 배터리에 불안과 공포(전기차 포비아)가 확산한 가운데 LG에너지솔루션(LG엔솔)이 배터리 관리 토털 설루션(BMTS) 브랜드를 만들고 사업 확장에 나선다. 전기차 배터리 생산을 넘어 소프트웨어 부문 강화로 수익원을 넓히려는 움직임으로 풀이된다.  LG엔솔은 25일 '항상 고객의 곁에'(Be around your side)란 뜻의 BMTS 브랜드 'B.around'(비.어라운드)를 공개했다. 어떤 상황에서든 배터리 상태를 실시간 모니터링하는 LG엔솔 BMTS의 기술 경쟁력을 상징한다는 설명이다.  비.어라운드 제품에는 기존 배터리 관리 시스템(BMS)에 클라우드와 인공지능(AI) 기술을 결합해 화재방지 등을 위한 안전 진단, 퇴화·수명 예측 등을 더 쉽게 할 계획이다. 배터리 불량 유형을 사전에 진단하고 퇴화 상태를 점검하는 소프트웨어를 통해 오랫동안 쓸 수 있게 할 방침이다.  이달훈 LG에너지솔루션 BMS개발센터장 상무는 \"오랜 기간 안전 진단 시스템을 위해 노력했고 이제는 배터리 관리 소프트웨어 사업으로 확장해 배터리의 건강한 사용을 위해 설루션을 제공하는 기업이 되려고 한다\"고 말했다.  김청환 기자 chk@hankookilbo.com<|eot_id|><|start_header_id|>assistant<|end_header_id|>회사리스트:\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "315efd71-8e8b-4c74-b525-11cd39f49d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.42it/s, est. speed input: 2447.92 toks/s, output: 123.94 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['LG에너지솔루션', 'LG엔솔루션', '배터리 관리 토털 설루션']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 샘플링 파라미터 설정\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    stop=[\"\\n<|end_of_text|>\", \"<|eot_id|>\"],  # 문자열로 중지 토큰 지정\n",
    "    repetition_penalty=1.0\n",
    ")\n",
    "\n",
    "# 생성\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "\n",
    "# 결과 출력\n",
    "for output in outputs:\n",
    "    print()\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed309392-17d9-44b7-8f33-8486e38e69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('company_extract_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93463d61-1388-4885-a358-b9c3f741a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df['instruction'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "436374b6-48cc-4c90-8f95-22240c1264e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(system_prompt, news_article):\n",
    "    # 프롬프트 템플릿 준비\n",
    "    template = string.Template(\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    \n",
    "${system_content}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    \n",
    "${user_content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\")\n",
    "    \n",
    "    user_prompt = news_article\n",
    "    \n",
    "    # 메시지 준비\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # 프롬프트 생성\n",
    "    prompt = template.safe_substitute({\n",
    "        \"system_content\": messages[0][\"content\"],\n",
    "        \"user_content\": messages[1][\"content\"]\n",
    "    })\n",
    "\n",
    "    prompt = prompt.strip() + \"회사리스트:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32948cef-0700-40a1-b452-4262bd13eea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "    \n",
      "1. 주어진 입력에 나와있는 회사명을 찾으세요 \n",
      "2. 회사명을 찾을수 없다면, 빈 파이썬 리스트를 출력하고, 임의로 답변하려고 시도하지 마세요. \n",
      "3. 출력은 반드시 파이썬의 리스트 형태로 생성하십시오.\n",
      "4. 뉴스데이터:이 주어지면 회사리스트: 다음에 파이썬 리스트 형태로 작성하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "    \n",
      "뉴스데이터: 5일 오전 비트코인이 2만달러를 회복했다. 사진 이미지투데이 비트코인이 반등세를 보이며 2만달러를 회복했다. 5일 오전 7시 49분 글로벌 코인시황 중계 사이트 코인마켓캡에서 비트코인 1BTC 비트코인 단위 당 가격은 24시간 전 대비 3.77% 상승한 2만1달러에 거래되고 있다. 이더리움은 1ETH 이더리움 단위 당 6.36% 오른 1140달러 솔라나는 1SOL 솔라나 단위 당 9.38% 뛴 36.38달러에 거래되고 있다. 리플은 1XRP 리플 단위 당 0.32달러에 거래되며 2.35% 상승했다. 투자자들이 이번 주 발표될 미국의 6월 고용지표 중앙은행인 연방준비제도 Fed·연준 의 6월 연방공개시장위원회 FOMC 회의록 공개를 주시하고 있는 가운데 비트코인 바닥론 이 슬며시 고개를 들고 있다. 가상화폐 암호화폐 전문 매체 유투데이에 따르면 알리 마르티네즈 애널리스트는 최근 트위터를 통해 가상화폐 온체인 데이터 분석 플랫폼 샌티멘트의 비트코인 MVRV 365D 지표가 2015년 1월 기록한 56.85% 2018년 12월 기록한 55.62%에 가까워졌다 며 지난달 중순 50.09% 현재 48.23%를 나타내고 있는 해당 지표는 지난 약세 사이클 바닥에서 나타났던 수치와 비슷한 수준을 기록하고 있다 고 말했다. 투자 심리는 여전히 얼어붙어 있으나 소폭 개선됐다. 국내 가상자산 거래소 업비트를 운영하고 있는 두나무의 디지털자산 공포 탐욕지수 는 전일 기준 32.90점으로 공포 로 집계됐다. 0과 가까울수록 매우 공포 로 시장 위축을 100과 가까울수록 매우 탐욕 으로 시장 호황을 의미한다. 오는 8일 현지시각 비농업 일자리 수와 실업률 시간당 평균임금 등을 포함한 미국 고용지표가 발표된다. 6일 공개되는 6월 FOMC 회의록을 통해서는 위원들의 인플레이션 우려 수준을 확인할 수 있을 것으로 보인다. 시장에서는 이달 연준이 금리를 0.75%포인트 인상에 나설 것으로 전망하고 있다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>회사리스트:\n"
     ]
    }
   ],
   "source": [
    "prompt = make_prompt(system_prompt, inputs[0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e33af4e1-034b-4c53-a06e-290e061512f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for input in inputs:\n",
    "    prompts.append(make_prompt(system_prompt, input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8422034a-7894-4d3c-9bf2-4608725f5f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5/5 [00:00<00:00, 14.32it/s, est. speed input: 9765.93 toks/s, output: 315.57 toks/s]\n"
     ]
    }
   ],
   "source": [
    "results = llm.generate(prompts[:5], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e996aab-59ca-4cc8-bcf5-e70a8e585ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['비트코인', '이더리움', '솔라나']\n",
      "--\n",
      " ['쌍용자동차', 'KG그룹', '영종도 네스트 호텔']\n",
      "--\n",
      " ['농림축산식품부', '대형마트', '국적운송사']\n",
      "--\n",
      " ['피토틱스', 'FCCE', '피토틱스 메타바이오틱스']\n",
      "--\n",
      " ['BMW', '메르세데스벤츠', '폭스바겐']\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result.outputs[0].text)\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f05c7c76-de70-4424-b2c2-2212163b35da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [00:14<00:00, 34.41it/s, est. speed input: 25642.66 toks/s, output: 611.92 toks/s]\n"
     ]
    }
   ],
   "source": [
    "results = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca7697c2-6836-4f4b-a868-e3ad99e1fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "014cea6b-da67-45fb-9d10-e4a0a6776494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = []\n",
    "for pred in results:\n",
    "    preds.append(ast.literal_eval(pred.outputs[0].text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6309ba31-dc15-4df5-b87d-b64bce9b4ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['쌍용자동차', 'KG그룹', '영종도 네스트 호텔']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a38a8b7a-be43-426e-ad32-8b64d8043b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for label in df['output'].to_list():\n",
    "    labels.append(ast.literal_eval(re.sub('회사리스트 ?: ?','',label).strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b92b06a0-826b-467e-870e-71a3432d3d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['쌍용자동차', 'KG그룹']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf9a99ae-237d-48ea-8148-9a2fbc3847c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'accuracy': 0.9980953800298062, 'f1_score': 0.9980953800298062}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from itertools import chain\n",
    "\n",
    "def calculate_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculate accuracy and F1 score for list of lists of companies.\n",
    "\n",
    "    :param labels: List of lists of true company labels.\n",
    "    :param preds: List of lists of predicted company labels.\n",
    "    :return: Dictionary containing accuracy and F1 score.\n",
    "    \"\"\"\n",
    "    if len(labels) != len(preds):\n",
    "        raise ValueError(\"Labels and predictions must have the same number of instances.\")\n",
    "    \n",
    "    # Flatten lists for binary representation\n",
    "    all_companies = set(chain.from_iterable(labels + preds))\n",
    "    all_companies = list(all_companies)  # Ensure consistent ordering\n",
    "    \n",
    "    # Create binary vectors for each instance\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for label, pred in zip(labels, preds):\n",
    "        label_set = set(label)\n",
    "        pred_set = set(pred)\n",
    "        \n",
    "        y_true.append([1 if company in label_set else 0 for company in all_companies])\n",
    "        y_pred.append([1 if company in pred_set else 0 for company in all_companies])\n",
    "    \n",
    "    # Flatten binary vectors for overall metrics\n",
    "    y_true_flat = list(chain.from_iterable(y_true))\n",
    "    y_pred_flat = list(chain.from_iterable(y_pred))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true_flat, y_pred_flat)\n",
    "    f1 = f1_score(y_true_flat, y_pred_flat, average=\"micro\")  # Use 'micro' for multilabel cases\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "metrics = calculate_metrics(labels, preds)\n",
    "print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a23ee-5846-42d2-8fb1-158c03e4c12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
